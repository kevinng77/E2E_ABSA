>>>>>>>>>>>>>>>>>>>>>2021-07-21 08:03:09.791525>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: elmo
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/elmo-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 2000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: True
>>> demo: False
>>> file_path: {'train': '/content/data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': '/content/data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': '/content/data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.9064 F1: 9.99% dev loss: 0.3200 F1: 9.49% 1.39 min
> Epoch: 2 Step: 200, train loss: 0.1747 F1: 19.01% dev loss: 0.1129 F1: 22.55% 1.45 min
> Epoch: 3 Step: 300, train loss: 0.1014 F1: 28.86% dev loss: 0.1200 F1: 25.18% 1.45 min
> Epoch: 5 Step: 400, train loss: 0.0913 F1: 32.97% dev loss: 0.0905 F1: 29.60% 1.46 min
> Epoch: 6 Step: 500, train loss: 0.0641 F1: 43.82% dev loss: 0.0832 F1: 33.21% 1.46 min
> Epoch: 7 Step: 600, train loss: 0.0398 F1: 62.32% dev loss: 0.0836 F1: 34.07% 1.45 min
> Epoch: 9 Step: 700, train loss: 0.0261 F1: 77.95% dev loss: 0.0809 F1: 35.37% 1.46 min
> Epoch: 10 Step: 800, train loss: 0.0175 F1: 86.54% dev loss: 0.0808 F1: 35.87% 1.45 min
> Epoch: 11 Step: 900, train loss: 0.0111 F1: 91.10% dev loss: 0.0824 F1: 36.45% 1.45 min
> Epoch: 13 Step: 1000, train loss: 0.0080 F1: 94.73% dev loss: 0.0840 F1: 34.40% 1.46 min
> Epoch: 14 Step: 1100, train loss: 0.0066 F1: 95.54% dev loss: 0.0845 F1: 37.35% 1.46 min
> Epoch: 15 Step: 1200, train loss: 0.0078 F1: 93.37% dev loss: 0.0886 F1: 34.15% 1.45 min
> Epoch: 17 Step: 1300, train loss: 0.0069 F1: 96.09% dev loss: 0.0905 F1: 34.85% 1.46 min
> Epoch: 18 Step: 1400, train loss: 0.0050 F1: 96.67% dev loss: 0.0903 F1: 33.80% 1.46 min
> Epoch: 19 Step: 1500, train loss: 0.0038 F1: 97.80% dev loss: 0.0908 F1: 36.62% 1.45 min
> Epoch: 21 Step: 1600, train loss: 0.0029 F1: 98.47% dev loss: 0.0936 F1: 35.44% 1.46 min
> Epoch: 22 Step: 1700, train loss: 0.0026 F1: 98.66% dev loss: 0.0939 F1: 34.32% 1.46 min
> Epoch: 23 Step: 1800, train loss: 0.0024 F1: 98.65% dev loss: 0.0934 F1: 34.45% 1.45 min
> Epoch: 25 Step: 1900, train loss: 0.0022 F1: 99.07% dev loss: 0.0947 F1: 33.66% 1.46 min
> Epoch: 26 Step: 2000, train loss: 0.0021 F1: 99.06% dev loss: 0.0947 F1: 34.42% 1.46 min
Initializing ELMo

>>>>>>>>>>>>>>>>>>>>>2021-07-21 08:39:28.881291>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: elmo
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/elmo-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 2000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: True
>>> demo: False
>>> file_path: {'train': '/content/data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': '/content/data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': '/content/data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 1.0709 F1: 9.43% dev loss: 0.3996 F1: 9.49% 1.47 min
> Epoch: 2 Step: 200, train loss: 0.2016 F1: 11.45% dev loss: 0.1249 F1: 13.24% 1.53 min
> Epoch: 3 Step: 300, train loss: 0.1061 F1: 25.87% dev loss: 0.0867 F1: 30.65% 1.54 min
> Epoch: 5 Step: 400, train loss: 0.0669 F1: 41.64% dev loss: 0.0748 F1: 35.33% 1.54 min
> Epoch: 6 Step: 500, train loss: 0.0332 F1: 73.17% dev loss: 0.0681 F1: 49.74% 1.54 min
> Epoch: 7 Step: 600, train loss: 0.0170 F1: 84.21% dev loss: 0.0661 F1: 49.69% 1.54 min
> Epoch: 9 Step: 700, train loss: 0.0079 F1: 96.95% dev loss: 0.0667 F1: 52.64% 1.54 min
> Epoch: 10 Step: 800, train loss: 0.0029 F1: 99.33% dev loss: 0.0687 F1: 56.29% 1.54 min
> Epoch: 11 Step: 900, train loss: 0.0014 F1: 99.56% dev loss: 0.0726 F1: 52.74% 1.54 min
> Epoch: 13 Step: 1000, train loss: 0.0008 F1: 99.94% dev loss: 0.0751 F1: 53.88% 1.54 min
> Epoch: 14 Step: 1100, train loss: 0.0006 F1: 99.95% dev loss: 0.0739 F1: 54.81% 1.54 min
> Epoch: 15 Step: 1200, train loss: 0.0004 F1: 100.00% dev loss: 0.0753 F1: 54.92% 1.54 min
> Epoch: 17 Step: 1300, train loss: 0.0004 F1: 100.00% dev loss: 0.0769 F1: 53.62% 1.54 min
> Epoch: 18 Step: 1400, train loss: 0.0003 F1: 99.98% dev loss: 0.0769 F1: 52.30% 1.54 min
> Epoch: 19 Step: 1500, train loss: 0.0003 F1: 99.98% dev loss: 0.0773 F1: 52.58% 1.54 min
> Epoch: 21 Step: 1600, train loss: 0.0002 F1: 100.00% dev loss: 0.0782 F1: 53.32% 1.55 min
> Epoch: 22 Step: 1700, train loss: 0.0002 F1: 99.98% dev loss: 0.0783 F1: 52.20% 1.54 min
> Epoch: 23 Step: 1800, train loss: 0.0002 F1: 100.00% dev loss: 0.0784 F1: 52.44% 1.54 min
> Epoch: 25 Step: 1900, train loss: 0.0002 F1: 100.00% dev loss: 0.0788 F1: 53.36% 1.55 min
> Epoch: 26 Step: 2000, train loss: 0.0002 F1: 100.00% dev loss: 0.0787 F1: 52.47% 1.54 min
Initializing ELMo

>>>>>>>>>>>>>>>>>>>>>2021-07-21 09:16:47.695851>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: elmo
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: CE
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/elmo-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 2000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: True
>>> demo: False
>>> file_path: {'train': '/content/data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': '/content/data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': '/content/data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.1, 0.8, 1.0, 1.0, 1.2, 1.2, 1.2, 1.0, 1.0, 1.0]
> Epoch: 1 Step: 100, train loss: 2.2119 F1: 12.82% dev loss: 1.8972 F1: 13.01% 1.47 min
> Epoch: 2 Step: 200, train loss: 1.3936 F1: 25.47% dev loss: 0.9645 F1: 27.69% 1.53 min
> Epoch: 3 Step: 300, train loss: 0.7407 F1: 37.18% dev loss: 0.6936 F1: 36.37% 1.53 min
> Epoch: 5 Step: 400, train loss: 0.3973 F1: 76.62% dev loss: 0.6536 F1: 43.83% 1.54 min
> Epoch: 6 Step: 500, train loss: 0.2277 F1: 86.80% dev loss: 0.7511 F1: 53.29% 1.54 min
> Epoch: 7 Step: 600, train loss: 0.1228 F1: 93.23% dev loss: 0.8010 F1: 51.83% 1.54 min
> Epoch: 9 Step: 700, train loss: 0.0564 F1: 98.27% dev loss: 0.7858 F1: 55.61% 1.54 min
> Epoch: 10 Step: 800, train loss: 0.0304 F1: 98.87% dev loss: 0.8399 F1: 53.85% 1.54 min
> Epoch: 11 Step: 900, train loss: 0.0206 F1: 99.16% dev loss: 1.0499 F1: 53.79% 1.53 min
> Epoch: 13 Step: 1000, train loss: 0.0142 F1: 99.56% dev loss: 1.0046 F1: 54.18% 1.54 min
> Epoch: 14 Step: 1100, train loss: 0.0106 F1: 99.65% dev loss: 0.9518 F1: 57.55% 1.54 min
> Epoch: 15 Step: 1200, train loss: 0.0079 F1: 99.84% dev loss: 1.0197 F1: 57.33% 1.54 min
> Epoch: 17 Step: 1300, train loss: 0.0064 F1: 99.80% dev loss: 1.0736 F1: 56.01% 1.54 min
> Epoch: 18 Step: 1400, train loss: 0.0058 F1: 99.89% dev loss: 1.0276 F1: 58.22% 1.54 min
> Epoch: 19 Step: 1500, train loss: 0.0048 F1: 99.96% dev loss: 1.0427 F1: 55.33% 1.53 min
> Epoch: 21 Step: 1600, train loss: 0.0044 F1: 100.00% dev loss: 1.0786 F1: 55.65% 1.54 min
> Epoch: 22 Step: 1700, train loss: 0.0042 F1: 100.00% dev loss: 1.0708 F1: 55.78% 1.53 min
> Epoch: 23 Step: 1800, train loss: 0.0037 F1: 100.00% dev loss: 1.0763 F1: 55.39% 1.54 min
> Epoch: 25 Step: 1900, train loss: 0.0036 F1: 100.00% dev loss: 1.0840 F1: 55.72% 1.54 min
> Epoch: 26 Step: 2000, train loss: 0.0037 F1: 100.00% dev loss: 1.0802 F1: 55.78% 1.54 min
Initializing ELMo

>>>>>>>>>>>>>>>>>>>>>2021-07-21 09:50:23.951662>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: elmo
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: CE
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/elmo-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 2000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/content/data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': '/content/data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': '/content/data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.1, 0.8, 1.0, 1.0, 1.2, 1.2, 1.2, 1.0, 1.0, 1.0]
> Epoch: 1 Step: 100, train loss: 2.2600 F1: 13.08% dev loss: 2.0939 F1: 13.08% 0.55 min
> Epoch: 2 Step: 200, train loss: 1.8167 F1: 12.27% dev loss: 1.4434 F1: 12.95% 0.56 min
> Epoch: 3 Step: 300, train loss: 1.2838 F1: 20.89% dev loss: 1.1254 F1: 19.75% 0.58 min
> Epoch: 5 Step: 400, train loss: 1.0297 F1: 29.88% dev loss: 0.9293 F1: 30.07% 0.59 min
> Epoch: 6 Step: 500, train loss: 0.8480 F1: 34.04% dev loss: 0.7978 F1: 31.67% 0.58 min
> Epoch: 7 Step: 600, train loss: 0.6801 F1: 39.56% dev loss: 0.7342 F1: 40.58% 0.59 min
> Epoch: 9 Step: 700, train loss: 0.5887 F1: 50.08% dev loss: 0.6942 F1: 38.26% 0.59 min
> Epoch: 10 Step: 800, train loss: 0.5145 F1: 57.59% dev loss: 0.6959 F1: 37.82% 0.59 min
> Epoch: 11 Step: 900, train loss: 0.4392 F1: 65.43% dev loss: 0.7326 F1: 45.40% 0.58 min
> Epoch: 13 Step: 1000, train loss: 0.3935 F1: 72.44% dev loss: 0.6901 F1: 41.74% 0.58 min
> Epoch: 14 Step: 1100, train loss: 0.3611 F1: 74.29% dev loss: 0.7396 F1: 46.22% 0.59 min
> Epoch: 15 Step: 1200, train loss: 0.3196 F1: 77.78% dev loss: 0.7517 F1: 41.71% 0.58 min
> Epoch: 17 Step: 1300, train loss: 0.2973 F1: 82.69% dev loss: 0.6746 F1: 40.30% 0.59 min
> Epoch: 18 Step: 1400, train loss: 0.2740 F1: 83.08% dev loss: 0.6954 F1: 40.14% 0.58 min
> Epoch: 19 Step: 1500, train loss: 0.2462 F1: 83.45% dev loss: 0.7335 F1: 41.11% 0.59 min
> Epoch: 21 Step: 1600, train loss: 0.2324 F1: 86.60% dev loss: 0.6974 F1: 39.76% 0.59 min
> Epoch: 22 Step: 1700, train loss: 0.2262 F1: 86.04% dev loss: 0.7298 F1: 39.89% 0.59 min
> Epoch: 23 Step: 1800, train loss: 0.2125 F1: 86.29% dev loss: 0.7523 F1: 38.98% 0.58 min
> Epoch: 25 Step: 1900, train loss: 0.2063 F1: 88.57% dev loss: 0.7455 F1: 40.12% 0.59 min
> Epoch: 26 Step: 2000, train loss: 0.2100 F1: 86.65% dev loss: 0.7441 F1: 38.97% 0.58 min
Initializing ELMo

>>>>>>>>>>>>>>>>>>>>>2021-07-21 10:11:28.663039>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: elmo
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: CE
>>> gamma: 3.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/elmo-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 2000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: True
>>> demo: False
>>> file_path: {'train': '/content/data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': '/content/data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': '/content/data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.1, 0.8, 1.0, 1.0, 1.2, 1.2, 1.2, 1.0, 1.0, 1.0]
> Epoch: 1 Step: 100, train loss: 2.2119 F1: 12.82% dev loss: 1.8972 F1: 13.01% 1.46 min
> Epoch: 2 Step: 200, train loss: 1.3937 F1: 25.36% dev loss: 0.9649 F1: 28.14% 1.53 min
> Epoch: 3 Step: 300, train loss: 0.7404 F1: 37.23% dev loss: 0.7046 F1: 35.97% 1.53 min
> Epoch: 5 Step: 400, train loss: 0.3927 F1: 74.48% dev loss: 0.6496 F1: 45.62% 1.54 min
> Epoch: 6 Step: 500, train loss: 0.2158 F1: 87.27% dev loss: 0.7530 F1: 47.88% 1.54 min
> Epoch: 7 Step: 600, train loss: 0.1224 F1: 92.75% dev loss: 0.7969 F1: 47.44% 1.53 min
> Epoch: 9 Step: 700, train loss: 0.0537 F1: 98.37% dev loss: 0.7821 F1: 57.32% 1.54 min
> Epoch: 10 Step: 800, train loss: 0.0326 F1: 98.90% dev loss: 0.8233 F1: 53.03% 1.54 min
> Epoch: 11 Step: 900, train loss: 0.0194 F1: 99.32% dev loss: 0.9489 F1: 55.41% 1.53 min
> Epoch: 13 Step: 1000, train loss: 0.0139 F1: 99.52% dev loss: 0.9507 F1: 56.79% 1.54 min
> Epoch: 14 Step: 1100, train loss: 0.0111 F1: 99.80% dev loss: 0.9024 F1: 54.54% 1.54 min
> Epoch: 15 Step: 1200, train loss: 0.0083 F1: 99.79% dev loss: 0.9788 F1: 56.58% 1.53 min
> Epoch: 17 Step: 1300, train loss: 0.0068 F1: 99.94% dev loss: 1.0260 F1: 55.87% 1.54 min
> Epoch: 18 Step: 1400, train loss: 0.0064 F1: 99.88% dev loss: 0.9749 F1: 56.65% 1.54 min
> Epoch: 19 Step: 1500, train loss: 0.0053 F1: 99.91% dev loss: 0.9979 F1: 56.40% 1.53 min
> Epoch: 21 Step: 1600, train loss: 0.0047 F1: 99.97% dev loss: 1.0350 F1: 55.86% 1.54 min
> Epoch: 22 Step: 1700, train loss: 0.0047 F1: 99.97% dev loss: 1.0210 F1: 57.07% 1.53 min
> Epoch: 23 Step: 1800, train loss: 0.0041 F1: 99.97% dev loss: 1.0250 F1: 56.69% 1.53 min
> Epoch: 25 Step: 1900, train loss: 0.0040 F1: 99.85% dev loss: 1.0370 F1: 55.88% 1.54 min
> Epoch: 26 Step: 2000, train loss: 0.0041 F1: 99.97% dev loss: 1.0326 F1: 56.09% 1.54 min
Initializing ELMo

>>>>>>>>>>>>>>>>>>>>>2021-07-21 10:45:03.285892>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 1e-05
>>> epochs: 100
>>> step: 100
>>> model_name: elmo
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/elmo-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 2000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: True
>>> demo: False
>>> file_path: {'train': '/content/data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': '/content/data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': '/content/data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 1e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 1.2855 F1: 9.87% dev loss: 1.1852 F1: 9.24% 1.50 min
> Epoch: 2 Step: 200, train loss: 0.9508 F1: 11.01% dev loss: 0.5458 F1: 9.49% 1.54 min
> Epoch: 3 Step: 300, train loss: 0.2766 F1: 9.45% dev loss: 0.1809 F1: 9.49% 1.54 min
> Epoch: 5 Step: 400, train loss: 0.1772 F1: 13.10% dev loss: 0.1448 F1: 11.45% 1.54 min
> Epoch: 6 Step: 500, train loss: 0.1452 F1: 13.07% dev loss: 0.1203 F1: 14.14% 1.54 min
> Epoch: 7 Step: 600, train loss: 0.1128 F1: 22.91% dev loss: 0.1055 F1: 15.21% 1.54 min
> Epoch: 9 Step: 700, train loss: 0.0984 F1: 27.16% dev loss: 0.0980 F1: 28.30% 1.54 min
> Epoch: 10 Step: 800, train loss: 0.0855 F1: 32.49% dev loss: 0.0915 F1: 29.44% 1.54 min
> Epoch: 11 Step: 900, train loss: 0.0701 F1: 36.78% dev loss: 0.0870 F1: 30.34% 1.53 min
> Epoch: 13 Step: 1000, train loss: 0.0628 F1: 38.48% dev loss: 0.0851 F1: 32.17% 1.54 min
> Epoch: 14 Step: 1100, train loss: 0.0551 F1: 42.24% dev loss: 0.0842 F1: 32.67% 1.54 min
> Epoch: 15 Step: 1200, train loss: 0.0461 F1: 44.77% dev loss: 0.0833 F1: 33.85% 1.54 min
> Epoch: 17 Step: 1300, train loss: 0.0420 F1: 45.35% dev loss: 0.0822 F1: 35.16% 1.54 min
> Epoch: 18 Step: 1400, train loss: 0.0382 F1: 51.65% dev loss: 0.0826 F1: 34.98% 1.54 min
> Epoch: 19 Step: 1500, train loss: 0.0327 F1: 60.29% dev loss: 0.0826 F1: 35.25% 1.54 min
> Epoch: 21 Step: 1600, train loss: 0.0309 F1: 63.40% dev loss: 0.0825 F1: 35.53% 1.54 min
> Epoch: 22 Step: 1700, train loss: 0.0293 F1: 64.60% dev loss: 0.0837 F1: 38.93% 1.54 min
> Epoch: 23 Step: 1800, train loss: 0.0261 F1: 65.36% dev loss: 0.0837 F1: 39.40% 1.54 min
> Epoch: 25 Step: 1900, train loss: 0.0258 F1: 66.93% dev loss: 0.0837 F1: 43.61% 1.54 min
> Epoch: 26 Step: 2000, train loss: 0.0261 F1: 66.50% dev loss: 0.0836 F1: 44.11% 1.54 min
Initializing ELMo

>>>>>>>>>>>>>>>>>>>>>2021-07-21 11:19:25.869534>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 4e-05
>>> epochs: 100
>>> step: 100
>>> model_name: elmo
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/elmo-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 2000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: True
>>> demo: False
>>> file_path: {'train': '/content/data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': '/content/data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': '/content/data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 4e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 1.1301 F1: 11.02% dev loss: 0.6213 F1: 9.49% 1.49 min
> Epoch: 2 Step: 200, train loss: 0.2505 F1: 11.64% dev loss: 0.1405 F1: 11.15% 1.54 min
> Epoch: 3 Step: 300, train loss: 0.1229 F1: 21.39% dev loss: 0.0997 F1: 21.58% 1.54 min
> Epoch: 5 Step: 400, train loss: 0.0851 F1: 36.53% dev loss: 0.0848 F1: 33.40% 1.54 min
> Epoch: 6 Step: 500, train loss: 0.0513 F1: 52.23% dev loss: 0.0698 F1: 46.97% 1.54 min
> Epoch: 7 Step: 600, train loss: 0.0254 F1: 73.78% dev loss: 0.0658 F1: 43.21% 1.54 min
> Epoch: 9 Step: 700, train loss: 0.0138 F1: 92.55% dev loss: 0.0703 F1: 49.34% 1.54 min
> Epoch: 10 Step: 800, train loss: 0.0083 F1: 94.75% dev loss: 0.0672 F1: 54.32% 1.54 min
> Epoch: 11 Step: 900, train loss: 0.0034 F1: 98.25% dev loss: 0.0716 F1: 48.31% 1.54 min
> Epoch: 13 Step: 1000, train loss: 0.0019 F1: 99.47% dev loss: 0.0714 F1: 54.28% 1.55 min
> Epoch: 14 Step: 1100, train loss: 0.0014 F1: 99.57% dev loss: 0.0708 F1: 54.94% 1.54 min
> Epoch: 15 Step: 1200, train loss: 0.0009 F1: 99.95% dev loss: 0.0732 F1: 54.21% 1.54 min
> Epoch: 17 Step: 1300, train loss: 0.0007 F1: 100.00% dev loss: 0.0753 F1: 52.89% 1.54 min
> Epoch: 18 Step: 1400, train loss: 0.0006 F1: 100.00% dev loss: 0.0742 F1: 53.48% 1.54 min
> Epoch: 19 Step: 1500, train loss: 0.0005 F1: 100.00% dev loss: 0.0750 F1: 53.95% 1.54 min
> Epoch: 21 Step: 1600, train loss: 0.0005 F1: 100.00% dev loss: 0.0761 F1: 53.83% 1.54 min
> Epoch: 22 Step: 1700, train loss: 0.0004 F1: 100.00% dev loss: 0.0763 F1: 53.66% 1.54 min
> Epoch: 23 Step: 1800, train loss: 0.0004 F1: 100.00% dev loss: 0.0765 F1: 53.90% 1.54 min
> Epoch: 25 Step: 1900, train loss: 0.0004 F1: 100.00% dev loss: 0.0765 F1: 53.77% 1.54 min
> Epoch: 26 Step: 2000, train loss: 0.0004 F1: 100.00% dev loss: 0.0766 F1: 54.07% 1.54 min
Initializing ELMo

>>>>>>>>>>>>>>>>>>>>>2021-07-21 11:51:39.038417>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: elmo
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 1.0
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/elmo-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 2000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: True
>>> demo: False
>>> file_path: {'train': '/content/data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': '/content/data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': '/content/data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
> Epoch: 1 Step: 100, train loss: 1.4277 F1: 9.43% dev loss: 0.5324 F1: 9.49% 1.54 min
> Epoch: 2 Step: 200, train loss: 0.2687 F1: 11.50% dev loss: 0.1663 F1: 13.17% 1.54 min
> Epoch: 3 Step: 300, train loss: 0.1411 F1: 26.13% dev loss: 0.1153 F1: 30.10% 1.54 min
> Epoch: 5 Step: 400, train loss: 0.0888 F1: 41.72% dev loss: 0.1002 F1: 35.11% 1.54 min
> Epoch: 6 Step: 500, train loss: 0.0457 F1: 70.81% dev loss: 0.0924 F1: 50.77% 1.54 min
> Epoch: 7 Step: 600, train loss: 0.0229 F1: 84.88% dev loss: 0.0975 F1: 48.43% 1.54 min
> Epoch: 9 Step: 700, train loss: 0.0103 F1: 97.89% dev loss: 0.0878 F1: 55.62% 1.54 min
> Epoch: 10 Step: 800, train loss: 0.0036 F1: 99.25% dev loss: 0.0941 F1: 53.46% 1.54 min
> Epoch: 11 Step: 900, train loss: 0.0017 F1: 99.65% dev loss: 0.0959 F1: 53.87% 1.54 min
> Epoch: 13 Step: 1000, train loss: 0.0010 F1: 100.00% dev loss: 0.0988 F1: 53.65% 1.54 min
> Epoch: 14 Step: 1100, train loss: 0.0008 F1: 99.98% dev loss: 0.0977 F1: 54.99% 1.54 min
> Epoch: 15 Step: 1200, train loss: 0.0005 F1: 100.00% dev loss: 0.0989 F1: 55.36% 1.54 min
> Epoch: 17 Step: 1300, train loss: 0.0004 F1: 100.00% dev loss: 0.1011 F1: 54.00% 1.54 min
> Epoch: 18 Step: 1400, train loss: 0.0004 F1: 100.00% dev loss: 0.1017 F1: 53.93% 1.54 min
> Epoch: 19 Step: 1500, train loss: 0.0003 F1: 100.00% dev loss: 0.1020 F1: 53.99% 1.54 min
> Epoch: 21 Step: 1600, train loss: 0.0003 F1: 100.00% dev loss: 0.1030 F1: 53.81% 1.54 min
> Epoch: 22 Step: 1700, train loss: 0.0003 F1: 99.98% dev loss: 0.1035 F1: 53.81% 1.54 min
> Epoch: 23 Step: 1800, train loss: 0.0002 F1: 100.00% dev loss: 0.1034 F1: 53.90% 1.54 min
> Epoch: 25 Step: 1900, train loss: 0.0002 F1: 100.00% dev loss: 0.1038 F1: 53.81% 1.54 min
> Epoch: 26 Step: 2000, train loss: 0.0003 F1: 100.00% dev loss: 0.1039 F1: 53.81% 1.54 min
Initializing ELMo

>>>>>>>>>>>>>>>>>>>>>2021-07-21 12:33:45.824826>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: elmo
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/elmo-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 2000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: True
>>> demo: False
>>> file_path: {'train': '/content/data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': '/content/data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': '/content/data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.9064 F1: 9.99% dev loss: 0.3200 F1: 9.49% 1.39 min
> Epoch: 2 Step: 200, train loss: 0.1758 F1: 18.82% dev loss: 0.1164 F1: 22.78% 1.44 min
> Epoch: 3 Step: 300, train loss: 0.0988 F1: 28.40% dev loss: 0.0919 F1: 26.95% 1.45 min
> Epoch: 5 Step: 400, train loss: 0.0693 F1: 39.73% dev loss: 0.0791 F1: 35.00% 1.46 min
> Epoch: 6 Step: 500, train loss: 0.0466 F1: 62.55% dev loss: 0.0706 F1: 34.83% 1.46 min
> Epoch: 7 Step: 600, train loss: 0.0207 F1: 81.95% dev loss: 0.0666 F1: 40.81% 1.45 min
> Epoch: 9 Step: 700, train loss: 0.0090 F1: 94.89% dev loss: 0.0680 F1: 46.51% 1.46 min
> Epoch: 10 Step: 800, train loss: 0.0047 F1: 98.39% dev loss: 0.0668 F1: 47.40% 1.46 min
> Epoch: 11 Step: 900, train loss: 0.0027 F1: 99.50% dev loss: 0.0686 F1: 43.97% 1.45 min
> Epoch: 13 Step: 1000, train loss: 0.0015 F1: 99.97% dev loss: 0.0680 F1: 47.46% 1.46 min
> Epoch: 14 Step: 1100, train loss: 0.0011 F1: 100.00% dev loss: 0.0683 F1: 45.87% 1.45 min
> Epoch: 15 Step: 1200, train loss: 0.0008 F1: 100.00% dev loss: 0.0691 F1: 47.13% 1.45 min
> Epoch: 17 Step: 1300, train loss: 0.0006 F1: 100.00% dev loss: 0.0702 F1: 48.07% 1.46 min
> Epoch: 18 Step: 1400, train loss: 0.0005 F1: 100.00% dev loss: 0.0701 F1: 46.59% 1.46 min
> Epoch: 19 Step: 1500, train loss: 0.0004 F1: 100.00% dev loss: 0.0701 F1: 47.42% 1.45 min
> Epoch: 21 Step: 1600, train loss: 0.0004 F1: 100.00% dev loss: 0.0705 F1: 47.56% 1.46 min
> Epoch: 22 Step: 1700, train loss: 0.0003 F1: 100.00% dev loss: 0.0707 F1: 46.32% 1.45 min
> Epoch: 23 Step: 1800, train loss: 0.0003 F1: 100.00% dev loss: 0.0708 F1: 47.41% 1.45 min
Initializing ELMo

>>>>>>>>>>>>>>>>>>>>>2021-07-21 13:00:48.574401>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: elmo
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: CE
>>> gamma: 3.0
>>> alpha: 2.0
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/elmo-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 2000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: True
>>> demo: False
>>> file_path: {'train': '/content/data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': '/content/data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': '/content/data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.1, 0.8, 1.0, 1.0, 1.2, 1.2, 1.2, 1.0, 1.0, 1.0]
> Epoch: 1 Step: 100, train loss: 2.1482 F1: 12.81% dev loss: 1.7990 F1: 13.41% 1.45 min
> Epoch: 2 Step: 200, train loss: 1.4346 F1: 18.48% dev loss: 1.1529 F1: 22.90% 1.45 min
> Epoch: 3 Step: 300, train loss: 0.9962 F1: 29.07% dev loss: 0.9260 F1: 27.00% 1.45 min
> Epoch: 5 Step: 400, train loss: 0.7953 F1: 34.72% dev loss: 0.8365 F1: 29.42% 1.46 min
> Epoch: 6 Step: 500, train loss: 0.6276 F1: 49.95% dev loss: 0.8041 F1: 30.36% 1.46 min
> Epoch: 7 Step: 600, train loss: 0.4569 F1: 64.58% dev loss: 0.8493 F1: 29.93% 1.45 min
> Epoch: 9 Step: 700, train loss: 0.3688 F1: 74.20% dev loss: 0.8083 F1: 34.29% 1.46 min
> Epoch: 10 Step: 800, train loss: 0.2698 F1: 83.58% dev loss: 0.8364 F1: 33.48% 1.45 min
> Epoch: 11 Step: 900, train loss: 0.1944 F1: 88.83% dev loss: 0.8835 F1: 31.57% 1.45 min
> Epoch: 13 Step: 1000, train loss: 0.1543 F1: 92.79% dev loss: 0.9206 F1: 30.31% 1.46 min
> Epoch: 14 Step: 1100, train loss: 0.1320 F1: 93.57% dev loss: 0.8800 F1: 32.91% 1.46 min
> Epoch: 15 Step: 1200, train loss: 0.1122 F1: 94.48% dev loss: 0.9321 F1: 32.32% 1.45 min
> Epoch: 17 Step: 1300, train loss: 0.1029 F1: 95.50% dev loss: 0.9703 F1: 30.92% 1.46 min
> Epoch: 18 Step: 1400, train loss: 0.0881 F1: 96.38% dev loss: 0.9397 F1: 34.13% 1.45 min
> Epoch: 19 Step: 1500, train loss: 0.0739 F1: 96.74% dev loss: 0.9783 F1: 33.81% 1.45 min
> Epoch: 21 Step: 1600, train loss: 0.0662 F1: 97.24% dev loss: 1.0314 F1: 35.32% 1.46 min
> Epoch: 22 Step: 1700, train loss: 0.0591 F1: 97.64% dev loss: 1.0231 F1: 33.88% 1.45 min
> Epoch: 23 Step: 1800, train loss: 0.0528 F1: 97.74% dev loss: 1.0217 F1: 33.52% 1.45 min
> Epoch: 25 Step: 1900, train loss: 0.0500 F1: 98.63% dev loss: 1.0375 F1: 34.72% 1.46 min
> Epoch: 26 Step: 2000, train loss: 0.0480 F1: 98.38% dev loss: 1.0412 F1: 33.45% 1.45 min
Initializing ELMo


>>>>>>>>>>>>>>>>>>>>>2021-07-21 13:33:14.966646>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: elmo
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/elmo-lstm_res14_seed8.pth
>>> seed: 8
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 2000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: True
>>> demo: False
>>> file_path: {'train': '/content/data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': '/content/data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': '/content/data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.9744 F1: 9.43% dev loss: 0.2715 F1: 9.49% 1.52 min
> Epoch: 2 Step: 200, train loss: 0.1948 F1: 11.67% dev loss: 0.1351 F1: 11.52% 1.54 min
> Epoch: 3 Step: 300, train loss: 0.1154 F1: 22.52% dev loss: 0.0963 F1: 24.88% 1.54 min
> Epoch: 5 Step: 400, train loss: 0.0764 F1: 37.91% dev loss: 0.0873 F1: 32.58% 1.54 min
> Epoch: 6 Step: 500, train loss: 0.0440 F1: 58.46% dev loss: 0.0734 F1: 45.24% 1.54 min
> Epoch: 7 Step: 600, train loss: 0.0235 F1: 74.87% dev loss: 0.0740 F1: 40.55% 1.54 min
> Epoch: 9 Step: 700, train loss: 0.0130 F1: 90.37% dev loss: 0.0865 F1: 48.15% 1.54 min
> Epoch: 10 Step: 800, train loss: 0.0074 F1: 94.87% dev loss: 0.0809 F1: 53.42% 1.54 min
> Epoch: 11 Step: 900, train loss: 0.0031 F1: 98.27% dev loss: 0.0836 F1: 51.49% 1.54 min
> Epoch: 13 Step: 1000, train loss: 0.0015 F1: 99.85% dev loss: 0.0858 F1: 55.30% 1.54 min
> Epoch: 14 Step: 1100, train loss: 0.0011 F1: 99.85% dev loss: 0.0869 F1: 52.36% 1.54 min
> Epoch: 15 Step: 1200, train loss: 0.0007 F1: 99.96% dev loss: 0.0882 F1: 53.51% 1.53 min
> Epoch: 17 Step: 1300, train loss: 0.0006 F1: 100.00% dev loss: 0.0895 F1: 53.59% 1.54 min
> Epoch: 18 Step: 1400, train loss: 0.0005 F1: 100.00% dev loss: 0.0899 F1: 54.46% 1.54 min
> Epoch: 19 Step: 1500, train loss: 0.0004 F1: 100.00% dev loss: 0.0906 F1: 54.72% 1.54 min
> Epoch: 21 Step: 1600, train loss: 0.0004 F1: 100.00% dev loss: 0.0912 F1: 54.63% 1.54 min
> Epoch: 22 Step: 1700, train loss: 0.0004 F1: 100.00% dev loss: 0.0914 F1: 54.81% 1.54 min
> Epoch: 23 Step: 1800, train loss: 0.0003 F1: 100.00% dev loss: 0.0918 F1: 54.80% 1.54 min
> Epoch: 25 Step: 1900, train loss: 0.0003 F1: 100.00% dev loss: 0.0920 F1: 54.81% 1.54 min
> Epoch: 26 Step: 2000, train loss: 0.0003 F1: 100.00% dev loss: 0.0920 F1: 54.79% 1.54 min
Initializing ELMo

>>>>>>>>>>>>>>>>>>>>>2021-07-21 14:06:44.786277>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 3e-05
>>> epochs: 100
>>> step: 100
>>> model_name: elmo
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/elmo-lstm_res14_seed8.pth
>>> seed: 8
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 2000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: True
>>> demo: False
>>> file_path: {'train': '/content/data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': '/content/data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': '/content/data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 3e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 1.2424 F1: 9.43% dev loss: 0.8711 F1: 9.49% 1.51 min
> Epoch: 2 Step: 200, train loss: 0.3761 F1: 9.43% dev loss: 0.2064 F1: 9.49% 1.54 min
> Epoch: 3 Step: 300, train loss: 0.1829 F1: 11.73% dev loss: 0.1482 F1: 13.30% 1.54 min
> Epoch: 5 Step: 400, train loss: 0.1389 F1: 20.99% dev loss: 0.1255 F1: 20.10% 1.55 min
> Epoch: 6 Step: 500, train loss: 0.1103 F1: 31.24% dev loss: 0.1063 F1: 31.33% 1.54 min
> Epoch: 7 Step: 600, train loss: 0.0780 F1: 38.78% dev loss: 0.0972 F1: 34.59% 1.54 min
> Epoch: 9 Step: 700, train loss: 0.0591 F1: 46.69% dev loss: 0.0971 F1: 39.40% 1.54 min
> Epoch: 10 Step: 800, train loss: 0.0452 F1: 59.38% dev loss: 0.0950 F1: 39.83% 1.54 min
> Epoch: 11 Step: 900, train loss: 0.0331 F1: 69.26% dev loss: 0.1030 F1: 38.55% 1.54 min
> Epoch: 13 Step: 1000, train loss: 0.0236 F1: 82.12% dev loss: 0.0995 F1: 42.87% 1.54 min
> Epoch: 14 Step: 1100, train loss: 0.0182 F1: 88.22% dev loss: 0.1046 F1: 42.36% 1.54 min
> Epoch: 15 Step: 1200, train loss: 0.0130 F1: 91.68% dev loss: 0.1085 F1: 43.89% 1.54 min
> Epoch: 17 Step: 1300, train loss: 0.0101 F1: 94.37% dev loss: 0.1082 F1: 43.94% 1.54 min
> Epoch: 18 Step: 1400, train loss: 0.0088 F1: 95.97% dev loss: 0.1096 F1: 45.84% 1.54 min
> Epoch: 19 Step: 1500, train loss: 0.0072 F1: 97.21% dev loss: 0.1128 F1: 41.38% 1.54 min
> Epoch: 21 Step: 1600, train loss: 0.0059 F1: 99.06% dev loss: 0.1127 F1: 44.20% 1.54 min
> Epoch: 22 Step: 1700, train loss: 0.0053 F1: 98.78% dev loss: 0.1131 F1: 45.58% 1.54 min
> Epoch: 23 Step: 1800, train loss: 0.0045 F1: 98.90% dev loss: 0.1137 F1: 44.87% 1.54 min
> Epoch: 25 Step: 1900, train loss: 0.0043 F1: 99.76% dev loss: 0.1146 F1: 44.98% 1.55 min
> Epoch: 26 Step: 2000, train loss: 0.0043 F1: 99.21% dev loss: 0.1148 F1: 45.47% 1.54 min
Initializing ELMo

>>>>>>>>>>>>>>>>>>>>>2021-07-21 14:40:19.452893>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: elmo
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 3.0
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/elmo-lstm_res14_seed8.pth
>>> seed: 8
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 2000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: True
>>> demo: False
>>> file_path: {'train': '/content/data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': '/content/data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': '/content/data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0]
> Epoch: 1 Step: 100, train loss: 4.4527 F1: 9.43% dev loss: 1.2805 F1: 9.49% 1.51 min
> Epoch: 2 Step: 200, train loss: 0.9082 F1: 11.19% dev loss: 0.6235 F1: 12.47% 1.54 min
> Epoch: 3 Step: 300, train loss: 0.5333 F1: 23.01% dev loss: 0.4439 F1: 23.01% 1.53 min
> Epoch: 5 Step: 400, train loss: 0.3503 F1: 39.97% dev loss: 0.4037 F1: 33.53% 1.54 min
> Epoch: 6 Step: 500, train loss: 0.2032 F1: 60.52% dev loss: 0.3409 F1: 40.93% 1.54 min
> Epoch: 7 Step: 600, train loss: 0.1207 F1: 73.86% dev loss: 0.3360 F1: 47.78% 1.54 min
> Epoch: 9 Step: 700, train loss: 0.0616 F1: 92.95% dev loss: 0.3642 F1: 45.06% 1.54 min
> Epoch: 10 Step: 800, train loss: 0.0335 F1: 96.16% dev loss: 0.3656 F1: 45.00% 1.54 min
> Epoch: 11 Step: 900, train loss: 0.0151 F1: 99.08% dev loss: 0.3751 F1: 44.62% 1.54 min
> Epoch: 13 Step: 1000, train loss: 0.0088 F1: 99.78% dev loss: 0.3805 F1: 47.18% 1.54 min
> Epoch: 14 Step: 1100, train loss: 0.0062 F1: 99.91% dev loss: 0.3868 F1: 48.41% 1.54 min
> Epoch: 15 Step: 1200, train loss: 0.0040 F1: 99.99% dev loss: 0.3984 F1: 45.08% 1.54 min
> Epoch: 17 Step: 1300, train loss: 0.0035 F1: 100.00% dev loss: 0.3998 F1: 44.88% 1.54 min
> Epoch: 18 Step: 1400, train loss: 0.0030 F1: 100.00% dev loss: 0.4015 F1: 46.37% 1.54 min
> Epoch: 19 Step: 1500, train loss: 0.0023 F1: 100.00% dev loss: 0.4049 F1: 44.91% 1.54 min
> Epoch: 21 Step: 1600, train loss: 0.0021 F1: 100.00% dev loss: 0.4079 F1: 44.53% 1.54 min
> Epoch: 22 Step: 1700, train loss: 0.0021 F1: 100.00% dev loss: 0.4089 F1: 46.55% 1.54 min
> Epoch: 23 Step: 1800, train loss: 0.0018 F1: 99.97% dev loss: 0.4099 F1: 46.48% 1.54 min
> Epoch: 25 Step: 1900, train loss: 0.0017 F1: 100.00% dev loss: 0.4112 F1: 44.78% 1.54 min
> Epoch: 26 Step: 2000, train loss: 0.0018 F1: 100.00% dev loss: 0.4119 F1: 46.44% 1.54 min

Lock 140192296948560 acquired on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock
Lock 140192296948560 released on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock
Lock 140192296949008 acquired on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock
Lock 140192296949008 released on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock
Lock 140192296948240 acquired on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock
Lock 140192296948240 released on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock
Lock 140192296947856 acquired on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock
Lock 140192296947856 released on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock
Lock 140192178700560 acquired on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock
Lock 140192178700560 released on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock

>>>>>>>>>>>>>>>>>>>>>2021-07-21 01:55:43.538770>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: CE
>>> gamma: 3.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 2000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: True
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.1, 0.8, 1.0, 1.0, 1.2, 1.2, 1.2, 1.0, 1.0, 1.0]
> Epoch: 1 Step: 100, train loss: 1.9242 F1: 9.43% dev loss: 1.5794 F1: 9.49% 1.15 min
> Epoch: 2 Step: 200, train loss: 1.3944 F1: 19.90% dev loss: 1.0735 F1: 24.66% 1.20 min
> Epoch: 3 Step: 300, train loss: 0.8649 F1: 32.09% dev loss: 0.8087 F1: 29.35% 1.23 min
> Epoch: 5 Step: 400, train loss: 0.6066 F1: 55.65% dev loss: 0.8701 F1: 34.57% 1.25 min
> Epoch: 6 Step: 500, train loss: 0.4421 F1: 62.56% dev loss: 0.6981 F1: 46.54% 1.26 min
> Epoch: 7 Step: 600, train loss: 0.3198 F1: 74.15% dev loss: 0.7858 F1: 46.16% 1.26 min
> Epoch: 9 Step: 700, train loss: 0.2484 F1: 86.29% dev loss: 0.8147 F1: 53.90% 1.26 min
> Epoch: 10 Step: 800, train loss: 0.1816 F1: 90.71% dev loss: 0.8929 F1: 55.44% 1.26 min
> Epoch: 11 Step: 900, train loss: 0.1071 F1: 93.49% dev loss: 0.8618 F1: 52.07% 1.26 min
> Epoch: 13 Step: 1000, train loss: 0.0707 F1: 97.20% dev loss: 0.8570 F1: 51.84% 1.26 min
> Epoch: 14 Step: 1100, train loss: 0.0573 F1: 96.68% dev loss: 0.9244 F1: 51.10% 1.26 min
> Epoch: 15 Step: 1200, train loss: 0.0436 F1: 97.07% dev loss: 0.9917 F1: 53.30% 1.26 min
> Epoch: 17 Step: 1300, train loss: 0.0354 F1: 98.77% dev loss: 1.0203 F1: 52.87% 1.26 min
> Epoch: 18 Step: 1400, train loss: 0.0322 F1: 97.36% dev loss: 1.0086 F1: 51.48% 1.26 min
> Epoch: 19 Step: 1500, train loss: 0.0268 F1: 98.22% dev loss: 1.0195 F1: 54.18% 1.26 min
> Epoch: 21 Step: 1600, train loss: 0.0221 F1: 98.96% dev loss: 1.0574 F1: 54.45% 1.26 min
> Epoch: 22 Step: 1700, train loss: 0.0211 F1: 98.52% dev loss: 1.0496 F1: 54.70% 1.26 min
> Epoch: 23 Step: 1800, train loss: 0.0186 F1: 98.81% dev loss: 1.0573 F1: 56.14% 1.26 min
> Epoch: 25 Step: 1900, train loss: 0.0160 F1: 99.07% dev loss: 1.0710 F1: 56.37% 1.26 min
> Epoch: 26 Step: 2000, train loss: 0.0161 F1: 98.73% dev loss: 1.0738 F1: 56.32% 1.26 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 02:22:18.767396>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 1e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 2000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: True
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 1e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 1.0045 F1: 9.43% dev loss: 0.3728 F1: 9.49% 1.19 min
> Epoch: 2 Step: 200, train loss: 0.2575 F1: 9.43% dev loss: 0.1908 F1: 9.49% 1.22 min
> Epoch: 3 Step: 300, train loss: 0.1983 F1: 9.45% dev loss: 0.1747 F1: 9.49% 1.25 min
> Epoch: 5 Step: 400, train loss: 0.1864 F1: 9.43% dev loss: 0.1638 F1: 9.49% 1.26 min
> Epoch: 6 Step: 500, train loss: 0.1730 F1: 9.43% dev loss: 0.1432 F1: 9.49% 1.25 min
> Epoch: 7 Step: 600, train loss: 0.1330 F1: 16.66% dev loss: 0.1129 F1: 18.49% 1.26 min
> Epoch: 9 Step: 700, train loss: 0.1090 F1: 26.44% dev loss: 0.1033 F1: 23.07% 1.26 min
> Epoch: 10 Step: 800, train loss: 0.0910 F1: 31.13% dev loss: 0.0925 F1: 27.94% 1.26 min
> Epoch: 11 Step: 900, train loss: 0.0745 F1: 34.43% dev loss: 0.0897 F1: 29.15% 1.26 min
> Epoch: 13 Step: 1000, train loss: 0.0676 F1: 36.96% dev loss: 0.0927 F1: 31.06% 1.26 min
> Epoch: 14 Step: 1100, train loss: 0.0593 F1: 42.84% dev loss: 0.0963 F1: 34.47% 1.26 min
> Epoch: 15 Step: 1200, train loss: 0.0501 F1: 51.05% dev loss: 0.0852 F1: 36.23% 1.26 min
> Epoch: 17 Step: 1300, train loss: 0.0452 F1: 58.49% dev loss: 0.0846 F1: 36.49% 1.26 min
> Epoch: 18 Step: 1400, train loss: 0.0413 F1: 59.17% dev loss: 0.0818 F1: 36.76% 1.26 min
> Epoch: 19 Step: 1500, train loss: 0.0362 F1: 60.28% dev loss: 0.0792 F1: 38.37% 1.26 min
> Epoch: 21 Step: 1600, train loss: 0.0346 F1: 63.23% dev loss: 0.0860 F1: 39.22% 1.26 min
> Epoch: 22 Step: 1700, train loss: 0.0332 F1: 62.42% dev loss: 0.0797 F1: 38.52% 1.26 min
> Epoch: 23 Step: 1800, train loss: 0.0295 F1: 65.97% dev loss: 0.0784 F1: 39.25% 1.26 min
> Epoch: 25 Step: 1900, train loss: 0.0298 F1: 66.79% dev loss: 0.0790 F1: 41.22% 1.26 min
> Epoch: 26 Step: 2000, train loss: 0.0293 F1: 67.51% dev loss: 0.0791 F1: 39.60% 1.26 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 02:51:27.462908>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 4e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 2000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: True
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 4e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.6485 F1: 9.43% dev loss: 0.1945 F1: 9.49% 1.16 min
> Epoch: 2 Step: 200, train loss: 0.2002 F1: 9.43% dev loss: 0.1682 F1: 9.49% 1.20 min
> Epoch: 3 Step: 300, train loss: 0.1641 F1: 14.54% dev loss: 0.1272 F1: 16.82% 1.23 min
> Epoch: 5 Step: 400, train loss: 0.1144 F1: 26.69% dev loss: 0.0962 F1: 28.87% 1.25 min
> Epoch: 6 Step: 500, train loss: 0.0804 F1: 36.31% dev loss: 0.0830 F1: 38.57% 1.26 min
> Epoch: 7 Step: 600, train loss: 0.0530 F1: 53.29% dev loss: 0.0838 F1: 38.16% 1.26 min
> Epoch: 9 Step: 700, train loss: 0.0343 F1: 71.76% dev loss: 0.0930 F1: 37.97% 1.26 min
> Epoch: 10 Step: 800, train loss: 0.0236 F1: 80.05% dev loss: 0.0971 F1: 37.83% 1.26 min
> Epoch: 11 Step: 900, train loss: 0.0166 F1: 83.71% dev loss: 0.0870 F1: 37.94% 1.26 min
> Epoch: 13 Step: 1000, train loss: 0.0122 F1: 89.24% dev loss: 0.0839 F1: 51.61% 1.26 min
> Epoch: 14 Step: 1100, train loss: 0.0092 F1: 91.05% dev loss: 0.0872 F1: 52.69% 1.26 min
> Epoch: 15 Step: 1200, train loss: 0.0066 F1: 93.80% dev loss: 0.0848 F1: 48.90% 1.26 min
> Epoch: 17 Step: 1300, train loss: 0.0047 F1: 96.43% dev loss: 0.0886 F1: 54.67% 1.26 min
> Epoch: 18 Step: 1400, train loss: 0.0037 F1: 96.38% dev loss: 0.0951 F1: 51.45% 1.26 min
> Epoch: 19 Step: 1500, train loss: 0.0031 F1: 97.68% dev loss: 0.0912 F1: 56.23% 1.26 min
> Epoch: 21 Step: 1600, train loss: 0.0023 F1: 98.28% dev loss: 0.0919 F1: 53.74% 1.26 min
> Epoch: 22 Step: 1700, train loss: 0.0025 F1: 98.27% dev loss: 0.0948 F1: 53.05% 1.26 min
> Epoch: 23 Step: 1800, train loss: 0.0019 F1: 98.68% dev loss: 0.0956 F1: 54.59% 1.26 min
> Epoch: 25 Step: 1900, train loss: 0.0018 F1: 98.11% dev loss: 0.0950 F1: 56.29% 1.26 min
> Epoch: 26 Step: 2000, train loss: 0.0018 F1: 98.76% dev loss: 0.0957 F1: 55.83% 1.26 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 03:19:19.831149>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 4e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 1.0
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 2000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: True
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 4e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
> Epoch: 1 Step: 100, train loss: 0.9992 F1: 9.43% dev loss: 0.3075 F1: 9.49% 1.17 min
> Epoch: 2 Step: 200, train loss: 0.3168 F1: 9.43% dev loss: 0.2718 F1: 9.49% 1.20 min
> Epoch: 3 Step: 300, train loss: 0.2600 F1: 14.47% dev loss: 0.1909 F1: 13.73% 1.23 min
> Epoch: 5 Step: 400, train loss: 0.1780 F1: 27.28% dev loss: 0.1520 F1: 26.53% 1.25 min
> Epoch: 6 Step: 500, train loss: 0.1254 F1: 36.47% dev loss: 0.1388 F1: 31.50% 1.26 min
> Epoch: 7 Step: 600, train loss: 0.0842 F1: 50.46% dev loss: 0.1281 F1: 35.75% 1.26 min
> Epoch: 9 Step: 700, train loss: 0.0593 F1: 70.53% dev loss: 0.1316 F1: 39.94% 1.26 min
> Epoch: 10 Step: 800, train loss: 0.0381 F1: 79.19% dev loss: 0.1433 F1: 36.39% 1.26 min
> Epoch: 11 Step: 900, train loss: 0.0295 F1: 84.07% dev loss: 0.1361 F1: 40.05% 1.26 min
> Epoch: 13 Step: 1000, train loss: 0.0193 F1: 91.72% dev loss: 0.1364 F1: 39.79% 1.26 min
> Epoch: 14 Step: 1100, train loss: 0.0142 F1: 92.41% dev loss: 0.1411 F1: 55.08% 1.26 min
> Epoch: 15 Step: 1200, train loss: 0.0103 F1: 93.69% dev loss: 0.1494 F1: 48.37% 1.26 min
> Epoch: 17 Step: 1300, train loss: 0.0081 F1: 97.56% dev loss: 0.1516 F1: 53.93% 1.25 min
> Epoch: 18 Step: 1400, train loss: 0.0065 F1: 97.19% dev loss: 0.1521 F1: 53.63% 1.26 min
> Epoch: 19 Step: 1500, train loss: 0.0050 F1: 97.87% dev loss: 0.1548 F1: 54.00% 1.26 min
> Epoch: 21 Step: 1600, train loss: 0.0039 F1: 98.81% dev loss: 0.1536 F1: 55.45% 1.26 min
> Epoch: 22 Step: 1700, train loss: 0.0039 F1: 98.76% dev loss: 0.1551 F1: 50.55% 1.26 min
> Epoch: 23 Step: 1800, train loss: 0.0031 F1: 99.01% dev loss: 0.1576 F1: 51.37% 1.26 min
> Epoch: 25 Step: 1900, train loss: 0.0028 F1: 98.95% dev loss: 0.1568 F1: 55.49% 1.26 min
> Epoch: 26 Step: 2000, train loss: 0.0028 F1: 99.18% dev loss: 0.1578 F1: 53.52% 1.26 min

Lock 140487534695376 acquired on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock
Lock 140487534695376 released on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock
Lock 140487534695824 acquired on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock
Lock 140487534695824 released on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock
Lock 140487534698320 acquired on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock
Lock 140487534698320 released on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock
Lock 140487534694800 acquired on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock
Lock 140487534694800 released on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock
Lock 140487534755600 acquired on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock
Lock 140487534755600 released on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock

>>>>>>>>>>>>>>>>>>>>>2021-07-21 05:18:53.535556>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 1.0
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 2000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: True
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
> Epoch: 1 Step: 100, train loss: 0.8037 F1: 9.43% dev loss: 0.2480 F1: 9.49% 1.27 min
> Epoch: 2 Step: 200, train loss: 0.2606 F1: 9.43% dev loss: 0.2208 F1: 9.49% 1.42 min
> Epoch: 3 Step: 300, train loss: 0.2002 F1: 15.60% dev loss: 0.1496 F1: 17.37% 1.41 min
> Epoch: 5 Step: 400, train loss: 0.1326 F1: 30.52% dev loss: 0.1229 F1: 30.74% 1.41 min
> Epoch: 6 Step: 500, train loss: 0.0878 F1: 50.14% dev loss: 0.1110 F1: 35.38% 1.41 min
> Epoch: 7 Step: 600, train loss: 0.0566 F1: 60.44% dev loss: 0.1053 F1: 40.36% 1.40 min
> Epoch: 9 Step: 700, train loss: 0.0374 F1: 80.94% dev loss: 0.1050 F1: 41.28% 1.41 min
> Epoch: 10 Step: 800, train loss: 0.0242 F1: 86.38% dev loss: 0.1103 F1: 53.09% 1.41 min
> Epoch: 11 Step: 900, train loss: 0.0144 F1: 89.86% dev loss: 0.1015 F1: 53.37% 1.41 min
> Epoch: 13 Step: 1000, train loss: 0.0094 F1: 95.30% dev loss: 0.1072 F1: 50.53% 1.41 min
> Epoch: 14 Step: 1100, train loss: 0.0066 F1: 95.65% dev loss: 0.1170 F1: 51.33% 1.41 min
> Epoch: 15 Step: 1200, train loss: 0.0048 F1: 96.25% dev loss: 0.1144 F1: 55.11% 1.40 min
> Epoch: 17 Step: 1300, train loss: 0.0036 F1: 98.01% dev loss: 0.1155 F1: 55.00% 1.41 min
> Epoch: 18 Step: 1400, train loss: 0.0031 F1: 97.63% dev loss: 0.1216 F1: 54.13% 1.41 min
> Epoch: 19 Step: 1500, train loss: 0.0024 F1: 98.51% dev loss: 0.1190 F1: 50.26% 1.41 min
> Epoch: 21 Step: 1600, train loss: 0.0017 F1: 99.40% dev loss: 0.1194 F1: 50.04% 1.41 min
> Epoch: 22 Step: 1700, train loss: 0.0020 F1: 98.71% dev loss: 0.1223 F1: 53.57% 1.40 min
> Epoch: 23 Step: 1800, train loss: 0.0017 F1: 98.99% dev loss: 0.1193 F1: 54.15% 1.41 min
> Epoch: 25 Step: 1900, train loss: 0.0013 F1: 99.68% dev loss: 0.1195 F1: 52.38% 1.41 min
> Epoch: 26 Step: 2000, train loss: 0.0015 F1: 99.39% dev loss: 0.1197 F1: 53.20% 1.41 min

Lock 140192296948560 acquired on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock
Lock 140192296948560 released on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock
Lock 140192296949008 acquired on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock
Lock 140192296949008 released on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock
Lock 140192296948240 acquired on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock
Lock 140192296948240 released on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock
Lock 140192296947856 acquired on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock
Lock 140192296947856 released on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock
Lock 140192178700560 acquired on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock
Lock 140192178700560 released on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock

>>>>>>>>>>>>>>>>>>>>>2021-07-21 01:55:43.538770>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: CE
>>> gamma: 3.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 2000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: True
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.1, 0.8, 1.0, 1.0, 1.2, 1.2, 1.2, 1.0, 1.0, 1.0]
> Epoch: 1 Step: 100, train loss: 1.9242 F1: 9.43% dev loss: 1.5794 F1: 9.49% 1.15 min
> Epoch: 2 Step: 200, train loss: 1.3944 F1: 19.90% dev loss: 1.0735 F1: 24.66% 1.20 min
> Epoch: 3 Step: 300, train loss: 0.8649 F1: 32.09% dev loss: 0.8087 F1: 29.35% 1.23 min
> Epoch: 5 Step: 400, train loss: 0.6066 F1: 55.65% dev loss: 0.8701 F1: 34.57% 1.25 min
> Epoch: 6 Step: 500, train loss: 0.4421 F1: 62.56% dev loss: 0.6981 F1: 46.54% 1.26 min
> Epoch: 7 Step: 600, train loss: 0.3198 F1: 74.15% dev loss: 0.7858 F1: 46.16% 1.26 min
> Epoch: 9 Step: 700, train loss: 0.2484 F1: 86.29% dev loss: 0.8147 F1: 53.90% 1.26 min
> Epoch: 10 Step: 800, train loss: 0.1816 F1: 90.71% dev loss: 0.8929 F1: 55.44% 1.26 min
> Epoch: 11 Step: 900, train loss: 0.1071 F1: 93.49% dev loss: 0.8618 F1: 52.07% 1.26 min
> Epoch: 13 Step: 1000, train loss: 0.0707 F1: 97.20% dev loss: 0.8570 F1: 51.84% 1.26 min
> Epoch: 14 Step: 1100, train loss: 0.0573 F1: 96.68% dev loss: 0.9244 F1: 51.10% 1.26 min
> Epoch: 15 Step: 1200, train loss: 0.0436 F1: 97.07% dev loss: 0.9917 F1: 53.30% 1.26 min
> Epoch: 17 Step: 1300, train loss: 0.0354 F1: 98.77% dev loss: 1.0203 F1: 52.87% 1.26 min
> Epoch: 18 Step: 1400, train loss: 0.0322 F1: 97.36% dev loss: 1.0086 F1: 51.48% 1.26 min
> Epoch: 19 Step: 1500, train loss: 0.0268 F1: 98.22% dev loss: 1.0195 F1: 54.18% 1.26 min
> Epoch: 21 Step: 1600, train loss: 0.0221 F1: 98.96% dev loss: 1.0574 F1: 54.45% 1.26 min
> Epoch: 22 Step: 1700, train loss: 0.0211 F1: 98.52% dev loss: 1.0496 F1: 54.70% 1.26 min
> Epoch: 23 Step: 1800, train loss: 0.0186 F1: 98.81% dev loss: 1.0573 F1: 56.14% 1.26 min
> Epoch: 25 Step: 1900, train loss: 0.0160 F1: 99.07% dev loss: 1.0710 F1: 56.37% 1.26 min
> Epoch: 26 Step: 2000, train loss: 0.0161 F1: 98.73% dev loss: 1.0738 F1: 56.32% 1.26 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 02:22:18.767396>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 1e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 2000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: True
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 1e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 1.0045 F1: 9.43% dev loss: 0.3728 F1: 9.49% 1.19 min
> Epoch: 2 Step: 200, train loss: 0.2575 F1: 9.43% dev loss: 0.1908 F1: 9.49% 1.22 min
> Epoch: 3 Step: 300, train loss: 0.1983 F1: 9.45% dev loss: 0.1747 F1: 9.49% 1.25 min
> Epoch: 5 Step: 400, train loss: 0.1864 F1: 9.43% dev loss: 0.1638 F1: 9.49% 1.26 min
> Epoch: 6 Step: 500, train loss: 0.1730 F1: 9.43% dev loss: 0.1432 F1: 9.49% 1.25 min
> Epoch: 7 Step: 600, train loss: 0.1330 F1: 16.66% dev loss: 0.1129 F1: 18.49% 1.26 min
> Epoch: 9 Step: 700, train loss: 0.1090 F1: 26.44% dev loss: 0.1033 F1: 23.07% 1.26 min
> Epoch: 10 Step: 800, train loss: 0.0910 F1: 31.13% dev loss: 0.0925 F1: 27.94% 1.26 min
> Epoch: 11 Step: 900, train loss: 0.0745 F1: 34.43% dev loss: 0.0897 F1: 29.15% 1.26 min
> Epoch: 13 Step: 1000, train loss: 0.0676 F1: 36.96% dev loss: 0.0927 F1: 31.06% 1.26 min
> Epoch: 14 Step: 1100, train loss: 0.0593 F1: 42.84% dev loss: 0.0963 F1: 34.47% 1.26 min
> Epoch: 15 Step: 1200, train loss: 0.0501 F1: 51.05% dev loss: 0.0852 F1: 36.23% 1.26 min
> Epoch: 17 Step: 1300, train loss: 0.0452 F1: 58.49% dev loss: 0.0846 F1: 36.49% 1.26 min
> Epoch: 18 Step: 1400, train loss: 0.0413 F1: 59.17% dev loss: 0.0818 F1: 36.76% 1.26 min
> Epoch: 19 Step: 1500, train loss: 0.0362 F1: 60.28% dev loss: 0.0792 F1: 38.37% 1.26 min
> Epoch: 21 Step: 1600, train loss: 0.0346 F1: 63.23% dev loss: 0.0860 F1: 39.22% 1.26 min
> Epoch: 22 Step: 1700, train loss: 0.0332 F1: 62.42% dev loss: 0.0797 F1: 38.52% 1.26 min
> Epoch: 23 Step: 1800, train loss: 0.0295 F1: 65.97% dev loss: 0.0784 F1: 39.25% 1.26 min
> Epoch: 25 Step: 1900, train loss: 0.0298 F1: 66.79% dev loss: 0.0790 F1: 41.22% 1.26 min
> Epoch: 26 Step: 2000, train loss: 0.0293 F1: 67.51% dev loss: 0.0791 F1: 39.60% 1.26 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 02:51:27.462908>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 4e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 2000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: True
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 4e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.6485 F1: 9.43% dev loss: 0.1945 F1: 9.49% 1.16 min
> Epoch: 2 Step: 200, train loss: 0.2002 F1: 9.43% dev loss: 0.1682 F1: 9.49% 1.20 min
> Epoch: 3 Step: 300, train loss: 0.1641 F1: 14.54% dev loss: 0.1272 F1: 16.82% 1.23 min
> Epoch: 5 Step: 400, train loss: 0.1144 F1: 26.69% dev loss: 0.0962 F1: 28.87% 1.25 min
> Epoch: 6 Step: 500, train loss: 0.0804 F1: 36.31% dev loss: 0.0830 F1: 38.57% 1.26 min
> Epoch: 7 Step: 600, train loss: 0.0530 F1: 53.29% dev loss: 0.0838 F1: 38.16% 1.26 min
> Epoch: 9 Step: 700, train loss: 0.0343 F1: 71.76% dev loss: 0.0930 F1: 37.97% 1.26 min
> Epoch: 10 Step: 800, train loss: 0.0236 F1: 80.05% dev loss: 0.0971 F1: 37.83% 1.26 min
> Epoch: 11 Step: 900, train loss: 0.0166 F1: 83.71% dev loss: 0.0870 F1: 37.94% 1.26 min
> Epoch: 13 Step: 1000, train loss: 0.0122 F1: 89.24% dev loss: 0.0839 F1: 51.61% 1.26 min
> Epoch: 14 Step: 1100, train loss: 0.0092 F1: 91.05% dev loss: 0.0872 F1: 52.69% 1.26 min
> Epoch: 15 Step: 1200, train loss: 0.0066 F1: 93.80% dev loss: 0.0848 F1: 48.90% 1.26 min
> Epoch: 17 Step: 1300, train loss: 0.0047 F1: 96.43% dev loss: 0.0886 F1: 54.67% 1.26 min
> Epoch: 18 Step: 1400, train loss: 0.0037 F1: 96.38% dev loss: 0.0951 F1: 51.45% 1.26 min
> Epoch: 19 Step: 1500, train loss: 0.0031 F1: 97.68% dev loss: 0.0912 F1: 56.23% 1.26 min
> Epoch: 21 Step: 1600, train loss: 0.0023 F1: 98.28% dev loss: 0.0919 F1: 53.74% 1.26 min
> Epoch: 22 Step: 1700, train loss: 0.0025 F1: 98.27% dev loss: 0.0948 F1: 53.05% 1.26 min
> Epoch: 23 Step: 1800, train loss: 0.0019 F1: 98.68% dev loss: 0.0956 F1: 54.59% 1.26 min
> Epoch: 25 Step: 1900, train loss: 0.0018 F1: 98.11% dev loss: 0.0950 F1: 56.29% 1.26 min
> Epoch: 26 Step: 2000, train loss: 0.0018 F1: 98.76% dev loss: 0.0957 F1: 55.83% 1.26 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 03:19:19.831149>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 4e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 1.0
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 2000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: True
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 4e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
> Epoch: 1 Step: 100, train loss: 0.9992 F1: 9.43% dev loss: 0.3075 F1: 9.49% 1.17 min
> Epoch: 2 Step: 200, train loss: 0.3168 F1: 9.43% dev loss: 0.2718 F1: 9.49% 1.20 min
> Epoch: 3 Step: 300, train loss: 0.2600 F1: 14.47% dev loss: 0.1909 F1: 13.73% 1.23 min
> Epoch: 5 Step: 400, train loss: 0.1780 F1: 27.28% dev loss: 0.1520 F1: 26.53% 1.25 min
> Epoch: 6 Step: 500, train loss: 0.1254 F1: 36.47% dev loss: 0.1388 F1: 31.50% 1.26 min
> Epoch: 7 Step: 600, train loss: 0.0842 F1: 50.46% dev loss: 0.1281 F1: 35.75% 1.26 min
> Epoch: 9 Step: 700, train loss: 0.0593 F1: 70.53% dev loss: 0.1316 F1: 39.94% 1.26 min
> Epoch: 10 Step: 800, train loss: 0.0381 F1: 79.19% dev loss: 0.1433 F1: 36.39% 1.26 min
> Epoch: 11 Step: 900, train loss: 0.0295 F1: 84.07% dev loss: 0.1361 F1: 40.05% 1.26 min
> Epoch: 13 Step: 1000, train loss: 0.0193 F1: 91.72% dev loss: 0.1364 F1: 39.79% 1.26 min
> Epoch: 14 Step: 1100, train loss: 0.0142 F1: 92.41% dev loss: 0.1411 F1: 55.08% 1.26 min
> Epoch: 15 Step: 1200, train loss: 0.0103 F1: 93.69% dev loss: 0.1494 F1: 48.37% 1.26 min
> Epoch: 17 Step: 1300, train loss: 0.0081 F1: 97.56% dev loss: 0.1516 F1: 53.93% 1.25 min
> Epoch: 18 Step: 1400, train loss: 0.0065 F1: 97.19% dev loss: 0.1521 F1: 53.63% 1.26 min
> Epoch: 19 Step: 1500, train loss: 0.0050 F1: 97.87% dev loss: 0.1548 F1: 54.00% 1.26 min
> Epoch: 21 Step: 1600, train loss: 0.0039 F1: 98.81% dev loss: 0.1536 F1: 55.45% 1.26 min
> Epoch: 22 Step: 1700, train loss: 0.0039 F1: 98.76% dev loss: 0.1551 F1: 50.55% 1.26 min
> Epoch: 23 Step: 1800, train loss: 0.0031 F1: 99.01% dev loss: 0.1576 F1: 51.37% 1.26 min
> Epoch: 25 Step: 1900, train loss: 0.0028 F1: 98.95% dev loss: 0.1568 F1: 55.49% 1.26 min
> Epoch: 26 Step: 2000, train loss: 0.0028 F1: 99.18% dev loss: 0.1578 F1: 53.52% 1.26 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 15:20:37.125221>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 600
>>> step: 200
>>> model_name: glove
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 2.0
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/glove-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 50000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0]
> Epoch: 2 Step: 200, train loss: 2.8296 F1: 9.82% dev loss: 2.5060 F1: 9.98% 0.04 min
> Epoch: 5 Step: 400, train loss: 1.8120 F1: 9.43% dev loss: 0.8519 F1: 9.49% 0.03 min
> Epoch: 7 Step: 600, train loss: 0.6250 F1: 9.45% dev loss: 0.4915 F1: 9.49% 0.03 min
> Epoch: 10 Step: 800, train loss: 0.5095 F1: 9.43% dev loss: 0.4396 F1: 9.49% 0.03 min
> Epoch: 13 Step: 1000, train loss: 0.4617 F1: 9.43% dev loss: 0.4140 F1: 9.49% 0.03 min
> Epoch: 15 Step: 1200, train loss: 0.4432 F1: 10.07% dev loss: 0.3976 F1: 9.49% 0.03 min
> Epoch: 18 Step: 1400, train loss: 0.4323 F1: 10.72% dev loss: 0.3851 F1: 10.29% 0.03 min
> Epoch: 21 Step: 1600, train loss: 0.4096 F1: 11.70% dev loss: 0.3750 F1: 10.87% 0.03 min
> Epoch: 23 Step: 1800, train loss: 0.4029 F1: 12.00% dev loss: 0.3667 F1: 11.38% 0.03 min
> Epoch: 26 Step: 2000, train loss: 0.3986 F1: 12.92% dev loss: 0.3595 F1: 12.71% 0.03 min
> Epoch: 29 Step: 2200, train loss: 0.3806 F1: 13.43% dev loss: 0.3525 F1: 13.01% 0.03 min
> Epoch: 31 Step: 2400, train loss: 0.3771 F1: 13.34% dev loss: 0.3462 F1: 13.20% 0.03 min
> Epoch: 34 Step: 2600, train loss: 0.3743 F1: 13.53% dev loss: 0.3401 F1: 13.13% 0.03 min
> Epoch: 37 Step: 2800, train loss: 0.3577 F1: 13.87% dev loss: 0.3338 F1: 13.26% 0.03 min
> Epoch: 39 Step: 3000, train loss: 0.3544 F1: 13.61% dev loss: 0.3277 F1: 13.35% 0.03 min
> Epoch: 42 Step: 3200, train loss: 0.3521 F1: 13.82% dev loss: 0.3218 F1: 13.78% 0.03 min
> Epoch: 45 Step: 3400, train loss: 0.3359 F1: 14.37% dev loss: 0.3154 F1: 14.00% 0.03 min
> Epoch: 47 Step: 3600, train loss: 0.3326 F1: 17.71% dev loss: 0.3093 F1: 14.20% 0.03 min
> Epoch: 50 Step: 3800, train loss: 0.3297 F1: 20.92% dev loss: 0.3039 F1: 16.67% 0.03 min
> Epoch: 53 Step: 4000, train loss: 0.3148 F1: 19.19% dev loss: 0.2984 F1: 16.22% 0.03 min
> Epoch: 55 Step: 4200, train loss: 0.3113 F1: 22.29% dev loss: 0.2929 F1: 17.21% 0.03 min
> Epoch: 58 Step: 4400, train loss: 0.3093 F1: 21.92% dev loss: 0.2886 F1: 19.99% 0.03 min
> Epoch: 61 Step: 4600, train loss: 0.2958 F1: 22.72% dev loss: 0.2839 F1: 19.99% 0.03 min
> Epoch: 63 Step: 4800, train loss: 0.2939 F1: 25.13% dev loss: 0.2793 F1: 21.11% 0.03 min
> Epoch: 66 Step: 5000, train loss: 0.2922 F1: 24.83% dev loss: 0.2757 F1: 23.05% 0.03 min
> Epoch: 69 Step: 5200, train loss: 0.2802 F1: 26.03% dev loss: 0.2717 F1: 22.89% 0.03 min
> Epoch: 71 Step: 5400, train loss: 0.2787 F1: 26.27% dev loss: 0.2678 F1: 24.38% 0.03 min
> Epoch: 74 Step: 5600, train loss: 0.2772 F1: 26.44% dev loss: 0.2649 F1: 25.00% 0.03 min
> Epoch: 77 Step: 5800, train loss: 0.2658 F1: 26.77% dev loss: 0.2613 F1: 24.88% 0.03 min
> Epoch: 79 Step: 6000, train loss: 0.2651 F1: 27.78% dev loss: 0.2581 F1: 25.50% 0.03 min
> Epoch: 82 Step: 6200, train loss: 0.2637 F1: 27.47% dev loss: 0.2560 F1: 26.13% 0.03 min
> Epoch: 85 Step: 6400, train loss: 0.2531 F1: 26.94% dev loss: 0.2525 F1: 26.10% 0.03 min
> Epoch: 87 Step: 6600, train loss: 0.2525 F1: 28.93% dev loss: 0.2499 F1: 26.76% 0.03 min
> Epoch: 90 Step: 6800, train loss: 0.2506 F1: 28.38% dev loss: 0.2485 F1: 26.41% 0.03 min
> Epoch: 93 Step: 7000, train loss: 0.2414 F1: 29.06% dev loss: 0.2453 F1: 27.25% 0.03 min
> Epoch: 95 Step: 7200, train loss: 0.2418 F1: 30.16% dev loss: 0.2435 F1: 27.68% 0.03 min
> Epoch: 98 Step: 7400, train loss: 0.2406 F1: 29.47% dev loss: 0.2425 F1: 29.31% 0.03 min
> Epoch: 101 Step: 7600, train loss: 0.2313 F1: 30.18% dev loss: 0.2397 F1: 28.24% 0.03 min
> Epoch: 103 Step: 7800, train loss: 0.2319 F1: 31.02% dev loss: 0.2380 F1: 29.30% 0.03 min
> Epoch: 106 Step: 8000, train loss: 0.2309 F1: 30.91% dev loss: 0.2372 F1: 29.43% 0.03 min
> Epoch: 109 Step: 8200, train loss: 0.2233 F1: 30.03% dev loss: 0.2356 F1: 29.80% 0.03 min
> Epoch: 111 Step: 8400, train loss: 0.2235 F1: 30.88% dev loss: 0.2342 F1: 30.33% 0.03 min
> Epoch: 114 Step: 8600, train loss: 0.2219 F1: 31.13% dev loss: 0.2337 F1: 30.20% 0.03 min
> Epoch: 117 Step: 8800, train loss: 0.2150 F1: 34.99% dev loss: 0.2322 F1: 30.89% 0.03 min
> Epoch: 119 Step: 9000, train loss: 0.2159 F1: 32.33% dev loss: 0.2306 F1: 30.94% 0.03 min
> Epoch: 122 Step: 9200, train loss: 0.2145 F1: 35.63% dev loss: 0.2307 F1: 30.11% 0.03 min
> Epoch: 125 Step: 9400, train loss: 0.2079 F1: 37.17% dev loss: 0.2294 F1: 31.41% 0.03 min
> Epoch: 127 Step: 9600, train loss: 0.2095 F1: 34.17% dev loss: 0.2286 F1: 31.13% 0.03 min
> Epoch: 130 Step: 9800, train loss: 0.2076 F1: 40.08% dev loss: 0.2281 F1: 31.32% 0.03 min
> Epoch: 133 Step: 10000, train loss: 0.2013 F1: 38.79% dev loss: 0.2266 F1: 31.39% 0.03 min
> Epoch: 135 Step: 10200, train loss: 0.2019 F1: 40.37% dev loss: 0.2254 F1: 30.54% 0.03 min
> Epoch: 138 Step: 10400, train loss: 0.2006 F1: 41.47% dev loss: 0.2250 F1: 31.66% 0.03 min
> Epoch: 141 Step: 10600, train loss: 0.1966 F1: 42.17% dev loss: 0.2241 F1: 31.32% 0.03 min
> Epoch: 143 Step: 10800, train loss: 0.1966 F1: 41.40% dev loss: 0.2230 F1: 31.42% 0.03 min
> Epoch: 146 Step: 11000, train loss: 0.1958 F1: 40.71% dev loss: 0.2229 F1: 32.08% 0.03 min
> Epoch: 149 Step: 11200, train loss: 0.1898 F1: 39.75% dev loss: 0.2214 F1: 31.52% 0.03 min
> Epoch: 151 Step: 11400, train loss: 0.1922 F1: 42.28% dev loss: 0.2196 F1: 31.74% 0.03 min
> Epoch: 154 Step: 11600, train loss: 0.1895 F1: 42.14% dev loss: 0.2204 F1: 33.28% 0.03 min
> Epoch: 157 Step: 11800, train loss: 0.1863 F1: 44.66% dev loss: 0.2196 F1: 32.24% 0.03 min
> Epoch: 159 Step: 12000, train loss: 0.1864 F1: 43.23% dev loss: 0.2191 F1: 33.18% 0.03 min
> Epoch: 162 Step: 12200, train loss: 0.1851 F1: 42.94% dev loss: 0.2185 F1: 33.35% 0.03 min
> Epoch: 165 Step: 12400, train loss: 0.1802 F1: 41.50% dev loss: 0.2183 F1: 32.55% 0.03 min
> Epoch: 167 Step: 12600, train loss: 0.1811 F1: 43.93% dev loss: 0.2181 F1: 33.23% 0.03 min
> Epoch: 170 Step: 12800, train loss: 0.1794 F1: 45.06% dev loss: 0.2168 F1: 33.72% 0.03 min
> Epoch: 173 Step: 13000, train loss: 0.1762 F1: 44.40% dev loss: 0.2171 F1: 33.48% 0.03 min
> Epoch: 175 Step: 13200, train loss: 0.1756 F1: 45.49% dev loss: 0.2165 F1: 33.56% 0.03 min
> Epoch: 178 Step: 13400, train loss: 0.1765 F1: 44.98% dev loss: 0.2168 F1: 33.48% 0.03 min
> Epoch: 181 Step: 13600, train loss: 0.1721 F1: 46.68% dev loss: 0.2158 F1: 33.67% 0.03 min
> Epoch: 183 Step: 13800, train loss: 0.1732 F1: 46.41% dev loss: 0.2154 F1: 33.97% 0.03 min
> Epoch: 186 Step: 14000, train loss: 0.1719 F1: 45.76% dev loss: 0.2143 F1: 36.37% 0.03 min
> Epoch: 189 Step: 14200, train loss: 0.1677 F1: 47.64% dev loss: 0.2141 F1: 33.71% 0.03 min
> Epoch: 191 Step: 14400, train loss: 0.1684 F1: 45.90% dev loss: 0.2141 F1: 34.05% 0.03 min
> Epoch: 194 Step: 14600, train loss: 0.1676 F1: 47.14% dev loss: 0.2131 F1: 36.33% 0.03 min
> Epoch: 197 Step: 14800, train loss: 0.1638 F1: 49.60% dev loss: 0.2127 F1: 36.73% 0.03 min
> Epoch: 199 Step: 15000, train loss: 0.1646 F1: 47.33% dev loss: 0.2123 F1: 37.02% 0.03 min
> Epoch: 202 Step: 15200, train loss: 0.1635 F1: 48.65% dev loss: 0.2117 F1: 36.67% 0.03 min
> Epoch: 205 Step: 15400, train loss: 0.1605 F1: 48.57% dev loss: 0.2135 F1: 37.19% 0.03 min
> Epoch: 207 Step: 15600, train loss: 0.1608 F1: 47.83% dev loss: 0.2123 F1: 37.01% 0.03 min
> Epoch: 210 Step: 15800, train loss: 0.1613 F1: 49.74% dev loss: 0.2113 F1: 36.94% 0.03 min
> Epoch: 213 Step: 16000, train loss: 0.1575 F1: 50.63% dev loss: 0.2117 F1: 37.08% 0.03 min
> Epoch: 215 Step: 16200, train loss: 0.1571 F1: 47.48% dev loss: 0.2108 F1: 37.53% 0.03 min
> Epoch: 218 Step: 16400, train loss: 0.1579 F1: 48.90% dev loss: 0.2109 F1: 36.90% 0.03 min
> Epoch: 221 Step: 16600, train loss: 0.1539 F1: 51.42% dev loss: 0.2106 F1: 37.42% 0.03 min
> Epoch: 223 Step: 16800, train loss: 0.1551 F1: 50.18% dev loss: 0.2093 F1: 38.58% 0.03 min
> Epoch: 226 Step: 17000, train loss: 0.1562 F1: 50.72% dev loss: 0.2096 F1: 38.09% 0.03 min
> Epoch: 229 Step: 17200, train loss: 0.1513 F1: 51.19% dev loss: 0.2104 F1: 37.51% 0.03 min
> Epoch: 231 Step: 17400, train loss: 0.1527 F1: 50.56% dev loss: 0.2091 F1: 38.94% 0.03 min
> Epoch: 234 Step: 17600, train loss: 0.1511 F1: 52.06% dev loss: 0.2104 F1: 38.06% 0.03 min
> Epoch: 237 Step: 17800, train loss: 0.1478 F1: 50.11% dev loss: 0.2099 F1: 37.17% 0.03 min
> Epoch: 239 Step: 18000, train loss: 0.1494 F1: 51.37% dev loss: 0.2085 F1: 38.87% 0.03 min
> Epoch: 242 Step: 18200, train loss: 0.1496 F1: 51.82% dev loss: 0.2082 F1: 38.02% 0.03 min
> Epoch: 245 Step: 18400, train loss: 0.1462 F1: 52.52% dev loss: 0.2084 F1: 38.68% 0.03 min
> Epoch: 247 Step: 18600, train loss: 0.1456 F1: 54.42% dev loss: 0.2080 F1: 38.80% 0.03 min
> Epoch: 250 Step: 18800, train loss: 0.1451 F1: 54.76% dev loss: 0.2082 F1: 38.72% 0.03 min
> Epoch: 253 Step: 19000, train loss: 0.1426 F1: 55.31% dev loss: 0.2090 F1: 38.68% 0.03 min
> Epoch: 255 Step: 19200, train loss: 0.1442 F1: 54.38% dev loss: 0.2071 F1: 38.95% 0.03 min
> Epoch: 258 Step: 19400, train loss: 0.1436 F1: 55.16% dev loss: 0.2073 F1: 38.43% 0.03 min
> Epoch: 261 Step: 19600, train loss: 0.1414 F1: 52.93% dev loss: 0.2074 F1: 38.60% 0.03 min
> Epoch: 263 Step: 19800, train loss: 0.1407 F1: 52.17% dev loss: 0.2068 F1: 38.73% 0.03 min
> Epoch: 266 Step: 20000, train loss: 0.1413 F1: 52.34% dev loss: 0.2070 F1: 38.73% 0.03 min
> Epoch: 269 Step: 20200, train loss: 0.1390 F1: 53.37% dev loss: 0.2073 F1: 38.21% 0.03 min
> Epoch: 271 Step: 20400, train loss: 0.1390 F1: 55.59% dev loss: 0.2063 F1: 38.34% 0.03 min
> Epoch: 274 Step: 20600, train loss: 0.1384 F1: 56.57% dev loss: 0.2059 F1: 38.59% 0.03 min
> Epoch: 277 Step: 20800, train loss: 0.1361 F1: 54.27% dev loss: 0.2079 F1: 38.41% 0.03 min
> Epoch: 279 Step: 21000, train loss: 0.1371 F1: 55.08% dev loss: 0.2065 F1: 38.36% 0.03 min
> Epoch: 282 Step: 21200, train loss: 0.1358 F1: 56.41% dev loss: 0.2072 F1: 38.56% 0.03 min
> Epoch: 285 Step: 21400, train loss: 0.1340 F1: 56.38% dev loss: 0.2076 F1: 38.26% 0.03 min
> Epoch: 287 Step: 21600, train loss: 0.1352 F1: 54.70% dev loss: 0.2067 F1: 38.49% 0.03 min
> Epoch: 290 Step: 21800, train loss: 0.1344 F1: 59.38% dev loss: 0.2058 F1: 37.32% 0.03 min
> Epoch: 293 Step: 22000, train loss: 0.1318 F1: 55.28% dev loss: 0.2081 F1: 38.51% 0.03 min
> Epoch: 295 Step: 22200, train loss: 0.1318 F1: 58.91% dev loss: 0.2063 F1: 37.69% 0.03 min
> Epoch: 298 Step: 22400, train loss: 0.1340 F1: 57.95% dev loss: 0.2080 F1: 37.23% 0.03 min
> Epoch: 301 Step: 22600, train loss: 0.1296 F1: 55.88% dev loss: 0.2082 F1: 38.35% 0.03 min
> Epoch: 303 Step: 22800, train loss: 0.1311 F1: 57.16% dev loss: 0.2073 F1: 36.86% 0.03 min
> Epoch: 306 Step: 23000, train loss: 0.1296 F1: 58.67% dev loss: 0.2074 F1: 37.54% 0.03 min
> Epoch: 309 Step: 23200, train loss: 0.1281 F1: 59.65% dev loss: 0.2068 F1: 37.40% 0.03 min
> Epoch: 311 Step: 23400, train loss: 0.1282 F1: 59.47% dev loss: 0.2060 F1: 36.79% 0.03 min
> Epoch: 314 Step: 23600, train loss: 0.1280 F1: 60.36% dev loss: 0.2069 F1: 37.52% 0.03 min
> Epoch: 317 Step: 23800, train loss: 0.1269 F1: 57.11% dev loss: 0.2079 F1: 36.85% 0.03 min
> Epoch: 319 Step: 24000, train loss: 0.1271 F1: 59.08% dev loss: 0.2071 F1: 36.97% 0.03 min
> Epoch: 322 Step: 24200, train loss: 0.1258 F1: 59.63% dev loss: 0.2069 F1: 39.99% 0.03 min
> Epoch: 325 Step: 24400, train loss: 0.1250 F1: 61.75% dev loss: 0.2073 F1: 37.37% 0.03 min
> Epoch: 327 Step: 24600, train loss: 0.1249 F1: 58.93% dev loss: 0.2064 F1: 36.62% 0.03 min
> Epoch: 330 Step: 24800, train loss: 0.1252 F1: 59.58% dev loss: 0.2064 F1: 37.34% 0.03 min
> Epoch: 333 Step: 25000, train loss: 0.1231 F1: 61.63% dev loss: 0.2072 F1: 36.55% 0.03 min
> Epoch: 335 Step: 25200, train loss: 0.1236 F1: 60.41% dev loss: 0.2085 F1: 36.53% 0.03 min
> Epoch: 338 Step: 25400, train loss: 0.1246 F1: 58.42% dev loss: 0.2066 F1: 39.99% 0.03 min
> Epoch: 341 Step: 25600, train loss: 0.1223 F1: 59.45% dev loss: 0.2073 F1: 36.62% 0.03 min
> Epoch: 343 Step: 25800, train loss: 0.1216 F1: 60.43% dev loss: 0.2076 F1: 36.26% 0.03 min
> Epoch: 346 Step: 26000, train loss: 0.1214 F1: 60.81% dev loss: 0.2067 F1: 40.23% 0.03 min
> Epoch: 349 Step: 26200, train loss: 0.1199 F1: 60.07% dev loss: 0.2071 F1: 36.76% 0.03 min
> Epoch: 351 Step: 26400, train loss: 0.1198 F1: 59.82% dev loss: 0.2064 F1: 36.57% 0.03 min
> Epoch: 354 Step: 26600, train loss: 0.1190 F1: 62.50% dev loss: 0.2058 F1: 39.40% 0.03 min
> Epoch: 357 Step: 26800, train loss: 0.1175 F1: 57.66% dev loss: 0.2077 F1: 36.51% 0.03 min
> Epoch: 359 Step: 27000, train loss: 0.1193 F1: 61.26% dev loss: 0.2059 F1: 36.48% 0.03 min
> Epoch: 362 Step: 27200, train loss: 0.1183 F1: 62.87% dev loss: 0.2060 F1: 36.62% 0.03 min
> Epoch: 365 Step: 27400, train loss: 0.1161 F1: 59.79% dev loss: 0.2055 F1: 37.30% 0.03 min
> Epoch: 367 Step: 27600, train loss: 0.1168 F1: 62.80% dev loss: 0.2067 F1: 36.59% 0.03 min
> Epoch: 370 Step: 27800, train loss: 0.1167 F1: 61.12% dev loss: 0.2060 F1: 39.27% 0.03 min
> Epoch: 373 Step: 28000, train loss: 0.1150 F1: 60.95% dev loss: 0.2071 F1: 36.55% 0.03 min
> Epoch: 375 Step: 28200, train loss: 0.1182 F1: 61.83% dev loss: 0.2075 F1: 36.17% 0.03 min
> Epoch: 378 Step: 28400, train loss: 0.1161 F1: 61.75% dev loss: 0.2070 F1: 39.04% 0.03 min
> Epoch: 381 Step: 28600, train loss: 0.1143 F1: 61.43% dev loss: 0.2090 F1: 36.40% 0.03 min
> Epoch: 383 Step: 28800, train loss: 0.1156 F1: 61.81% dev loss: 0.2086 F1: 36.18% 0.03 min
> Epoch: 386 Step: 29000, train loss: 0.1152 F1: 62.84% dev loss: 0.2071 F1: 39.25% 0.03 min
> Epoch: 389 Step: 29200, train loss: 0.1123 F1: 63.05% dev loss: 0.2079 F1: 36.66% 0.03 min
> Epoch: 391 Step: 29400, train loss: 0.1128 F1: 62.15% dev loss: 0.2073 F1: 36.28% 0.03 min
> Epoch: 394 Step: 29600, train loss: 0.1135 F1: 64.30% dev loss: 0.2075 F1: 38.62% 0.03 min
> Epoch: 397 Step: 29800, train loss: 0.1112 F1: 62.81% dev loss: 0.2068 F1: 36.40% 0.03 min
> Epoch: 399 Step: 30000, train loss: 0.1111 F1: 63.61% dev loss: 0.2074 F1: 36.18% 0.03 min
> Epoch: 402 Step: 30200, train loss: 0.1120 F1: 63.02% dev loss: 0.2080 F1: 39.10% 0.03 min
> Epoch: 405 Step: 30400, train loss: 0.1121 F1: 66.14% dev loss: 0.2079 F1: 36.46% 0.03 min
> Epoch: 407 Step: 30600, train loss: 0.1118 F1: 62.77% dev loss: 0.2074 F1: 36.35% 0.03 min
> Epoch: 410 Step: 30800, train loss: 0.1109 F1: 64.97% dev loss: 0.2065 F1: 39.53% 0.03 min
> Epoch: 413 Step: 31000, train loss: 0.1089 F1: 64.51% dev loss: 0.2070 F1: 36.81% 0.03 min
> Epoch: 415 Step: 31200, train loss: 0.1103 F1: 64.47% dev loss: 0.2080 F1: 36.49% 0.03 min
> Epoch: 418 Step: 31400, train loss: 0.1098 F1: 64.91% dev loss: 0.2072 F1: 36.45% 0.03 min
> Epoch: 421 Step: 31600, train loss: 0.1087 F1: 63.23% dev loss: 0.2075 F1: 36.78% 0.03 min
> Epoch: 423 Step: 31800, train loss: 0.1097 F1: 63.43% dev loss: 0.2084 F1: 36.33% 0.03 min
> Epoch: 426 Step: 32000, train loss: 0.1095 F1: 65.32% dev loss: 0.2074 F1: 39.35% 0.03 min
> Epoch: 429 Step: 32200, train loss: 0.1077 F1: 63.43% dev loss: 0.2093 F1: 36.47% 0.03 min
> Epoch: 431 Step: 32400, train loss: 0.1087 F1: 63.18% dev loss: 0.2079 F1: 36.13% 0.03 min
> Epoch: 434 Step: 32600, train loss: 0.1090 F1: 65.59% dev loss: 0.2064 F1: 38.57% 0.03 min
> Epoch: 437 Step: 32800, train loss: 0.1068 F1: 64.87% dev loss: 0.2074 F1: 36.40% 0.03 min
> Epoch: 439 Step: 33000, train loss: 0.1062 F1: 64.77% dev loss: 0.2072 F1: 36.42% 0.03 min
> Epoch: 442 Step: 33200, train loss: 0.1072 F1: 65.73% dev loss: 0.2083 F1: 38.47% 0.03 min
> Epoch: 445 Step: 33400, train loss: 0.1056 F1: 67.00% dev loss: 0.2087 F1: 36.27% 0.03 min
> Epoch: 447 Step: 33600, train loss: 0.1057 F1: 65.65% dev loss: 0.2086 F1: 36.38% 0.03 min
> Epoch: 450 Step: 33800, train loss: 0.1058 F1: 66.57% dev loss: 0.2087 F1: 36.14% 0.03 min
> Epoch: 453 Step: 34000, train loss: 0.1050 F1: 66.00% dev loss: 0.2091 F1: 36.51% 0.03 min
> Epoch: 455 Step: 34200, train loss: 0.1047 F1: 66.03% dev loss: 0.2088 F1: 36.38% 0.03 min
> Epoch: 458 Step: 34400, train loss: 0.1055 F1: 66.22% dev loss: 0.2076 F1: 38.74% 0.03 min
> Epoch: 461 Step: 34600, train loss: 0.1032 F1: 66.14% dev loss: 0.2093 F1: 36.28% 0.03 min
> Epoch: 463 Step: 34800, train loss: 0.1042 F1: 65.60% dev loss: 0.2082 F1: 36.44% 0.03 min
> Epoch: 466 Step: 35000, train loss: 0.1043 F1: 67.20% dev loss: 0.2094 F1: 38.55% 0.03 min
> Epoch: 469 Step: 35200, train loss: 0.1027 F1: 66.59% dev loss: 0.2094 F1: 36.46% 0.03 min
> Epoch: 471 Step: 35400, train loss: 0.1027 F1: 66.88% dev loss: 0.2093 F1: 36.48% 0.03 min
> Epoch: 474 Step: 35600, train loss: 0.1036 F1: 67.18% dev loss: 0.2093 F1: 38.69% 0.03 min
> Epoch: 477 Step: 35800, train loss: 0.1022 F1: 68.44% dev loss: 0.2101 F1: 36.56% 0.03 min
> Epoch: 479 Step: 36000, train loss: 0.1024 F1: 65.97% dev loss: 0.2096 F1: 37.33% 0.03 min
> Epoch: 482 Step: 36200, train loss: 0.1019 F1: 67.75% dev loss: 0.2100 F1: 36.26% 0.03 min
> Epoch: 485 Step: 36400, train loss: 0.1012 F1: 67.04% dev loss: 0.2103 F1: 36.53% 0.03 min
> Epoch: 487 Step: 36600, train loss: 0.1019 F1: 65.04% dev loss: 0.2097 F1: 36.37% 0.03 min
> Epoch: 490 Step: 36800, train loss: 0.1020 F1: 66.47% dev loss: 0.2093 F1: 37.36% 0.03 min
> Epoch: 493 Step: 37000, train loss: 0.1006 F1: 64.59% dev loss: 0.2097 F1: 36.40% 0.03 min
> Epoch: 495 Step: 37200, train loss: 0.1017 F1: 66.20% dev loss: 0.2099 F1: 37.35% 0.03 min
> Epoch: 498 Step: 37400, train loss: 0.1015 F1: 68.56% dev loss: 0.2093 F1: 37.37% 0.03 min
> Epoch: 501 Step: 37600, train loss: 0.1002 F1: 66.90% dev loss: 0.2099 F1: 36.61% 0.03 min
> Epoch: 503 Step: 37800, train loss: 0.1011 F1: 66.93% dev loss: 0.2097 F1: 37.49% 0.03 min
> Epoch: 506 Step: 38000, train loss: 0.1014 F1: 67.41% dev loss: 0.2096 F1: 37.35% 0.03 min
> Epoch: 509 Step: 38200, train loss: 0.0993 F1: 68.43% dev loss: 0.2103 F1: 36.52% 0.03 min
> Epoch: 511 Step: 38400, train loss: 0.1012 F1: 66.65% dev loss: 0.2103 F1: 36.49% 0.03 min
> Epoch: 514 Step: 38600, train loss: 0.1003 F1: 68.34% dev loss: 0.2093 F1: 39.44% 0.03 min
> Epoch: 517 Step: 38800, train loss: 0.0988 F1: 66.07% dev loss: 0.2105 F1: 36.47% 0.03 min
> Epoch: 519 Step: 39000, train loss: 0.0990 F1: 67.10% dev loss: 0.2089 F1: 37.49% 0.03 min
> Epoch: 522 Step: 39200, train loss: 0.0999 F1: 68.11% dev loss: 0.2091 F1: 36.34% 0.03 min
> Epoch: 525 Step: 39400, train loss: 0.0983 F1: 67.82% dev loss: 0.2090 F1: 37.66% 0.03 min
> Epoch: 527 Step: 39600, train loss: 0.0990 F1: 67.04% dev loss: 0.2098 F1: 36.52% 0.03 min
> Epoch: 530 Step: 39800, train loss: 0.0985 F1: 67.04% dev loss: 0.2093 F1: 37.37% 0.03 min
> Epoch: 533 Step: 40000, train loss: 0.0987 F1: 68.89% dev loss: 0.2103 F1: 36.42% 0.03 min
> Epoch: 535 Step: 40200, train loss: 0.0990 F1: 67.48% dev loss: 0.2092 F1: 37.52% 0.03 min
> Epoch: 538 Step: 40400, train loss: 0.0984 F1: 67.24% dev loss: 0.2086 F1: 39.67% 0.03 min
> Epoch: 541 Step: 40600, train loss: 0.0970 F1: 67.09% dev loss: 0.2087 F1: 37.63% 0.03 min
> Epoch: 543 Step: 40800, train loss: 0.0973 F1: 67.15% dev loss: 0.2100 F1: 36.56% 0.03 min
> Epoch: 546 Step: 41000, train loss: 0.0972 F1: 68.31% dev loss: 0.2099 F1: 37.43% 0.03 min
> Epoch: 549 Step: 41200, train loss: 0.0965 F1: 67.54% dev loss: 0.2104 F1: 36.61% 0.03 min
> Epoch: 551 Step: 41400, train loss: 0.0972 F1: 68.00% dev loss: 0.2095 F1: 37.43% 0.03 min
> Epoch: 554 Step: 41600, train loss: 0.0977 F1: 69.33% dev loss: 0.2088 F1: 37.41% 0.03 min
> Epoch: 557 Step: 41800, train loss: 0.0966 F1: 69.14% dev loss: 0.2095 F1: 37.32% 0.03 min
> Epoch: 559 Step: 42000, train loss: 0.0969 F1: 69.18% dev loss: 0.2095 F1: 37.45% 0.03 min
> Epoch: 562 Step: 42200, train loss: 0.0962 F1: 68.30% dev loss: 0.2092 F1: 37.42% 0.03 min
> Epoch: 565 Step: 42400, train loss: 0.0965 F1: 65.68% dev loss: 0.2098 F1: 37.28% 0.03 min
> Epoch: 567 Step: 42600, train loss: 0.0959 F1: 68.62% dev loss: 0.2096 F1: 37.37% 0.03 min
> Epoch: 570 Step: 42800, train loss: 0.0966 F1: 68.01% dev loss: 0.2103 F1: 36.41% 0.03 min
> Epoch: 573 Step: 43000, train loss: 0.0963 F1: 67.51% dev loss: 0.2102 F1: 37.29% 0.03 min
> Epoch: 575 Step: 43200, train loss: 0.0954 F1: 67.81% dev loss: 0.2105 F1: 36.44% 0.03 min
> Epoch: 578 Step: 43400, train loss: 0.0959 F1: 69.01% dev loss: 0.2098 F1: 37.41% 0.03 min
> Epoch: 581 Step: 43600, train loss: 0.0963 F1: 69.72% dev loss: 0.2099 F1: 37.62% 0.03 min
> Epoch: 583 Step: 43800, train loss: 0.0950 F1: 68.36% dev loss: 0.2100 F1: 37.55% 0.03 min
> Epoch: 586 Step: 44000, train loss: 0.0962 F1: 68.29% dev loss: 0.2099 F1: 37.45% 0.03 min
> Epoch: 589 Step: 44200, train loss: 0.0948 F1: 67.37% dev loss: 0.2110 F1: 36.30% 0.03 min
> Epoch: 591 Step: 44400, train loss: 0.0959 F1: 68.95% dev loss: 0.2096 F1: 37.63% 0.03 min
> Epoch: 594 Step: 44600, train loss: 0.0957 F1: 69.21% dev loss: 0.2099 F1: 37.45% 0.03 min
> Epoch: 597 Step: 44800, train loss: 0.0949 F1: 67.46% dev loss: 0.2108 F1: 37.44% 0.03 min
> Epoch: 599 Step: 45000, train loss: 0.0954 F1: 68.71% dev loss: 0.2101 F1: 37.50% 0.03 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 15:35:37.901715>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 600
>>> step: 200
>>> model_name: glove
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 2.0
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/glove-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 50000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0]
> Epoch: 2 Step: 200, train loss: 3.2246 F1: 9.83% dev loss: 2.9090 F1: 9.98% 0.04 min
> Epoch: 5 Step: 400, train loss: 2.1719 F1: 9.43% dev loss: 1.0579 F1: 9.49% 0.03 min
> Epoch: 7 Step: 600, train loss: 0.7420 F1: 9.45% dev loss: 0.5803 F1: 9.49% 0.03 min
> Epoch: 10 Step: 800, train loss: 0.6026 F1: 9.43% dev loss: 0.5238 F1: 9.49% 0.03 min
> Epoch: 13 Step: 1000, train loss: 0.5485 F1: 9.43% dev loss: 0.4937 F1: 9.49% 0.03 min
> Epoch: 15 Step: 1200, train loss: 0.5261 F1: 9.45% dev loss: 0.4736 F1: 9.49% 0.03 min
> Epoch: 18 Step: 1400, train loss: 0.5123 F1: 10.41% dev loss: 0.4582 F1: 9.50% 0.03 min
> Epoch: 21 Step: 1600, train loss: 0.4857 F1: 11.53% dev loss: 0.4458 F1: 10.48% 0.03 min
> Epoch: 23 Step: 1800, train loss: 0.4774 F1: 11.91% dev loss: 0.4357 F1: 11.42% 0.03 min
> Epoch: 26 Step: 2000, train loss: 0.4719 F1: 12.85% dev loss: 0.4271 F1: 12.60% 0.03 min
> Epoch: 29 Step: 2200, train loss: 0.4513 F1: 13.44% dev loss: 0.4190 F1: 13.02% 0.03 min
> Epoch: 31 Step: 2400, train loss: 0.4474 F1: 13.33% dev loss: 0.4118 F1: 13.14% 0.03 min
> Epoch: 34 Step: 2600, train loss: 0.4440 F1: 13.51% dev loss: 0.4049 F1: 13.11% 0.03 min
> Epoch: 37 Step: 2800, train loss: 0.4255 F1: 13.90% dev loss: 0.3977 F1: 13.34% 0.03 min
> Epoch: 39 Step: 3000, train loss: 0.4222 F1: 13.62% dev loss: 0.3910 F1: 13.36% 0.03 min
> Epoch: 42 Step: 3200, train loss: 0.4198 F1: 13.80% dev loss: 0.3845 F1: 13.66% 0.03 min
> Epoch: 45 Step: 3400, train loss: 0.4018 F1: 14.37% dev loss: 0.3777 F1: 13.91% 0.03 min
> Epoch: 47 Step: 3600, train loss: 0.3987 F1: 19.45% dev loss: 0.3710 F1: 14.10% 0.03 min
> Epoch: 50 Step: 3800, train loss: 0.3956 F1: 17.43% dev loss: 0.3651 F1: 14.37% 0.03 min
> Epoch: 53 Step: 4000, train loss: 0.3788 F1: 17.23% dev loss: 0.3589 F1: 17.06% 0.03 min
> Epoch: 55 Step: 4200, train loss: 0.3750 F1: 19.80% dev loss: 0.3527 F1: 16.55% 0.03 min
> Epoch: 58 Step: 4400, train loss: 0.3726 F1: 23.09% dev loss: 0.3477 F1: 17.11% 0.03 min
> Epoch: 61 Step: 4600, train loss: 0.3573 F1: 19.60% dev loss: 0.3425 F1: 21.46% 0.03 min
> Epoch: 63 Step: 4800, train loss: 0.3552 F1: 23.85% dev loss: 0.3372 F1: 19.51% 0.03 min
> Epoch: 66 Step: 5000, train loss: 0.3531 F1: 25.11% dev loss: 0.3330 F1: 21.38% 0.03 min
> Epoch: 69 Step: 5200, train loss: 0.3397 F1: 24.97% dev loss: 0.3286 F1: 23.16% 0.03 min
> Epoch: 71 Step: 5400, train loss: 0.3382 F1: 26.10% dev loss: 0.3241 F1: 22.22% 0.03 min
> Epoch: 74 Step: 5600, train loss: 0.3363 F1: 27.21% dev loss: 0.3207 F1: 23.38% 0.03 min
> Epoch: 77 Step: 5800, train loss: 0.3234 F1: 26.28% dev loss: 0.3168 F1: 22.94% 0.03 min
> Epoch: 79 Step: 6000, train loss: 0.3230 F1: 26.98% dev loss: 0.3130 F1: 23.72% 0.03 min
> Epoch: 82 Step: 6200, train loss: 0.3215 F1: 26.89% dev loss: 0.3104 F1: 26.31% 0.03 min
> Epoch: 85 Step: 6400, train loss: 0.3093 F1: 26.05% dev loss: 0.3066 F1: 26.03% 0.03 min
> Epoch: 87 Step: 6600, train loss: 0.3089 F1: 28.82% dev loss: 0.3036 F1: 26.08% 0.03 min
> Epoch: 90 Step: 6800, train loss: 0.3069 F1: 27.98% dev loss: 0.3018 F1: 26.68% 0.03 min
> Epoch: 93 Step: 7000, train loss: 0.2962 F1: 28.31% dev loss: 0.2982 F1: 26.42% 0.03 min
> Epoch: 95 Step: 7200, train loss: 0.2970 F1: 29.71% dev loss: 0.2960 F1: 26.69% 0.03 min
> Epoch: 98 Step: 7400, train loss: 0.2957 F1: 28.67% dev loss: 0.2947 F1: 27.15% 0.03 min
> Epoch: 101 Step: 7600, train loss: 0.2849 F1: 29.59% dev loss: 0.2915 F1: 26.90% 0.03 min
> Epoch: 103 Step: 7800, train loss: 0.2859 F1: 30.53% dev loss: 0.2895 F1: 27.62% 0.03 min
> Epoch: 106 Step: 8000, train loss: 0.2849 F1: 30.35% dev loss: 0.2886 F1: 29.81% 0.03 min
> Epoch: 109 Step: 8200, train loss: 0.2760 F1: 29.77% dev loss: 0.2866 F1: 29.77% 0.03 min
> Epoch: 111 Step: 8400, train loss: 0.2764 F1: 30.61% dev loss: 0.2849 F1: 30.32% 0.03 min
> Epoch: 114 Step: 8600, train loss: 0.2747 F1: 31.16% dev loss: 0.2844 F1: 30.22% 0.03 min
> Epoch: 117 Step: 8800, train loss: 0.2668 F1: 30.69% dev loss: 0.2827 F1: 30.53% 0.03 min
> Epoch: 119 Step: 9000, train loss: 0.2680 F1: 32.04% dev loss: 0.2810 F1: 30.50% 0.03 min
> Epoch: 122 Step: 9200, train loss: 0.2664 F1: 31.62% dev loss: 0.2810 F1: 30.69% 0.03 min
> Epoch: 125 Step: 9400, train loss: 0.2587 F1: 32.97% dev loss: 0.2793 F1: 30.71% 0.03 min
> Epoch: 127 Step: 9600, train loss: 0.2607 F1: 32.62% dev loss: 0.2783 F1: 31.08% 0.03 min
> Epoch: 130 Step: 9800, train loss: 0.2587 F1: 33.16% dev loss: 0.2778 F1: 30.76% 0.03 min
> Epoch: 133 Step: 10000, train loss: 0.2515 F1: 32.81% dev loss: 0.2761 F1: 31.03% 0.03 min
> Epoch: 135 Step: 10200, train loss: 0.2521 F1: 38.33% dev loss: 0.2747 F1: 30.91% 0.03 min
> Epoch: 138 Step: 10400, train loss: 0.2511 F1: 38.57% dev loss: 0.2741 F1: 31.25% 0.03 min
> Epoch: 141 Step: 10600, train loss: 0.2461 F1: 37.25% dev loss: 0.2733 F1: 31.63% 0.03 min
> Epoch: 143 Step: 10800, train loss: 0.2464 F1: 41.21% dev loss: 0.2719 F1: 31.53% 0.03 min
> Epoch: 146 Step: 11000, train loss: 0.2457 F1: 37.35% dev loss: 0.2717 F1: 31.83% 0.03 min
> Epoch: 149 Step: 11200, train loss: 0.2384 F1: 39.12% dev loss: 0.2702 F1: 32.03% 0.03 min
> Epoch: 151 Step: 11400, train loss: 0.2415 F1: 40.56% dev loss: 0.2679 F1: 32.03% 0.03 min
> Epoch: 154 Step: 11600, train loss: 0.2386 F1: 41.13% dev loss: 0.2686 F1: 32.89% 0.03 min
> Epoch: 157 Step: 11800, train loss: 0.2349 F1: 41.87% dev loss: 0.2677 F1: 32.86% 0.03 min
> Epoch: 159 Step: 12000, train loss: 0.2351 F1: 40.76% dev loss: 0.2667 F1: 32.28% 0.03 min
> Epoch: 162 Step: 12200, train loss: 0.2336 F1: 40.50% dev loss: 0.2665 F1: 33.50% 0.03 min
> Epoch: 165 Step: 12400, train loss: 0.2277 F1: 40.91% dev loss: 0.2659 F1: 32.60% 0.03 min
> Epoch: 167 Step: 12600, train loss: 0.2291 F1: 41.73% dev loss: 0.2654 F1: 32.36% 0.03 min
> Epoch: 170 Step: 12800, train loss: 0.2273 F1: 43.71% dev loss: 0.2639 F1: 33.18% 0.03 min
> Epoch: 173 Step: 13000, train loss: 0.2233 F1: 44.97% dev loss: 0.2644 F1: 32.50% 0.03 min
> Epoch: 175 Step: 13200, train loss: 0.2229 F1: 44.69% dev loss: 0.2634 F1: 32.83% 0.03 min
> Epoch: 178 Step: 13400, train loss: 0.2240 F1: 42.96% dev loss: 0.2633 F1: 33.81% 0.03 min
> Epoch: 181 Step: 13600, train loss: 0.2189 F1: 44.99% dev loss: 0.2624 F1: 33.44% 0.03 min
> Epoch: 183 Step: 13800, train loss: 0.2203 F1: 45.19% dev loss: 0.2614 F1: 33.63% 0.03 min
> Epoch: 186 Step: 14000, train loss: 0.2187 F1: 44.86% dev loss: 0.2608 F1: 33.88% 0.03 min
> Epoch: 189 Step: 14200, train loss: 0.2138 F1: 46.51% dev loss: 0.2606 F1: 33.45% 0.03 min
> Epoch: 191 Step: 14400, train loss: 0.2149 F1: 45.91% dev loss: 0.2598 F1: 33.91% 0.03 min
> Epoch: 194 Step: 14600, train loss: 0.2141 F1: 45.94% dev loss: 0.2591 F1: 33.94% 0.03 min
> Epoch: 197 Step: 14800, train loss: 0.2093 F1: 47.35% dev loss: 0.2583 F1: 33.86% 0.03 min
> Epoch: 199 Step: 15000, train loss: 0.2107 F1: 45.90% dev loss: 0.2577 F1: 35.62% 0.03 min
> Epoch: 202 Step: 15200, train loss: 0.2092 F1: 48.22% dev loss: 0.2573 F1: 35.53% 0.03 min
> Epoch: 205 Step: 15400, train loss: 0.2055 F1: 47.99% dev loss: 0.2587 F1: 33.94% 0.03 min
> Epoch: 207 Step: 15600, train loss: 0.2061 F1: 45.50% dev loss: 0.2568 F1: 35.96% 0.03 min
> Epoch: 210 Step: 15800, train loss: 0.2065 F1: 49.46% dev loss: 0.2563 F1: 36.82% 0.03 min
> Epoch: 213 Step: 16000, train loss: 0.2021 F1: 49.05% dev loss: 0.2566 F1: 35.50% 0.03 min
> Epoch: 215 Step: 16200, train loss: 0.2018 F1: 46.09% dev loss: 0.2552 F1: 35.33% 0.03 min
> Epoch: 218 Step: 16400, train loss: 0.2029 F1: 48.22% dev loss: 0.2560 F1: 37.12% 0.03 min
> Epoch: 221 Step: 16600, train loss: 0.1980 F1: 48.61% dev loss: 0.2555 F1: 35.79% 0.03 min
> Epoch: 223 Step: 16800, train loss: 0.1995 F1: 48.93% dev loss: 0.2540 F1: 37.45% 0.03 min
> Epoch: 226 Step: 17000, train loss: 0.2006 F1: 48.75% dev loss: 0.2544 F1: 37.07% 0.03 min
> Epoch: 229 Step: 17200, train loss: 0.1951 F1: 50.69% dev loss: 0.2546 F1: 37.38% 0.03 min
> Epoch: 231 Step: 17400, train loss: 0.1970 F1: 50.56% dev loss: 0.2532 F1: 37.36% 0.03 min
> Epoch: 234 Step: 17600, train loss: 0.1954 F1: 51.27% dev loss: 0.2548 F1: 37.43% 0.03 min
> Epoch: 237 Step: 17800, train loss: 0.1909 F1: 49.47% dev loss: 0.2543 F1: 37.19% 0.03 min
> Epoch: 239 Step: 18000, train loss: 0.1929 F1: 51.37% dev loss: 0.2528 F1: 38.74% 0.03 min
> Epoch: 242 Step: 18200, train loss: 0.1933 F1: 50.44% dev loss: 0.2531 F1: 38.20% 0.03 min
> Epoch: 245 Step: 18400, train loss: 0.1892 F1: 51.55% dev loss: 0.2526 F1: 37.09% 0.03 min
> Epoch: 247 Step: 18600, train loss: 0.1889 F1: 51.19% dev loss: 0.2516 F1: 38.71% 0.03 min
> Epoch: 250 Step: 18800, train loss: 0.1884 F1: 51.37% dev loss: 0.2528 F1: 37.02% 0.03 min
> Epoch: 253 Step: 19000, train loss: 0.1852 F1: 54.10% dev loss: 0.2533 F1: 37.39% 0.03 min
> Epoch: 255 Step: 19200, train loss: 0.1870 F1: 49.44% dev loss: 0.2509 F1: 38.29% 0.03 min
> Epoch: 258 Step: 19400, train loss: 0.1865 F1: 52.01% dev loss: 0.2512 F1: 37.86% 0.03 min
> Epoch: 261 Step: 19600, train loss: 0.1837 F1: 51.58% dev loss: 0.2512 F1: 37.08% 0.03 min
> Epoch: 263 Step: 19800, train loss: 0.1832 F1: 51.05% dev loss: 0.2502 F1: 38.03% 0.03 min
> Epoch: 266 Step: 20000, train loss: 0.1838 F1: 51.98% dev loss: 0.2509 F1: 37.92% 0.03 min
> Epoch: 269 Step: 20200, train loss: 0.1811 F1: 53.35% dev loss: 0.2511 F1: 37.01% 0.03 min
> Epoch: 271 Step: 20400, train loss: 0.1811 F1: 51.60% dev loss: 0.2499 F1: 37.97% 0.03 min
> Epoch: 274 Step: 20600, train loss: 0.1805 F1: 51.33% dev loss: 0.2499 F1: 38.49% 0.03 min
> Epoch: 277 Step: 20800, train loss: 0.1776 F1: 54.26% dev loss: 0.2512 F1: 37.27% 0.03 min
> Epoch: 279 Step: 21000, train loss: 0.1791 F1: 53.58% dev loss: 0.2496 F1: 37.31% 0.03 min
> Epoch: 282 Step: 21200, train loss: 0.1774 F1: 53.35% dev loss: 0.2505 F1: 38.47% 0.03 min
> Epoch: 285 Step: 21400, train loss: 0.1752 F1: 53.35% dev loss: 0.2501 F1: 36.80% 0.03 min
> Epoch: 287 Step: 21600, train loss: 0.1765 F1: 52.11% dev loss: 0.2496 F1: 37.05% 0.03 min
> Epoch: 290 Step: 21800, train loss: 0.1758 F1: 54.30% dev loss: 0.2495 F1: 38.21% 0.03 min
> Epoch: 293 Step: 22000, train loss: 0.1723 F1: 54.72% dev loss: 0.2510 F1: 37.54% 0.03 min
> Epoch: 295 Step: 22200, train loss: 0.1727 F1: 53.28% dev loss: 0.2490 F1: 36.71% 0.03 min
> Epoch: 298 Step: 22400, train loss: 0.1751 F1: 57.29% dev loss: 0.2498 F1: 37.91% 0.03 min
> Epoch: 301 Step: 22600, train loss: 0.1701 F1: 52.75% dev loss: 0.2507 F1: 37.15% 0.03 min
> Epoch: 303 Step: 22800, train loss: 0.1720 F1: 56.42% dev loss: 0.2498 F1: 37.01% 0.03 min
> Epoch: 306 Step: 23000, train loss: 0.1706 F1: 57.05% dev loss: 0.2500 F1: 36.99% 0.03 min
> Epoch: 309 Step: 23200, train loss: 0.1681 F1: 59.02% dev loss: 0.2493 F1: 36.89% 0.03 min
> Epoch: 311 Step: 23400, train loss: 0.1686 F1: 57.49% dev loss: 0.2487 F1: 36.86% 0.03 min
> Epoch: 314 Step: 23600, train loss: 0.1682 F1: 58.64% dev loss: 0.2495 F1: 37.10% 0.03 min
> Epoch: 317 Step: 23800, train loss: 0.1666 F1: 57.81% dev loss: 0.2505 F1: 36.85% 0.03 min
> Epoch: 319 Step: 24000, train loss: 0.1670 F1: 57.62% dev loss: 0.2496 F1: 37.20% 0.03 min
> Epoch: 322 Step: 24200, train loss: 0.1656 F1: 57.21% dev loss: 0.2500 F1: 36.85% 0.03 min
> Epoch: 325 Step: 24400, train loss: 0.1645 F1: 55.84% dev loss: 0.2498 F1: 37.39% 0.03 min
> Epoch: 327 Step: 24600, train loss: 0.1648 F1: 58.49% dev loss: 0.2490 F1: 37.39% 0.03 min
> Epoch: 330 Step: 24800, train loss: 0.1647 F1: 57.77% dev loss: 0.2496 F1: 37.15% 0.03 min
> Epoch: 333 Step: 25000, train loss: 0.1622 F1: 56.38% dev loss: 0.2494 F1: 37.31% 0.03 min
> Epoch: 335 Step: 25200, train loss: 0.1632 F1: 59.38% dev loss: 0.2507 F1: 37.45% 0.03 min
> Epoch: 338 Step: 25400, train loss: 0.1642 F1: 58.61% dev loss: 0.2494 F1: 36.50% 0.03 min
> Epoch: 341 Step: 25600, train loss: 0.1614 F1: 60.86% dev loss: 0.2496 F1: 36.84% 0.03 min
> Epoch: 343 Step: 25800, train loss: 0.1609 F1: 58.58% dev loss: 0.2500 F1: 37.12% 0.03 min
> Epoch: 346 Step: 26000, train loss: 0.1603 F1: 59.61% dev loss: 0.2496 F1: 36.71% 0.03 min
> Epoch: 349 Step: 26200, train loss: 0.1582 F1: 60.84% dev loss: 0.2494 F1: 36.93% 0.03 min
> Epoch: 351 Step: 26400, train loss: 0.1587 F1: 58.38% dev loss: 0.2489 F1: 37.32% 0.03 min
> Epoch: 354 Step: 26600, train loss: 0.1576 F1: 62.18% dev loss: 0.2492 F1: 36.70% 0.03 min
> Epoch: 357 Step: 26800, train loss: 0.1557 F1: 55.44% dev loss: 0.2505 F1: 37.09% 0.03 min
> Epoch: 359 Step: 27000, train loss: 0.1578 F1: 59.56% dev loss: 0.2487 F1: 36.96% 0.03 min
> Epoch: 362 Step: 27200, train loss: 0.1566 F1: 60.70% dev loss: 0.2488 F1: 37.10% 0.03 min
> Epoch: 365 Step: 27400, train loss: 0.1540 F1: 56.34% dev loss: 0.2484 F1: 36.31% 0.03 min
> Epoch: 367 Step: 27600, train loss: 0.1554 F1: 60.39% dev loss: 0.2491 F1: 36.67% 0.03 min
> Epoch: 370 Step: 27800, train loss: 0.1547 F1: 61.44% dev loss: 0.2495 F1: 36.57% 0.03 min
> Epoch: 373 Step: 28000, train loss: 0.1529 F1: 58.47% dev loss: 0.2500 F1: 36.69% 0.03 min
> Epoch: 375 Step: 28200, train loss: 0.1567 F1: 61.12% dev loss: 0.2499 F1: 36.86% 0.03 min
> Epoch: 378 Step: 28400, train loss: 0.1544 F1: 59.55% dev loss: 0.2505 F1: 36.91% 0.03 min
> Epoch: 381 Step: 28600, train loss: 0.1517 F1: 61.56% dev loss: 0.2516 F1: 37.09% 0.03 min
> Epoch: 383 Step: 28800, train loss: 0.1537 F1: 60.07% dev loss: 0.2505 F1: 36.61% 0.03 min
> Epoch: 386 Step: 29000, train loss: 0.1531 F1: 59.39% dev loss: 0.2500 F1: 36.77% 0.03 min
> Epoch: 389 Step: 29200, train loss: 0.1495 F1: 61.61% dev loss: 0.2510 F1: 37.25% 0.03 min
> Epoch: 391 Step: 29400, train loss: 0.1501 F1: 62.20% dev loss: 0.2503 F1: 36.98% 0.03 min
> Epoch: 394 Step: 29600, train loss: 0.1512 F1: 61.16% dev loss: 0.2505 F1: 39.21% 0.03 min
> Epoch: 397 Step: 29800, train loss: 0.1482 F1: 62.38% dev loss: 0.2501 F1: 36.25% 0.03 min
> Epoch: 399 Step: 30000, train loss: 0.1481 F1: 62.36% dev loss: 0.2504 F1: 37.25% 0.03 min
> Epoch: 402 Step: 30200, train loss: 0.1496 F1: 60.94% dev loss: 0.2516 F1: 37.11% 0.03 min
> Epoch: 405 Step: 30400, train loss: 0.1493 F1: 64.96% dev loss: 0.2509 F1: 36.83% 0.03 min
> Epoch: 407 Step: 30600, train loss: 0.1489 F1: 61.34% dev loss: 0.2504 F1: 37.18% 0.03 min
> Epoch: 410 Step: 30800, train loss: 0.1483 F1: 63.74% dev loss: 0.2504 F1: 36.49% 0.03 min
> Epoch: 413 Step: 31000, train loss: 0.1453 F1: 65.47% dev loss: 0.2505 F1: 36.71% 0.03 min
> Epoch: 415 Step: 31200, train loss: 0.1474 F1: 62.29% dev loss: 0.2512 F1: 37.67% 0.03 min
> Epoch: 418 Step: 31400, train loss: 0.1468 F1: 63.19% dev loss: 0.2511 F1: 36.65% 0.03 min
> Epoch: 421 Step: 31600, train loss: 0.1452 F1: 61.89% dev loss: 0.2509 F1: 36.25% 0.03 min
> Epoch: 423 Step: 31800, train loss: 0.1465 F1: 61.49% dev loss: 0.2513 F1: 37.34% 0.03 min
> Epoch: 426 Step: 32000, train loss: 0.1464 F1: 63.20% dev loss: 0.2511 F1: 38.98% 0.03 min
> Epoch: 429 Step: 32200, train loss: 0.1440 F1: 62.48% dev loss: 0.2523 F1: 37.30% 0.03 min
> Epoch: 431 Step: 32400, train loss: 0.1456 F1: 59.63% dev loss: 0.2502 F1: 37.02% 0.03 min
> Epoch: 434 Step: 32600, train loss: 0.1455 F1: 63.24% dev loss: 0.2501 F1: 38.93% 0.03 min
> Epoch: 437 Step: 32800, train loss: 0.1428 F1: 59.47% dev loss: 0.2508 F1: 36.64% 0.03 min
> Epoch: 439 Step: 33000, train loss: 0.1423 F1: 62.65% dev loss: 0.2510 F1: 36.52% 0.03 min
> Epoch: 442 Step: 33200, train loss: 0.1433 F1: 63.72% dev loss: 0.2511 F1: 38.88% 0.03 min
> Epoch: 445 Step: 33400, train loss: 0.1414 F1: 64.26% dev loss: 0.2517 F1: 37.20% 0.03 min
> Epoch: 447 Step: 33600, train loss: 0.1420 F1: 64.27% dev loss: 0.2520 F1: 36.59% 0.03 min
> Epoch: 450 Step: 33800, train loss: 0.1422 F1: 65.10% dev loss: 0.2524 F1: 37.01% 0.03 min
> Epoch: 453 Step: 34000, train loss: 0.1409 F1: 64.34% dev loss: 0.2522 F1: 36.49% 0.03 min
> Epoch: 455 Step: 34200, train loss: 0.1405 F1: 64.17% dev loss: 0.2518 F1: 36.90% 0.03 min
> Epoch: 458 Step: 34400, train loss: 0.1418 F1: 63.14% dev loss: 0.2512 F1: 36.74% 0.03 min
> Epoch: 461 Step: 34600, train loss: 0.1386 F1: 65.37% dev loss: 0.2526 F1: 36.76% 0.03 min
> Epoch: 463 Step: 34800, train loss: 0.1399 F1: 64.72% dev loss: 0.2516 F1: 36.60% 0.03 min
> Epoch: 466 Step: 35000, train loss: 0.1401 F1: 65.67% dev loss: 0.2525 F1: 39.34% 0.03 min
> Epoch: 469 Step: 35200, train loss: 0.1378 F1: 65.36% dev loss: 0.2524 F1: 36.39% 0.03 min
> Epoch: 471 Step: 35400, train loss: 0.1381 F1: 64.66% dev loss: 0.2525 F1: 36.14% 0.03 min
> Epoch: 474 Step: 35600, train loss: 0.1394 F1: 66.66% dev loss: 0.2525 F1: 36.96% 0.03 min
> Epoch: 477 Step: 35800, train loss: 0.1372 F1: 67.62% dev loss: 0.2531 F1: 36.61% 0.03 min
> Epoch: 479 Step: 36000, train loss: 0.1377 F1: 63.09% dev loss: 0.2522 F1: 35.90% 0.03 min
> Epoch: 482 Step: 36200, train loss: 0.1370 F1: 65.68% dev loss: 0.2527 F1: 36.93% 0.03 min
> Epoch: 485 Step: 36400, train loss: 0.1363 F1: 64.59% dev loss: 0.2531 F1: 36.31% 0.03 min
> Epoch: 487 Step: 36600, train loss: 0.1372 F1: 63.06% dev loss: 0.2528 F1: 36.32% 0.03 min
> Epoch: 490 Step: 36800, train loss: 0.1376 F1: 63.94% dev loss: 0.2526 F1: 35.75% 0.03 min
> Epoch: 493 Step: 37000, train loss: 0.1355 F1: 61.31% dev loss: 0.2528 F1: 35.91% 0.03 min
> Epoch: 495 Step: 37200, train loss: 0.1368 F1: 65.47% dev loss: 0.2532 F1: 36.13% 0.03 min
> Epoch: 498 Step: 37400, train loss: 0.1369 F1: 66.62% dev loss: 0.2526 F1: 35.76% 0.03 min
> Epoch: 501 Step: 37600, train loss: 0.1350 F1: 65.66% dev loss: 0.2527 F1: 35.72% 0.03 min
> Epoch: 503 Step: 37800, train loss: 0.1364 F1: 65.46% dev loss: 0.2525 F1: 36.08% 0.03 min
> Epoch: 506 Step: 38000, train loss: 0.1368 F1: 66.95% dev loss: 0.2524 F1: 36.08% 0.03 min
> Epoch: 509 Step: 38200, train loss: 0.1335 F1: 66.49% dev loss: 0.2529 F1: 36.23% 0.03 min
> Epoch: 511 Step: 38400, train loss: 0.1364 F1: 64.82% dev loss: 0.2528 F1: 36.28% 0.03 min
> Epoch: 514 Step: 38600, train loss: 0.1355 F1: 66.16% dev loss: 0.2529 F1: 38.36% 0.03 min
> Epoch: 517 Step: 38800, train loss: 0.1336 F1: 65.92% dev loss: 0.2539 F1: 36.48% 0.03 min
> Epoch: 519 Step: 39000, train loss: 0.1340 F1: 65.55% dev loss: 0.2526 F1: 36.06% 0.03 min
> Epoch: 522 Step: 39200, train loss: 0.1349 F1: 67.45% dev loss: 0.2529 F1: 36.60% 0.03 min
> Epoch: 525 Step: 39400, train loss: 0.1328 F1: 66.82% dev loss: 0.2523 F1: 35.81% 0.03 min
> Epoch: 527 Step: 39600, train loss: 0.1334 F1: 65.28% dev loss: 0.2533 F1: 36.22% 0.03 min
> Epoch: 530 Step: 39800, train loss: 0.1334 F1: 66.07% dev loss: 0.2526 F1: 35.81% 0.03 min
> Epoch: 533 Step: 40000, train loss: 0.1330 F1: 68.61% dev loss: 0.2537 F1: 36.61% 0.03 min
> Epoch: 535 Step: 40200, train loss: 0.1333 F1: 65.52% dev loss: 0.2528 F1: 35.75% 0.03 min
> Epoch: 538 Step: 40400, train loss: 0.1331 F1: 67.12% dev loss: 0.2525 F1: 38.13% 0.03 min
> Epoch: 541 Step: 40600, train loss: 0.1314 F1: 65.13% dev loss: 0.2525 F1: 35.82% 0.03 min
> Epoch: 543 Step: 40800, train loss: 0.1317 F1: 66.68% dev loss: 0.2532 F1: 35.77% 0.03 min
> Epoch: 546 Step: 41000, train loss: 0.1318 F1: 67.73% dev loss: 0.2531 F1: 38.26% 0.03 min
> Epoch: 549 Step: 41200, train loss: 0.1307 F1: 63.10% dev loss: 0.2538 F1: 35.94% 0.03 min
> Epoch: 551 Step: 41400, train loss: 0.1316 F1: 65.88% dev loss: 0.2531 F1: 36.26% 0.03 min
> Epoch: 554 Step: 41600, train loss: 0.1323 F1: 67.34% dev loss: 0.2526 F1: 38.03% 0.03 min
> Epoch: 557 Step: 41800, train loss: 0.1308 F1: 66.56% dev loss: 0.2529 F1: 36.27% 0.03 min
> Epoch: 559 Step: 42000, train loss: 0.1310 F1: 68.13% dev loss: 0.2531 F1: 35.90% 0.03 min
> Epoch: 562 Step: 42200, train loss: 0.1304 F1: 68.49% dev loss: 0.2528 F1: 38.33% 0.03 min
> Epoch: 565 Step: 42400, train loss: 0.1304 F1: 64.79% dev loss: 0.2533 F1: 35.77% 0.03 min
> Epoch: 567 Step: 42600, train loss: 0.1300 F1: 67.82% dev loss: 0.2535 F1: 35.77% 0.03 min
> Epoch: 570 Step: 42800, train loss: 0.1308 F1: 67.21% dev loss: 0.2539 F1: 35.94% 0.03 min
> Epoch: 573 Step: 43000, train loss: 0.1303 F1: 64.57% dev loss: 0.2539 F1: 35.95% 0.03 min
> Epoch: 575 Step: 43200, train loss: 0.1296 F1: 67.30% dev loss: 0.2542 F1: 35.95% 0.03 min
> Epoch: 578 Step: 43400, train loss: 0.1300 F1: 65.70% dev loss: 0.2537 F1: 38.22% 0.03 min
> Epoch: 581 Step: 43600, train loss: 0.1302 F1: 66.91% dev loss: 0.2538 F1: 35.77% 0.03 min
> Epoch: 583 Step: 43800, train loss: 0.1289 F1: 66.32% dev loss: 0.2537 F1: 35.77% 0.03 min
> Epoch: 586 Step: 44000, train loss: 0.1304 F1: 67.07% dev loss: 0.2537 F1: 35.94% 0.03 min
> Epoch: 589 Step: 44200, train loss: 0.1288 F1: 66.42% dev loss: 0.2545 F1: 35.94% 0.03 min
> Epoch: 591 Step: 44400, train loss: 0.1297 F1: 66.78% dev loss: 0.2535 F1: 35.91% 0.03 min
> Epoch: 594 Step: 44600, train loss: 0.1295 F1: 67.90% dev loss: 0.2535 F1: 35.81% 0.03 min
> Epoch: 597 Step: 44800, train loss: 0.1286 F1: 66.46% dev loss: 0.2544 F1: 35.72% 0.03 min
> Epoch: 599 Step: 45000, train loss: 0.1293 F1: 67.47% dev loss: 0.2541 F1: 35.77% 0.03 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 15:42:11.003720>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 600
>>> step: 200
>>> model_name: glove
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 1.0
>>> alpha: 2.0
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/glove-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 50000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0]
> Epoch: 2 Step: 200, train loss: 3.6758 F1: 9.76% dev loss: 3.3801 F1: 9.98% 0.04 min
> Epoch: 5 Step: 400, train loss: 2.6345 F1: 9.43% dev loss: 1.3909 F1: 9.49% 0.03 min
> Epoch: 7 Step: 600, train loss: 0.9226 F1: 9.45% dev loss: 0.7093 F1: 9.49% 0.03 min
> Epoch: 10 Step: 800, train loss: 0.7374 F1: 9.43% dev loss: 0.6479 F1: 9.49% 0.03 min
> Epoch: 13 Step: 1000, train loss: 0.6756 F1: 9.43% dev loss: 0.6113 F1: 9.49% 0.03 min
> Epoch: 15 Step: 1200, train loss: 0.6463 F1: 9.45% dev loss: 0.5848 F1: 9.49% 0.03 min
> Epoch: 18 Step: 1400, train loss: 0.6273 F1: 9.43% dev loss: 0.5642 F1: 9.50% 0.03 min
> Epoch: 21 Step: 1600, train loss: 0.5944 F1: 10.95% dev loss: 0.5477 F1: 10.72% 0.03 min
> Epoch: 23 Step: 1800, train loss: 0.5827 F1: 11.60% dev loss: 0.5345 F1: 11.19% 0.03 min
> Epoch: 26 Step: 2000, train loss: 0.5747 F1: 12.57% dev loss: 0.5232 F1: 12.03% 0.03 min
> Epoch: 29 Step: 2200, train loss: 0.5502 F1: 13.36% dev loss: 0.5129 F1: 12.76% 0.03 min
> Epoch: 31 Step: 2400, train loss: 0.5448 F1: 13.31% dev loss: 0.5037 F1: 13.06% 0.03 min
> Epoch: 34 Step: 2600, train loss: 0.5402 F1: 13.51% dev loss: 0.4952 F1: 13.08% 0.03 min
> Epoch: 37 Step: 2800, train loss: 0.5188 F1: 13.94% dev loss: 0.4867 F1: 13.17% 0.03 min
> Epoch: 39 Step: 3000, train loss: 0.5152 F1: 13.58% dev loss: 0.4788 F1: 13.41% 0.03 min
> Epoch: 42 Step: 3200, train loss: 0.5124 F1: 13.73% dev loss: 0.4715 F1: 13.58% 0.03 min
> Epoch: 45 Step: 3400, train loss: 0.4920 F1: 14.25% dev loss: 0.4639 F1: 13.77% 0.03 min
> Epoch: 47 Step: 3600, train loss: 0.4891 F1: 13.97% dev loss: 0.4564 F1: 13.91% 0.03 min
> Epoch: 50 Step: 3800, train loss: 0.4858 F1: 14.12% dev loss: 0.4499 F1: 14.26% 0.03 min
> Epoch: 53 Step: 4000, train loss: 0.4669 F1: 14.55% dev loss: 0.4430 F1: 14.40% 0.03 min
> Epoch: 55 Step: 4200, train loss: 0.4632 F1: 18.94% dev loss: 0.4359 F1: 18.77% 0.03 min
> Epoch: 58 Step: 4400, train loss: 0.4606 F1: 19.14% dev loss: 0.4303 F1: 17.51% 0.03 min
> Epoch: 61 Step: 4600, train loss: 0.4430 F1: 18.50% dev loss: 0.4242 F1: 17.54% 0.03 min
> Epoch: 63 Step: 4800, train loss: 0.4410 F1: 23.14% dev loss: 0.4179 F1: 17.23% 0.03 min
> Epoch: 66 Step: 5000, train loss: 0.4383 F1: 23.47% dev loss: 0.4131 F1: 21.85% 0.03 min
> Epoch: 69 Step: 5200, train loss: 0.4229 F1: 24.25% dev loss: 0.4079 F1: 22.14% 0.03 min
> Epoch: 71 Step: 5400, train loss: 0.4215 F1: 24.06% dev loss: 0.4025 F1: 21.41% 0.03 min
> Epoch: 74 Step: 5600, train loss: 0.4189 F1: 24.81% dev loss: 0.3986 F1: 23.39% 0.03 min
> Epoch: 77 Step: 5800, train loss: 0.4041 F1: 26.63% dev loss: 0.3940 F1: 23.36% 0.03 min
> Epoch: 79 Step: 6000, train loss: 0.4039 F1: 26.86% dev loss: 0.3895 F1: 23.89% 0.03 min
> Epoch: 82 Step: 6200, train loss: 0.4022 F1: 27.25% dev loss: 0.3864 F1: 24.69% 0.03 min
> Epoch: 85 Step: 6400, train loss: 0.3880 F1: 25.66% dev loss: 0.3821 F1: 24.70% 0.03 min
> Epoch: 87 Step: 6600, train loss: 0.3877 F1: 28.07% dev loss: 0.3785 F1: 26.81% 0.03 min
> Epoch: 90 Step: 6800, train loss: 0.3853 F1: 27.38% dev loss: 0.3763 F1: 26.78% 0.03 min
> Epoch: 93 Step: 7000, train loss: 0.3730 F1: 27.16% dev loss: 0.3724 F1: 26.79% 0.03 min
> Epoch: 95 Step: 7200, train loss: 0.3742 F1: 29.33% dev loss: 0.3694 F1: 26.01% 0.03 min
> Epoch: 98 Step: 7400, train loss: 0.3727 F1: 28.02% dev loss: 0.3677 F1: 26.99% 0.03 min
> Epoch: 101 Step: 7600, train loss: 0.3600 F1: 28.97% dev loss: 0.3642 F1: 26.32% 0.03 min
> Epoch: 103 Step: 7800, train loss: 0.3613 F1: 29.56% dev loss: 0.3617 F1: 27.11% 0.03 min
> Epoch: 106 Step: 8000, train loss: 0.3603 F1: 30.52% dev loss: 0.3605 F1: 27.84% 0.03 min
> Epoch: 109 Step: 8200, train loss: 0.3497 F1: 30.01% dev loss: 0.3581 F1: 28.19% 0.03 min
> Epoch: 111 Step: 8400, train loss: 0.3506 F1: 30.43% dev loss: 0.3558 F1: 28.86% 0.03 min
> Epoch: 114 Step: 8600, train loss: 0.3486 F1: 30.96% dev loss: 0.3551 F1: 29.79% 0.03 min
> Epoch: 117 Step: 8800, train loss: 0.3396 F1: 30.07% dev loss: 0.3531 F1: 28.70% 0.03 min
> Epoch: 119 Step: 9000, train loss: 0.3410 F1: 31.79% dev loss: 0.3512 F1: 29.92% 0.03 min
> Epoch: 122 Step: 9200, train loss: 0.3391 F1: 31.70% dev loss: 0.3509 F1: 30.20% 0.03 min
> Epoch: 125 Step: 9400, train loss: 0.3299 F1: 32.39% dev loss: 0.3489 F1: 29.55% 0.03 min
> Epoch: 127 Step: 9600, train loss: 0.3326 F1: 32.20% dev loss: 0.3477 F1: 30.31% 0.03 min
> Epoch: 130 Step: 9800, train loss: 0.3305 F1: 32.74% dev loss: 0.3474 F1: 30.47% 0.03 min
> Epoch: 133 Step: 10000, train loss: 0.3219 F1: 32.42% dev loss: 0.3452 F1: 30.13% 0.03 min
> Epoch: 135 Step: 10200, train loss: 0.3228 F1: 36.48% dev loss: 0.3437 F1: 30.46% 0.03 min
> Epoch: 138 Step: 10400, train loss: 0.3218 F1: 36.85% dev loss: 0.3429 F1: 31.53% 0.03 min
> Epoch: 141 Step: 10600, train loss: 0.3158 F1: 36.03% dev loss: 0.3417 F1: 31.28% 0.03 min
> Epoch: 143 Step: 10800, train loss: 0.3165 F1: 37.63% dev loss: 0.3404 F1: 31.62% 0.03 min
> Epoch: 146 Step: 11000, train loss: 0.3157 F1: 36.82% dev loss: 0.3395 F1: 31.93% 0.03 min
> Epoch: 149 Step: 11200, train loss: 0.3070 F1: 38.40% dev loss: 0.3381 F1: 31.92% 0.03 min
> Epoch: 151 Step: 11400, train loss: 0.3109 F1: 35.36% dev loss: 0.3357 F1: 31.92% 0.03 min
> Epoch: 154 Step: 11600, train loss: 0.3076 F1: 38.29% dev loss: 0.3363 F1: 32.28% 0.03 min
> Epoch: 157 Step: 11800, train loss: 0.3030 F1: 41.06% dev loss: 0.3350 F1: 32.41% 0.03 min
> Epoch: 159 Step: 12000, train loss: 0.3040 F1: 40.69% dev loss: 0.3337 F1: 32.64% 0.03 min
> Epoch: 162 Step: 12200, train loss: 0.3020 F1: 40.48% dev loss: 0.3337 F1: 32.93% 0.03 min
> Epoch: 165 Step: 12400, train loss: 0.2945 F1: 43.01% dev loss: 0.3328 F1: 32.85% 0.03 min
> Epoch: 167 Step: 12600, train loss: 0.2971 F1: 40.60% dev loss: 0.3322 F1: 32.73% 0.03 min
> Epoch: 170 Step: 12800, train loss: 0.2949 F1: 42.85% dev loss: 0.3304 F1: 33.35% 0.03 min
> Epoch: 173 Step: 13000, train loss: 0.2899 F1: 44.09% dev loss: 0.3311 F1: 32.96% 0.03 min
> Epoch: 175 Step: 13200, train loss: 0.2900 F1: 43.28% dev loss: 0.3303 F1: 33.66% 0.03 min
> Epoch: 178 Step: 13400, train loss: 0.2911 F1: 43.43% dev loss: 0.3297 F1: 33.62% 0.03 min
> Epoch: 181 Step: 13600, train loss: 0.2851 F1: 44.01% dev loss: 0.3282 F1: 33.42% 0.03 min
> Epoch: 183 Step: 13800, train loss: 0.2871 F1: 44.09% dev loss: 0.3267 F1: 34.03% 0.03 min
> Epoch: 186 Step: 14000, train loss: 0.2853 F1: 44.66% dev loss: 0.3264 F1: 33.76% 0.03 min
> Epoch: 189 Step: 14200, train loss: 0.2793 F1: 44.36% dev loss: 0.3262 F1: 33.56% 0.03 min
> Epoch: 191 Step: 14400, train loss: 0.2809 F1: 44.73% dev loss: 0.3253 F1: 33.95% 0.03 min
> Epoch: 194 Step: 14600, train loss: 0.2801 F1: 43.72% dev loss: 0.3246 F1: 33.75% 0.03 min
> Epoch: 197 Step: 14800, train loss: 0.2739 F1: 46.48% dev loss: 0.3233 F1: 34.22% 0.03 min
> Epoch: 199 Step: 15000, train loss: 0.2762 F1: 44.33% dev loss: 0.3221 F1: 34.37% 0.03 min
> Epoch: 202 Step: 15200, train loss: 0.2744 F1: 45.84% dev loss: 0.3222 F1: 33.71% 0.03 min
> Epoch: 205 Step: 15400, train loss: 0.2696 F1: 45.60% dev loss: 0.3231 F1: 33.48% 0.03 min
> Epoch: 207 Step: 15600, train loss: 0.2708 F1: 44.85% dev loss: 0.3209 F1: 33.74% 0.03 min
> Epoch: 210 Step: 15800, train loss: 0.2713 F1: 47.39% dev loss: 0.3204 F1: 36.32% 0.03 min
> Epoch: 213 Step: 16000, train loss: 0.2660 F1: 48.32% dev loss: 0.3203 F1: 34.66% 0.03 min
> Epoch: 215 Step: 16200, train loss: 0.2659 F1: 42.96% dev loss: 0.3188 F1: 34.55% 0.03 min
> Epoch: 218 Step: 16400, train loss: 0.2673 F1: 46.74% dev loss: 0.3203 F1: 36.10% 0.03 min
> Epoch: 221 Step: 16600, train loss: 0.2609 F1: 48.52% dev loss: 0.3189 F1: 34.65% 0.03 min
> Epoch: 223 Step: 16800, train loss: 0.2633 F1: 47.33% dev loss: 0.3173 F1: 35.51% 0.03 min
> Epoch: 226 Step: 17000, train loss: 0.2642 F1: 47.81% dev loss: 0.3177 F1: 35.97% 0.03 min
> Epoch: 229 Step: 17200, train loss: 0.2579 F1: 48.07% dev loss: 0.3177 F1: 35.55% 0.03 min
> Epoch: 231 Step: 17400, train loss: 0.2604 F1: 48.34% dev loss: 0.3158 F1: 36.05% 0.03 min
> Epoch: 234 Step: 17600, train loss: 0.2583 F1: 49.82% dev loss: 0.3178 F1: 35.97% 0.03 min
> Epoch: 237 Step: 17800, train loss: 0.2530 F1: 48.30% dev loss: 0.3170 F1: 35.75% 0.03 min
> Epoch: 239 Step: 18000, train loss: 0.2556 F1: 49.37% dev loss: 0.3154 F1: 35.65% 0.03 min
> Epoch: 242 Step: 18200, train loss: 0.2563 F1: 48.89% dev loss: 0.3161 F1: 36.74% 0.03 min
> Epoch: 245 Step: 18400, train loss: 0.2510 F1: 51.12% dev loss: 0.3149 F1: 35.97% 0.03 min
> Epoch: 247 Step: 18600, train loss: 0.2515 F1: 49.13% dev loss: 0.3133 F1: 35.93% 0.03 min
> Epoch: 250 Step: 18800, train loss: 0.2510 F1: 50.02% dev loss: 0.3149 F1: 36.56% 0.03 min
> Epoch: 253 Step: 19000, train loss: 0.2467 F1: 50.95% dev loss: 0.3148 F1: 36.29% 0.03 min
> Epoch: 255 Step: 19200, train loss: 0.2487 F1: 48.54% dev loss: 0.3121 F1: 36.22% 0.03 min
> Epoch: 258 Step: 19400, train loss: 0.2484 F1: 49.92% dev loss: 0.3126 F1: 37.33% 0.03 min
> Epoch: 261 Step: 19600, train loss: 0.2453 F1: 52.87% dev loss: 0.3125 F1: 35.84% 0.03 min
> Epoch: 263 Step: 19800, train loss: 0.2449 F1: 49.14% dev loss: 0.3115 F1: 38.61% 0.03 min
> Epoch: 266 Step: 20000, train loss: 0.2453 F1: 49.98% dev loss: 0.3122 F1: 38.25% 0.03 min
> Epoch: 269 Step: 20200, train loss: 0.2418 F1: 52.43% dev loss: 0.3119 F1: 37.06% 0.03 min
> Epoch: 271 Step: 20400, train loss: 0.2425 F1: 50.46% dev loss: 0.3108 F1: 38.60% 0.03 min
> Epoch: 274 Step: 20600, train loss: 0.2417 F1: 50.02% dev loss: 0.3109 F1: 38.47% 0.03 min
> Epoch: 277 Step: 20800, train loss: 0.2379 F1: 52.83% dev loss: 0.3115 F1: 37.38% 0.03 min
> Epoch: 279 Step: 21000, train loss: 0.2398 F1: 51.40% dev loss: 0.3097 F1: 37.12% 0.03 min
> Epoch: 282 Step: 21200, train loss: 0.2382 F1: 52.56% dev loss: 0.3110 F1: 36.86% 0.03 min
> Epoch: 285 Step: 21400, train loss: 0.2348 F1: 53.32% dev loss: 0.3102 F1: 37.31% 0.03 min
> Epoch: 287 Step: 21600, train loss: 0.2368 F1: 50.92% dev loss: 0.3098 F1: 37.04% 0.03 min
> Epoch: 290 Step: 21800, train loss: 0.2363 F1: 53.34% dev loss: 0.3098 F1: 37.41% 0.03 min
> Epoch: 293 Step: 22000, train loss: 0.2315 F1: 52.69% dev loss: 0.3114 F1: 37.43% 0.03 min
> Epoch: 295 Step: 22200, train loss: 0.2328 F1: 52.52% dev loss: 0.3084 F1: 38.24% 0.03 min
> Epoch: 298 Step: 22400, train loss: 0.2351 F1: 52.28% dev loss: 0.3093 F1: 38.83% 0.03 min
> Epoch: 301 Step: 22600, train loss: 0.2293 F1: 52.24% dev loss: 0.3100 F1: 37.54% 0.03 min
> Epoch: 303 Step: 22800, train loss: 0.2315 F1: 53.87% dev loss: 0.3086 F1: 37.88% 0.03 min
> Epoch: 306 Step: 23000, train loss: 0.2300 F1: 54.50% dev loss: 0.3094 F1: 38.47% 0.03 min
> Epoch: 309 Step: 23200, train loss: 0.2270 F1: 53.65% dev loss: 0.3085 F1: 36.83% 0.03 min
> Epoch: 311 Step: 23400, train loss: 0.2282 F1: 55.01% dev loss: 0.3078 F1: 36.76% 0.03 min
> Epoch: 314 Step: 23600, train loss: 0.2272 F1: 55.75% dev loss: 0.3086 F1: 37.87% 0.03 min
> Epoch: 317 Step: 23800, train loss: 0.2247 F1: 53.35% dev loss: 0.3093 F1: 37.35% 0.03 min
> Epoch: 319 Step: 24000, train loss: 0.2255 F1: 53.90% dev loss: 0.3084 F1: 37.14% 0.03 min
> Epoch: 322 Step: 24200, train loss: 0.2242 F1: 53.66% dev loss: 0.3088 F1: 37.76% 0.03 min
> Epoch: 325 Step: 24400, train loss: 0.2222 F1: 55.10% dev loss: 0.3085 F1: 37.41% 0.03 min
> Epoch: 327 Step: 24600, train loss: 0.2235 F1: 54.00% dev loss: 0.3074 F1: 37.26% 0.03 min
> Epoch: 330 Step: 24800, train loss: 0.2232 F1: 56.96% dev loss: 0.3079 F1: 37.43% 0.03 min
> Epoch: 333 Step: 25000, train loss: 0.2198 F1: 55.21% dev loss: 0.3073 F1: 37.11% 0.03 min
> Epoch: 335 Step: 25200, train loss: 0.2215 F1: 57.06% dev loss: 0.3084 F1: 37.25% 0.03 min
> Epoch: 338 Step: 25400, train loss: 0.2228 F1: 54.66% dev loss: 0.3077 F1: 37.31% 0.03 min
> Epoch: 341 Step: 25600, train loss: 0.2191 F1: 59.84% dev loss: 0.3072 F1: 37.22% 0.03 min
> Epoch: 343 Step: 25800, train loss: 0.2189 F1: 55.95% dev loss: 0.3076 F1: 37.42% 0.03 min
> Epoch: 346 Step: 26000, train loss: 0.2178 F1: 57.10% dev loss: 0.3080 F1: 37.25% 0.03 min
> Epoch: 349 Step: 26200, train loss: 0.2153 F1: 54.12% dev loss: 0.3073 F1: 37.26% 0.03 min
> Epoch: 351 Step: 26400, train loss: 0.2164 F1: 57.16% dev loss: 0.3064 F1: 37.27% 0.03 min
> Epoch: 354 Step: 26600, train loss: 0.2151 F1: 58.84% dev loss: 0.3074 F1: 37.35% 0.03 min
> Epoch: 357 Step: 26800, train loss: 0.2130 F1: 56.09% dev loss: 0.3082 F1: 37.56% 0.03 min
> Epoch: 359 Step: 27000, train loss: 0.2148 F1: 57.19% dev loss: 0.3062 F1: 37.71% 0.03 min
> Epoch: 362 Step: 27200, train loss: 0.2134 F1: 58.51% dev loss: 0.3063 F1: 37.58% 0.03 min
> Epoch: 365 Step: 27400, train loss: 0.2105 F1: 55.38% dev loss: 0.3058 F1: 37.24% 0.03 min
> Epoch: 367 Step: 27600, train loss: 0.2124 F1: 57.04% dev loss: 0.3067 F1: 37.56% 0.03 min
> Epoch: 370 Step: 27800, train loss: 0.2117 F1: 59.18% dev loss: 0.3073 F1: 37.29% 0.03 min
> Epoch: 373 Step: 28000, train loss: 0.2090 F1: 57.89% dev loss: 0.3073 F1: 37.14% 0.03 min
> Epoch: 375 Step: 28200, train loss: 0.2135 F1: 59.65% dev loss: 0.3064 F1: 37.32% 0.03 min
> Epoch: 378 Step: 28400, train loss: 0.2108 F1: 58.29% dev loss: 0.3071 F1: 37.69% 0.03 min
> Epoch: 381 Step: 28600, train loss: 0.2076 F1: 60.15% dev loss: 0.3082 F1: 37.61% 0.03 min
> Epoch: 383 Step: 28800, train loss: 0.2101 F1: 56.95% dev loss: 0.3066 F1: 37.65% 0.03 min
> Epoch: 386 Step: 29000, train loss: 0.2095 F1: 58.05% dev loss: 0.3068 F1: 37.45% 0.03 min
> Epoch: 389 Step: 29200, train loss: 0.2052 F1: 59.47% dev loss: 0.3079 F1: 37.72% 0.03 min
> Epoch: 391 Step: 29400, train loss: 0.2061 F1: 59.62% dev loss: 0.3074 F1: 37.66% 0.03 min
> Epoch: 394 Step: 29600, train loss: 0.2072 F1: 58.79% dev loss: 0.3073 F1: 37.51% 0.03 min
> Epoch: 397 Step: 29800, train loss: 0.2032 F1: 57.96% dev loss: 0.3072 F1: 36.99% 0.03 min
> Epoch: 399 Step: 30000, train loss: 0.2036 F1: 61.32% dev loss: 0.3069 F1: 37.68% 0.03 min
> Epoch: 402 Step: 30200, train loss: 0.2056 F1: 59.85% dev loss: 0.3081 F1: 37.73% 0.03 min
> Epoch: 405 Step: 30400, train loss: 0.2049 F1: 62.92% dev loss: 0.3080 F1: 37.39% 0.03 min
> Epoch: 407 Step: 30600, train loss: 0.2047 F1: 59.34% dev loss: 0.3068 F1: 37.74% 0.03 min
> Epoch: 410 Step: 30800, train loss: 0.2042 F1: 61.77% dev loss: 0.3071 F1: 37.22% 0.03 min
> Epoch: 413 Step: 31000, train loss: 0.2002 F1: 62.95% dev loss: 0.3068 F1: 37.41% 0.03 min
> Epoch: 415 Step: 31200, train loss: 0.2030 F1: 59.78% dev loss: 0.3069 F1: 37.91% 0.03 min
> Epoch: 418 Step: 31400, train loss: 0.2020 F1: 59.38% dev loss: 0.3074 F1: 37.74% 0.03 min
> Epoch: 421 Step: 31600, train loss: 0.2001 F1: 57.84% dev loss: 0.3073 F1: 37.00% 0.03 min
> Epoch: 423 Step: 31800, train loss: 0.2018 F1: 60.59% dev loss: 0.3072 F1: 37.76% 0.03 min
> Epoch: 426 Step: 32000, train loss: 0.2015 F1: 61.83% dev loss: 0.3075 F1: 37.77% 0.03 min
> Epoch: 429 Step: 32200, train loss: 0.1987 F1: 59.71% dev loss: 0.3080 F1: 37.49% 0.03 min
> Epoch: 431 Step: 32400, train loss: 0.2010 F1: 57.57% dev loss: 0.3065 F1: 37.54% 0.03 min
> Epoch: 434 Step: 32600, train loss: 0.2001 F1: 61.80% dev loss: 0.3067 F1: 37.23% 0.03 min
> Epoch: 437 Step: 32800, train loss: 0.1972 F1: 58.29% dev loss: 0.3069 F1: 37.15% 0.03 min
> Epoch: 439 Step: 33000, train loss: 0.1966 F1: 61.50% dev loss: 0.3070 F1: 37.14% 0.03 min
> Epoch: 442 Step: 33200, train loss: 0.1977 F1: 62.26% dev loss: 0.3074 F1: 37.04% 0.03 min
> Epoch: 445 Step: 33400, train loss: 0.1957 F1: 61.99% dev loss: 0.3080 F1: 37.29% 0.03 min
> Epoch: 447 Step: 33600, train loss: 0.1966 F1: 61.24% dev loss: 0.3084 F1: 37.59% 0.03 min
> Epoch: 450 Step: 33800, train loss: 0.1964 F1: 64.56% dev loss: 0.3086 F1: 37.29% 0.03 min
> Epoch: 453 Step: 34000, train loss: 0.1948 F1: 62.24% dev loss: 0.3079 F1: 37.06% 0.03 min
> Epoch: 455 Step: 34200, train loss: 0.1945 F1: 61.58% dev loss: 0.3078 F1: 37.25% 0.03 min
> Epoch: 458 Step: 34400, train loss: 0.1965 F1: 61.56% dev loss: 0.3078 F1: 37.61% 0.03 min
> Epoch: 461 Step: 34600, train loss: 0.1925 F1: 61.75% dev loss: 0.3086 F1: 36.98% 0.03 min
> Epoch: 463 Step: 34800, train loss: 0.1942 F1: 61.53% dev loss: 0.3081 F1: 36.76% 0.03 min
> Epoch: 466 Step: 35000, train loss: 0.1941 F1: 63.27% dev loss: 0.3083 F1: 37.54% 0.03 min
> Epoch: 469 Step: 35200, train loss: 0.1911 F1: 63.64% dev loss: 0.3080 F1: 36.88% 0.03 min
> Epoch: 471 Step: 35400, train loss: 0.1918 F1: 63.09% dev loss: 0.3084 F1: 36.96% 0.03 min
> Epoch: 474 Step: 35600, train loss: 0.1935 F1: 64.27% dev loss: 0.3084 F1: 37.42% 0.03 min
> Epoch: 477 Step: 35800, train loss: 0.1908 F1: 66.15% dev loss: 0.3086 F1: 37.08% 0.03 min
> Epoch: 479 Step: 36000, train loss: 0.1914 F1: 61.61% dev loss: 0.3077 F1: 36.38% 0.03 min
> Epoch: 482 Step: 36200, train loss: 0.1904 F1: 64.34% dev loss: 0.3084 F1: 37.80% 0.03 min
> Epoch: 485 Step: 36400, train loss: 0.1892 F1: 65.97% dev loss: 0.3085 F1: 36.99% 0.03 min
> Epoch: 487 Step: 36600, train loss: 0.1904 F1: 61.53% dev loss: 0.3081 F1: 36.99% 0.03 min
> Epoch: 490 Step: 36800, train loss: 0.1914 F1: 62.41% dev loss: 0.3084 F1: 37.02% 0.03 min
> Epoch: 493 Step: 37000, train loss: 0.1887 F1: 61.68% dev loss: 0.3088 F1: 36.83% 0.03 min
> Epoch: 495 Step: 37200, train loss: 0.1902 F1: 62.91% dev loss: 0.3086 F1: 36.34% 0.03 min
> Epoch: 498 Step: 37400, train loss: 0.1903 F1: 64.26% dev loss: 0.3086 F1: 36.24% 0.03 min
> Epoch: 501 Step: 37600, train loss: 0.1879 F1: 65.73% dev loss: 0.3082 F1: 36.66% 0.03 min
> Epoch: 503 Step: 37800, train loss: 0.1895 F1: 63.49% dev loss: 0.3077 F1: 36.40% 0.03 min
> Epoch: 506 Step: 38000, train loss: 0.1898 F1: 64.54% dev loss: 0.3082 F1: 36.26% 0.03 min
> Epoch: 509 Step: 38200, train loss: 0.1857 F1: 65.86% dev loss: 0.3086 F1: 36.88% 0.03 min
> Epoch: 511 Step: 38400, train loss: 0.1895 F1: 62.68% dev loss: 0.3083 F1: 36.47% 0.03 min
> Epoch: 514 Step: 38600, train loss: 0.1889 F1: 64.61% dev loss: 0.3082 F1: 36.30% 0.03 min
> Epoch: 517 Step: 38800, train loss: 0.1864 F1: 66.30% dev loss: 0.3090 F1: 36.45% 0.03 min
> Epoch: 519 Step: 39000, train loss: 0.1867 F1: 63.79% dev loss: 0.3076 F1: 36.26% 0.03 min
> Epoch: 522 Step: 39200, train loss: 0.1878 F1: 65.71% dev loss: 0.3086 F1: 36.66% 0.03 min
> Epoch: 525 Step: 39400, train loss: 0.1853 F1: 65.12% dev loss: 0.3077 F1: 36.34% 0.03 min
> Epoch: 527 Step: 39600, train loss: 0.1857 F1: 62.26% dev loss: 0.3088 F1: 36.23% 0.03 min
> Epoch: 530 Step: 39800, train loss: 0.1862 F1: 64.60% dev loss: 0.3078 F1: 39.89% 0.03 min
> Epoch: 533 Step: 40000, train loss: 0.1857 F1: 63.28% dev loss: 0.3093 F1: 37.12% 0.03 min
> Epoch: 535 Step: 40200, train loss: 0.1857 F1: 63.95% dev loss: 0.3085 F1: 36.27% 0.03 min
> Epoch: 538 Step: 40400, train loss: 0.1860 F1: 64.62% dev loss: 0.3080 F1: 36.44% 0.03 min
> Epoch: 541 Step: 40600, train loss: 0.1834 F1: 64.57% dev loss: 0.3083 F1: 36.40% 0.03 min
> Epoch: 543 Step: 40800, train loss: 0.1842 F1: 64.01% dev loss: 0.3084 F1: 36.14% 0.03 min
> Epoch: 546 Step: 41000, train loss: 0.1840 F1: 64.99% dev loss: 0.3083 F1: 39.65% 0.03 min
> Epoch: 549 Step: 41200, train loss: 0.1826 F1: 62.28% dev loss: 0.3093 F1: 36.39% 0.03 min
> Epoch: 551 Step: 41400, train loss: 0.1839 F1: 65.27% dev loss: 0.3087 F1: 36.01% 0.03 min
> Epoch: 554 Step: 41600, train loss: 0.1848 F1: 64.62% dev loss: 0.3081 F1: 40.01% 0.03 min
> Epoch: 557 Step: 41800, train loss: 0.1824 F1: 63.55% dev loss: 0.3085 F1: 39.39% 0.03 min
> Epoch: 559 Step: 42000, train loss: 0.1835 F1: 65.45% dev loss: 0.3094 F1: 36.36% 0.03 min
> Epoch: 562 Step: 42200, train loss: 0.1828 F1: 65.62% dev loss: 0.3082 F1: 39.76% 0.03 min
> Epoch: 565 Step: 42400, train loss: 0.1821 F1: 63.85% dev loss: 0.3090 F1: 36.04% 0.03 min
> Epoch: 567 Step: 42600, train loss: 0.1825 F1: 66.20% dev loss: 0.3091 F1: 36.18% 0.03 min
> Epoch: 570 Step: 42800, train loss: 0.1835 F1: 65.81% dev loss: 0.3090 F1: 37.09% 0.03 min
> Epoch: 573 Step: 43000, train loss: 0.1821 F1: 66.88% dev loss: 0.3092 F1: 36.29% 0.03 min
> Epoch: 575 Step: 43200, train loss: 0.1822 F1: 64.66% dev loss: 0.3093 F1: 36.42% 0.03 min
> Epoch: 578 Step: 43400, train loss: 0.1821 F1: 65.59% dev loss: 0.3089 F1: 36.54% 0.03 min
> Epoch: 581 Step: 43600, train loss: 0.1822 F1: 65.40% dev loss: 0.3090 F1: 39.18% 0.03 min
> Epoch: 583 Step: 43800, train loss: 0.1811 F1: 64.89% dev loss: 0.3089 F1: 40.09% 0.03 min
> Epoch: 586 Step: 44000, train loss: 0.1828 F1: 65.06% dev loss: 0.3092 F1: 36.06% 0.03 min
> Epoch: 589 Step: 44200, train loss: 0.1808 F1: 64.59% dev loss: 0.3098 F1: 36.26% 0.03 min
> Epoch: 591 Step: 44400, train loss: 0.1817 F1: 65.51% dev loss: 0.3090 F1: 39.34% 0.03 min
> Epoch: 594 Step: 44600, train loss: 0.1813 F1: 67.10% dev loss: 0.3088 F1: 39.89% 0.03 min
> Epoch: 597 Step: 44800, train loss: 0.1798 F1: 65.32% dev loss: 0.3098 F1: 36.16% 0.03 min
> Epoch: 599 Step: 45000, train loss: 0.1814 F1: 64.91% dev loss: 0.3096 F1: 36.05% 0.03 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 15:51:37.177007>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 600
>>> step: 200
>>> model_name: glove
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 1.0
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/glove-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 50000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
> Epoch: 2 Step: 200, train loss: 1.4148 F1: 9.82% dev loss: 1.2530 F1: 9.98% 0.04 min
> Epoch: 5 Step: 400, train loss: 0.9060 F1: 9.43% dev loss: 0.4260 F1: 9.49% 0.03 min
> Epoch: 7 Step: 600, train loss: 0.3125 F1: 9.45% dev loss: 0.2457 F1: 9.49% 0.03 min
> Epoch: 10 Step: 800, train loss: 0.2548 F1: 9.43% dev loss: 0.2198 F1: 9.49% 0.03 min
> Epoch: 13 Step: 1000, train loss: 0.2308 F1: 9.43% dev loss: 0.2070 F1: 9.49% 0.03 min
> Epoch: 15 Step: 1200, train loss: 0.2216 F1: 10.07% dev loss: 0.1988 F1: 9.49% 0.03 min
> Epoch: 18 Step: 1400, train loss: 0.2161 F1: 10.72% dev loss: 0.1926 F1: 10.29% 0.03 min
> Epoch: 21 Step: 1600, train loss: 0.2048 F1: 11.70% dev loss: 0.1875 F1: 10.87% 0.03 min
> Epoch: 23 Step: 1800, train loss: 0.2015 F1: 12.00% dev loss: 0.1833 F1: 11.38% 0.03 min
> Epoch: 26 Step: 2000, train loss: 0.1993 F1: 12.92% dev loss: 0.1797 F1: 12.71% 0.03 min
> Epoch: 29 Step: 2200, train loss: 0.1903 F1: 13.43% dev loss: 0.1762 F1: 13.01% 0.03 min
> Epoch: 31 Step: 2400, train loss: 0.1886 F1: 13.34% dev loss: 0.1731 F1: 13.20% 0.03 min
> Epoch: 34 Step: 2600, train loss: 0.1872 F1: 13.53% dev loss: 0.1701 F1: 13.13% 0.03 min
> Epoch: 37 Step: 2800, train loss: 0.1789 F1: 13.87% dev loss: 0.1669 F1: 13.26% 0.03 min
> Epoch: 39 Step: 3000, train loss: 0.1772 F1: 13.61% dev loss: 0.1638 F1: 13.35% 0.03 min
> Epoch: 42 Step: 3200, train loss: 0.1761 F1: 13.82% dev loss: 0.1609 F1: 13.78% 0.03 min
> Epoch: 45 Step: 3400, train loss: 0.1679 F1: 14.37% dev loss: 0.1577 F1: 14.00% 0.03 min
> Epoch: 47 Step: 3600, train loss: 0.1663 F1: 17.71% dev loss: 0.1546 F1: 14.20% 0.03 min
> Epoch: 50 Step: 3800, train loss: 0.1649 F1: 20.92% dev loss: 0.1519 F1: 16.67% 0.03 min
> Epoch: 53 Step: 4000, train loss: 0.1574 F1: 19.19% dev loss: 0.1492 F1: 16.22% 0.03 min
> Epoch: 55 Step: 4200, train loss: 0.1557 F1: 22.29% dev loss: 0.1465 F1: 17.21% 0.03 min
> Epoch: 58 Step: 4400, train loss: 0.1547 F1: 21.92% dev loss: 0.1443 F1: 19.99% 0.03 min
> Epoch: 61 Step: 4600, train loss: 0.1479 F1: 22.72% dev loss: 0.1420 F1: 19.99% 0.03 min
> Epoch: 63 Step: 4800, train loss: 0.1469 F1: 25.13% dev loss: 0.1397 F1: 21.11% 0.03 min
> Epoch: 66 Step: 5000, train loss: 0.1461 F1: 24.83% dev loss: 0.1378 F1: 23.05% 0.03 min
> Epoch: 69 Step: 5200, train loss: 0.1401 F1: 26.03% dev loss: 0.1358 F1: 22.89% 0.03 min
> Epoch: 71 Step: 5400, train loss: 0.1393 F1: 26.27% dev loss: 0.1339 F1: 24.38% 0.03 min
> Epoch: 74 Step: 5600, train loss: 0.1386 F1: 26.44% dev loss: 0.1325 F1: 25.00% 0.03 min
> Epoch: 77 Step: 5800, train loss: 0.1329 F1: 26.77% dev loss: 0.1306 F1: 24.88% 0.03 min
> Epoch: 79 Step: 6000, train loss: 0.1326 F1: 27.78% dev loss: 0.1291 F1: 25.50% 0.03 min
> Epoch: 82 Step: 6200, train loss: 0.1319 F1: 27.47% dev loss: 0.1280 F1: 26.13% 0.03 min
> Epoch: 85 Step: 6400, train loss: 0.1266 F1: 26.94% dev loss: 0.1262 F1: 26.10% 0.03 min
> Epoch: 87 Step: 6600, train loss: 0.1263 F1: 28.93% dev loss: 0.1249 F1: 26.76% 0.03 min
> Epoch: 90 Step: 6800, train loss: 0.1253 F1: 28.38% dev loss: 0.1243 F1: 26.41% 0.03 min
> Epoch: 93 Step: 7000, train loss: 0.1207 F1: 29.06% dev loss: 0.1226 F1: 27.25% 0.03 min
> Epoch: 95 Step: 7200, train loss: 0.1209 F1: 30.16% dev loss: 0.1218 F1: 27.68% 0.03 min
> Epoch: 98 Step: 7400, train loss: 0.1203 F1: 29.47% dev loss: 0.1212 F1: 29.31% 0.03 min
> Epoch: 101 Step: 7600, train loss: 0.1156 F1: 30.18% dev loss: 0.1198 F1: 28.24% 0.03 min
> Epoch: 103 Step: 7800, train loss: 0.1160 F1: 31.02% dev loss: 0.1190 F1: 29.30% 0.03 min
> Epoch: 106 Step: 8000, train loss: 0.1155 F1: 30.91% dev loss: 0.1186 F1: 29.53% 0.03 min
> Epoch: 109 Step: 8200, train loss: 0.1116 F1: 30.03% dev loss: 0.1178 F1: 29.80% 0.03 min
> Epoch: 111 Step: 8400, train loss: 0.1118 F1: 30.88% dev loss: 0.1171 F1: 30.33% 0.03 min
> Epoch: 114 Step: 8600, train loss: 0.1109 F1: 31.13% dev loss: 0.1169 F1: 30.20% 0.03 min
> Epoch: 117 Step: 8800, train loss: 0.1075 F1: 34.99% dev loss: 0.1161 F1: 30.89% 0.03 min
> Epoch: 119 Step: 9000, train loss: 0.1080 F1: 32.33% dev loss: 0.1153 F1: 30.94% 0.03 min
> Epoch: 122 Step: 9200, train loss: 0.1073 F1: 35.63% dev loss: 0.1154 F1: 30.11% 0.03 min
> Epoch: 125 Step: 9400, train loss: 0.1040 F1: 37.17% dev loss: 0.1147 F1: 31.41% 0.03 min
> Epoch: 127 Step: 9600, train loss: 0.1047 F1: 34.16% dev loss: 0.1143 F1: 31.13% 0.03 min
> Epoch: 130 Step: 9800, train loss: 0.1038 F1: 40.08% dev loss: 0.1140 F1: 31.32% 0.03 min
> Epoch: 133 Step: 10000, train loss: 0.1006 F1: 38.79% dev loss: 0.1133 F1: 31.39% 0.03 min
> Epoch: 135 Step: 10200, train loss: 0.1009 F1: 40.37% dev loss: 0.1127 F1: 30.54% 0.03 min
> Epoch: 138 Step: 10400, train loss: 0.1003 F1: 41.47% dev loss: 0.1125 F1: 31.66% 0.03 min
> Epoch: 141 Step: 10600, train loss: 0.0983 F1: 42.17% dev loss: 0.1121 F1: 31.32% 0.03 min
> Epoch: 143 Step: 10800, train loss: 0.0983 F1: 41.40% dev loss: 0.1115 F1: 31.42% 0.03 min
> Epoch: 146 Step: 11000, train loss: 0.0979 F1: 40.71% dev loss: 0.1115 F1: 32.08% 0.03 min
> Epoch: 149 Step: 11200, train loss: 0.0949 F1: 39.75% dev loss: 0.1107 F1: 31.52% 0.03 min
> Epoch: 151 Step: 11400, train loss: 0.0961 F1: 42.27% dev loss: 0.1098 F1: 31.74% 0.03 min
> Epoch: 154 Step: 11600, train loss: 0.0947 F1: 42.14% dev loss: 0.1102 F1: 33.28% 0.03 min
> Epoch: 157 Step: 11800, train loss: 0.0932 F1: 44.66% dev loss: 0.1098 F1: 32.24% 0.03 min
> Epoch: 159 Step: 12000, train loss: 0.0932 F1: 43.23% dev loss: 0.1095 F1: 33.18% 0.03 min
> Epoch: 162 Step: 12200, train loss: 0.0925 F1: 42.94% dev loss: 0.1092 F1: 33.35% 0.03 min
> Epoch: 165 Step: 12400, train loss: 0.0901 F1: 41.50% dev loss: 0.1091 F1: 32.55% 0.03 min
> Epoch: 167 Step: 12600, train loss: 0.0905 F1: 43.93% dev loss: 0.1091 F1: 33.23% 0.03 min
> Epoch: 170 Step: 12800, train loss: 0.0897 F1: 45.06% dev loss: 0.1084 F1: 33.72% 0.03 min
> Epoch: 173 Step: 13000, train loss: 0.0881 F1: 44.40% dev loss: 0.1085 F1: 33.48% 0.03 min
> Epoch: 175 Step: 13200, train loss: 0.0878 F1: 45.49% dev loss: 0.1083 F1: 33.56% 0.03 min
> Epoch: 178 Step: 13400, train loss: 0.0882 F1: 44.98% dev loss: 0.1084 F1: 33.48% 0.03 min
> Epoch: 181 Step: 13600, train loss: 0.0860 F1: 46.68% dev loss: 0.1079 F1: 33.67% 0.03 min
> Epoch: 183 Step: 13800, train loss: 0.0866 F1: 46.41% dev loss: 0.1077 F1: 33.97% 0.03 min
> Epoch: 186 Step: 14000, train loss: 0.0860 F1: 45.76% dev loss: 0.1072 F1: 36.37% 0.03 min
> Epoch: 189 Step: 14200, train loss: 0.0838 F1: 47.64% dev loss: 0.1071 F1: 33.71% 0.03 min
> Epoch: 191 Step: 14400, train loss: 0.0842 F1: 45.90% dev loss: 0.1070 F1: 34.05% 0.03 min
> Epoch: 194 Step: 14600, train loss: 0.0838 F1: 47.14% dev loss: 0.1066 F1: 36.33% 0.03 min
> Epoch: 197 Step: 14800, train loss: 0.0819 F1: 49.60% dev loss: 0.1063 F1: 36.73% 0.03 min
> Epoch: 199 Step: 15000, train loss: 0.0823 F1: 47.33% dev loss: 0.1061 F1: 37.02% 0.03 min
> Epoch: 202 Step: 15200, train loss: 0.0817 F1: 48.65% dev loss: 0.1059 F1: 36.67% 0.03 min
> Epoch: 205 Step: 15400, train loss: 0.0802 F1: 48.57% dev loss: 0.1068 F1: 37.19% 0.03 min
> Epoch: 207 Step: 15600, train loss: 0.0804 F1: 47.81% dev loss: 0.1061 F1: 37.01% 0.03 min
> Epoch: 210 Step: 15800, train loss: 0.0807 F1: 49.74% dev loss: 0.1056 F1: 36.94% 0.03 min
> Epoch: 213 Step: 16000, train loss: 0.0787 F1: 50.63% dev loss: 0.1058 F1: 37.08% 0.03 min
> Epoch: 215 Step: 16200, train loss: 0.0786 F1: 47.48% dev loss: 0.1054 F1: 37.53% 0.03 min
> Epoch: 218 Step: 16400, train loss: 0.0789 F1: 48.89% dev loss: 0.1054 F1: 36.90% 0.03 min
> Epoch: 221 Step: 16600, train loss: 0.0769 F1: 51.42% dev loss: 0.1053 F1: 37.42% 0.03 min
> Epoch: 223 Step: 16800, train loss: 0.0775 F1: 50.18% dev loss: 0.1047 F1: 38.58% 0.03 min
> Epoch: 226 Step: 17000, train loss: 0.0781 F1: 50.72% dev loss: 0.1048 F1: 38.09% 0.03 min
> Epoch: 229 Step: 17200, train loss: 0.0756 F1: 51.19% dev loss: 0.1052 F1: 37.51% 0.03 min
> Epoch: 231 Step: 17400, train loss: 0.0764 F1: 50.56% dev loss: 0.1045 F1: 38.94% 0.03 min
> Epoch: 234 Step: 17600, train loss: 0.0756 F1: 52.06% dev loss: 0.1052 F1: 38.06% 0.03 min
> Epoch: 237 Step: 17800, train loss: 0.0739 F1: 50.11% dev loss: 0.1049 F1: 37.17% 0.03 min
> Epoch: 239 Step: 18000, train loss: 0.0747 F1: 51.37% dev loss: 0.1043 F1: 38.87% 0.03 min
> Epoch: 242 Step: 18200, train loss: 0.0748 F1: 51.82% dev loss: 0.1041 F1: 38.02% 0.03 min
> Epoch: 245 Step: 18400, train loss: 0.0731 F1: 52.52% dev loss: 0.1042 F1: 38.68% 0.03 min
> Epoch: 247 Step: 18600, train loss: 0.0728 F1: 54.42% dev loss: 0.1040 F1: 38.80% 0.03 min
> Epoch: 250 Step: 18800, train loss: 0.0726 F1: 54.76% dev loss: 0.1041 F1: 38.72% 0.03 min
> Epoch: 253 Step: 19000, train loss: 0.0713 F1: 55.31% dev loss: 0.1045 F1: 38.68% 0.03 min
> Epoch: 255 Step: 19200, train loss: 0.0721 F1: 54.38% dev loss: 0.1036 F1: 38.95% 0.03 min
> Epoch: 258 Step: 19400, train loss: 0.0718 F1: 55.16% dev loss: 0.1037 F1: 38.43% 0.03 min
> Epoch: 261 Step: 19600, train loss: 0.0707 F1: 52.93% dev loss: 0.1037 F1: 38.60% 0.03 min
> Epoch: 263 Step: 19800, train loss: 0.0704 F1: 52.17% dev loss: 0.1034 F1: 38.73% 0.03 min
> Epoch: 266 Step: 20000, train loss: 0.0707 F1: 52.34% dev loss: 0.1035 F1: 38.73% 0.03 min
> Epoch: 269 Step: 20200, train loss: 0.0695 F1: 53.37% dev loss: 0.1036 F1: 38.21% 0.03 min
> Epoch: 271 Step: 20400, train loss: 0.0695 F1: 55.50% dev loss: 0.1032 F1: 38.34% 0.03 min
> Epoch: 274 Step: 20600, train loss: 0.0692 F1: 56.57% dev loss: 0.1030 F1: 38.59% 0.03 min
> Epoch: 277 Step: 20800, train loss: 0.0680 F1: 54.27% dev loss: 0.1039 F1: 38.41% 0.03 min
> Epoch: 279 Step: 21000, train loss: 0.0685 F1: 55.08% dev loss: 0.1032 F1: 38.36% 0.03 min
> Epoch: 282 Step: 21200, train loss: 0.0679 F1: 56.41% dev loss: 0.1036 F1: 38.56% 0.03 min
> Epoch: 285 Step: 21400, train loss: 0.0670 F1: 56.38% dev loss: 0.1038 F1: 38.26% 0.03 min
> Epoch: 287 Step: 21600, train loss: 0.0676 F1: 54.70% dev loss: 0.1033 F1: 38.49% 0.03 min
> Epoch: 290 Step: 21800, train loss: 0.0672 F1: 59.38% dev loss: 0.1029 F1: 37.32% 0.03 min
> Epoch: 293 Step: 22000, train loss: 0.0659 F1: 55.28% dev loss: 0.1041 F1: 38.51% 0.03 min
> Epoch: 295 Step: 22200, train loss: 0.0659 F1: 58.91% dev loss: 0.1031 F1: 37.69% 0.03 min
> Epoch: 298 Step: 22400, train loss: 0.0670 F1: 57.95% dev loss: 0.1040 F1: 37.23% 0.03 min
> Epoch: 301 Step: 22600, train loss: 0.0648 F1: 55.88% dev loss: 0.1041 F1: 38.35% 0.03 min
> Epoch: 303 Step: 22800, train loss: 0.0656 F1: 57.16% dev loss: 0.1036 F1: 36.86% 0.03 min
> Epoch: 306 Step: 23000, train loss: 0.0648 F1: 58.67% dev loss: 0.1037 F1: 37.54% 0.03 min
> Epoch: 309 Step: 23200, train loss: 0.0640 F1: 59.65% dev loss: 0.1034 F1: 37.40% 0.03 min
> Epoch: 311 Step: 23400, train loss: 0.0641 F1: 59.47% dev loss: 0.1030 F1: 36.79% 0.03 min
> Epoch: 314 Step: 23600, train loss: 0.0640 F1: 60.36% dev loss: 0.1034 F1: 37.52% 0.03 min
> Epoch: 317 Step: 23800, train loss: 0.0635 F1: 57.11% dev loss: 0.1039 F1: 36.85% 0.03 min
> Epoch: 319 Step: 24000, train loss: 0.0635 F1: 59.06% dev loss: 0.1036 F1: 36.97% 0.03 min
> Epoch: 322 Step: 24200, train loss: 0.0629 F1: 59.63% dev loss: 0.1034 F1: 39.99% 0.03 min
> Epoch: 325 Step: 24400, train loss: 0.0625 F1: 61.75% dev loss: 0.1037 F1: 37.37% 0.03 min
> Epoch: 327 Step: 24600, train loss: 0.0624 F1: 58.93% dev loss: 0.1032 F1: 36.62% 0.03 min
> Epoch: 330 Step: 24800, train loss: 0.0626 F1: 59.58% dev loss: 0.1032 F1: 37.34% 0.03 min
> Epoch: 333 Step: 25000, train loss: 0.0615 F1: 61.63% dev loss: 0.1036 F1: 36.55% 0.03 min
> Epoch: 335 Step: 25200, train loss: 0.0618 F1: 60.41% dev loss: 0.1042 F1: 36.53% 0.03 min
> Epoch: 338 Step: 25400, train loss: 0.0623 F1: 58.42% dev loss: 0.1033 F1: 39.99% 0.03 min
> Epoch: 341 Step: 25600, train loss: 0.0612 F1: 59.43% dev loss: 0.1036 F1: 36.62% 0.03 min
> Epoch: 343 Step: 25800, train loss: 0.0608 F1: 60.43% dev loss: 0.1038 F1: 36.26% 0.03 min
> Epoch: 346 Step: 26000, train loss: 0.0607 F1: 60.78% dev loss: 0.1033 F1: 40.23% 0.03 min
> Epoch: 349 Step: 26200, train loss: 0.0599 F1: 60.07% dev loss: 0.1035 F1: 36.76% 0.03 min
> Epoch: 351 Step: 26400, train loss: 0.0599 F1: 59.82% dev loss: 0.1032 F1: 36.57% 0.03 min
> Epoch: 354 Step: 26600, train loss: 0.0595 F1: 62.50% dev loss: 0.1029 F1: 39.40% 0.03 min
> Epoch: 357 Step: 26800, train loss: 0.0587 F1: 57.66% dev loss: 0.1039 F1: 36.51% 0.03 min
> Epoch: 359 Step: 27000, train loss: 0.0596 F1: 61.26% dev loss: 0.1030 F1: 36.48% 0.03 min
> Epoch: 362 Step: 27200, train loss: 0.0591 F1: 62.87% dev loss: 0.1030 F1: 36.62% 0.03 min
> Epoch: 365 Step: 27400, train loss: 0.0580 F1: 59.79% dev loss: 0.1028 F1: 37.30% 0.03 min
> Epoch: 367 Step: 27600, train loss: 0.0584 F1: 62.79% dev loss: 0.1034 F1: 36.59% 0.03 min
> Epoch: 370 Step: 27800, train loss: 0.0584 F1: 61.12% dev loss: 0.1030 F1: 39.27% 0.03 min
> Epoch: 373 Step: 28000, train loss: 0.0575 F1: 60.95% dev loss: 0.1035 F1: 36.55% 0.03 min
> Epoch: 375 Step: 28200, train loss: 0.0591 F1: 61.83% dev loss: 0.1038 F1: 36.17% 0.03 min
> Epoch: 378 Step: 28400, train loss: 0.0581 F1: 61.75% dev loss: 0.1035 F1: 39.04% 0.03 min
> Epoch: 381 Step: 28600, train loss: 0.0571 F1: 61.43% dev loss: 0.1045 F1: 36.40% 0.03 min
> Epoch: 383 Step: 28800, train loss: 0.0578 F1: 61.81% dev loss: 0.1043 F1: 36.18% 0.03 min
> Epoch: 386 Step: 29000, train loss: 0.0576 F1: 62.84% dev loss: 0.1036 F1: 39.25% 0.03 min
> Epoch: 389 Step: 29200, train loss: 0.0562 F1: 63.05% dev loss: 0.1040 F1: 36.66% 0.03 min
> Epoch: 391 Step: 29400, train loss: 0.0564 F1: 62.15% dev loss: 0.1036 F1: 36.28% 0.03 min
> Epoch: 394 Step: 29600, train loss: 0.0568 F1: 64.30% dev loss: 0.1038 F1: 38.62% 0.03 min
> Epoch: 397 Step: 29800, train loss: 0.0556 F1: 62.81% dev loss: 0.1034 F1: 36.40% 0.03 min
> Epoch: 399 Step: 30000, train loss: 0.0555 F1: 63.61% dev loss: 0.1037 F1: 36.18% 0.03 min
> Epoch: 402 Step: 30200, train loss: 0.0560 F1: 63.02% dev loss: 0.1040 F1: 39.10% 0.03 min
> Epoch: 405 Step: 30400, train loss: 0.0561 F1: 66.14% dev loss: 0.1039 F1: 36.46% 0.03 min
> Epoch: 407 Step: 30600, train loss: 0.0559 F1: 62.77% dev loss: 0.1037 F1: 36.35% 0.03 min
> Epoch: 410 Step: 30800, train loss: 0.0555 F1: 64.97% dev loss: 0.1033 F1: 39.53% 0.03 min
> Epoch: 413 Step: 31000, train loss: 0.0544 F1: 64.51% dev loss: 0.1035 F1: 36.81% 0.03 min
> Epoch: 415 Step: 31200, train loss: 0.0551 F1: 64.47% dev loss: 0.1040 F1: 36.49% 0.03 min
> Epoch: 418 Step: 31400, train loss: 0.0549 F1: 64.91% dev loss: 0.1036 F1: 36.45% 0.03 min
> Epoch: 421 Step: 31600, train loss: 0.0544 F1: 63.23% dev loss: 0.1037 F1: 36.78% 0.03 min
> Epoch: 423 Step: 31800, train loss: 0.0548 F1: 63.43% dev loss: 0.1042 F1: 36.33% 0.03 min
> Epoch: 426 Step: 32000, train loss: 0.0548 F1: 65.32% dev loss: 0.1037 F1: 39.35% 0.03 min
> Epoch: 429 Step: 32200, train loss: 0.0539 F1: 63.43% dev loss: 0.1047 F1: 36.47% 0.03 min
> Epoch: 431 Step: 32400, train loss: 0.0544 F1: 63.18% dev loss: 0.1039 F1: 36.13% 0.03 min
> Epoch: 434 Step: 32600, train loss: 0.0545 F1: 65.59% dev loss: 0.1032 F1: 38.57% 0.03 min
> Epoch: 437 Step: 32800, train loss: 0.0534 F1: 64.87% dev loss: 0.1037 F1: 36.40% 0.03 min
> Epoch: 439 Step: 33000, train loss: 0.0531 F1: 64.77% dev loss: 0.1036 F1: 36.42% 0.03 min
> Epoch: 442 Step: 33200, train loss: 0.0536 F1: 65.73% dev loss: 0.1041 F1: 38.47% 0.03 min
> Epoch: 445 Step: 33400, train loss: 0.0528 F1: 67.00% dev loss: 0.1043 F1: 36.27% 0.03 min
> Epoch: 447 Step: 33600, train loss: 0.0529 F1: 65.65% dev loss: 0.1043 F1: 36.38% 0.03 min
> Epoch: 450 Step: 33800, train loss: 0.0529 F1: 66.57% dev loss: 0.1044 F1: 36.14% 0.03 min
> Epoch: 453 Step: 34000, train loss: 0.0525 F1: 66.00% dev loss: 0.1046 F1: 36.51% 0.03 min
> Epoch: 455 Step: 34200, train loss: 0.0523 F1: 66.03% dev loss: 0.1044 F1: 36.38% 0.03 min
> Epoch: 458 Step: 34400, train loss: 0.0528 F1: 66.22% dev loss: 0.1038 F1: 38.74% 0.03 min
> Epoch: 461 Step: 34600, train loss: 0.0516 F1: 66.14% dev loss: 0.1047 F1: 36.28% 0.03 min
> Epoch: 463 Step: 34800, train loss: 0.0521 F1: 65.60% dev loss: 0.1041 F1: 36.44% 0.03 min
> Epoch: 466 Step: 35000, train loss: 0.0521 F1: 67.20% dev loss: 0.1047 F1: 38.55% 0.03 min
> Epoch: 469 Step: 35200, train loss: 0.0513 F1: 66.59% dev loss: 0.1047 F1: 36.46% 0.03 min
> Epoch: 471 Step: 35400, train loss: 0.0513 F1: 66.88% dev loss: 0.1046 F1: 36.48% 0.03 min
> Epoch: 474 Step: 35600, train loss: 0.0518 F1: 67.18% dev loss: 0.1046 F1: 38.69% 0.03 min
> Epoch: 477 Step: 35800, train loss: 0.0511 F1: 68.44% dev loss: 0.1051 F1: 36.56% 0.03 min
> Epoch: 479 Step: 36000, train loss: 0.0512 F1: 65.97% dev loss: 0.1048 F1: 37.33% 0.03 min
> Epoch: 482 Step: 36200, train loss: 0.0509 F1: 67.75% dev loss: 0.1050 F1: 36.26% 0.03 min
> Epoch: 485 Step: 36400, train loss: 0.0506 F1: 67.04% dev loss: 0.1052 F1: 36.53% 0.03 min
> Epoch: 487 Step: 36600, train loss: 0.0509 F1: 65.04% dev loss: 0.1048 F1: 36.37% 0.03 min
> Epoch: 490 Step: 36800, train loss: 0.0510 F1: 66.47% dev loss: 0.1046 F1: 37.36% 0.03 min
> Epoch: 493 Step: 37000, train loss: 0.0503 F1: 64.59% dev loss: 0.1049 F1: 36.40% 0.03 min
> Epoch: 495 Step: 37200, train loss: 0.0508 F1: 66.20% dev loss: 0.1050 F1: 37.35% 0.03 min
> Epoch: 498 Step: 37400, train loss: 0.0507 F1: 68.56% dev loss: 0.1046 F1: 37.37% 0.03 min
> Epoch: 501 Step: 37600, train loss: 0.0501 F1: 66.90% dev loss: 0.1049 F1: 36.61% 0.03 min
> Epoch: 503 Step: 37800, train loss: 0.0506 F1: 66.93% dev loss: 0.1049 F1: 37.49% 0.03 min
> Epoch: 506 Step: 38000, train loss: 0.0507 F1: 67.41% dev loss: 0.1048 F1: 37.35% 0.03 min
> Epoch: 509 Step: 38200, train loss: 0.0497 F1: 68.43% dev loss: 0.1051 F1: 36.52% 0.03 min
> Epoch: 511 Step: 38400, train loss: 0.0506 F1: 66.65% dev loss: 0.1052 F1: 36.49% 0.03 min
> Epoch: 514 Step: 38600, train loss: 0.0501 F1: 68.34% dev loss: 0.1047 F1: 39.44% 0.03 min
> Epoch: 517 Step: 38800, train loss: 0.0494 F1: 66.07% dev loss: 0.1053 F1: 36.47% 0.03 min
> Epoch: 519 Step: 39000, train loss: 0.0495 F1: 67.10% dev loss: 0.1044 F1: 37.49% 0.03 min
> Epoch: 522 Step: 39200, train loss: 0.0500 F1: 68.11% dev loss: 0.1046 F1: 36.34% 0.03 min
> Epoch: 525 Step: 39400, train loss: 0.0492 F1: 67.82% dev loss: 0.1045 F1: 37.66% 0.03 min
> Epoch: 527 Step: 39600, train loss: 0.0495 F1: 67.06% dev loss: 0.1049 F1: 36.52% 0.03 min
> Epoch: 530 Step: 39800, train loss: 0.0492 F1: 67.04% dev loss: 0.1047 F1: 37.37% 0.03 min
> Epoch: 533 Step: 40000, train loss: 0.0494 F1: 68.89% dev loss: 0.1052 F1: 36.42% 0.03 min
> Epoch: 535 Step: 40200, train loss: 0.0495 F1: 67.48% dev loss: 0.1046 F1: 37.52% 0.03 min
> Epoch: 538 Step: 40400, train loss: 0.0492 F1: 67.24% dev loss: 0.1043 F1: 39.67% 0.03 min
> Epoch: 541 Step: 40600, train loss: 0.0485 F1: 67.09% dev loss: 0.1043 F1: 37.63% 0.03 min
> Epoch: 543 Step: 40800, train loss: 0.0487 F1: 67.15% dev loss: 0.1050 F1: 36.56% 0.03 min
> Epoch: 546 Step: 41000, train loss: 0.0486 F1: 68.31% dev loss: 0.1049 F1: 37.43% 0.03 min
> Epoch: 549 Step: 41200, train loss: 0.0482 F1: 67.54% dev loss: 0.1052 F1: 36.61% 0.03 min
> Epoch: 551 Step: 41400, train loss: 0.0486 F1: 68.00% dev loss: 0.1048 F1: 37.43% 0.03 min
> Epoch: 554 Step: 41600, train loss: 0.0489 F1: 69.33% dev loss: 0.1044 F1: 37.41% 0.03 min
> Epoch: 557 Step: 41800, train loss: 0.0483 F1: 69.14% dev loss: 0.1048 F1: 37.32% 0.03 min
> Epoch: 559 Step: 42000, train loss: 0.0484 F1: 69.18% dev loss: 0.1048 F1: 37.54% 0.03 min
> Epoch: 562 Step: 42200, train loss: 0.0481 F1: 68.30% dev loss: 0.1046 F1: 37.42% 0.03 min
> Epoch: 565 Step: 42400, train loss: 0.0482 F1: 65.68% dev loss: 0.1049 F1: 37.28% 0.03 min
> Epoch: 567 Step: 42600, train loss: 0.0479 F1: 68.62% dev loss: 0.1048 F1: 37.37% 0.03 min
> Epoch: 570 Step: 42800, train loss: 0.0483 F1: 68.01% dev loss: 0.1052 F1: 36.41% 0.03 min
> Epoch: 573 Step: 43000, train loss: 0.0481 F1: 67.51% dev loss: 0.1051 F1: 37.29% 0.03 min
> Epoch: 575 Step: 43200, train loss: 0.0477 F1: 67.81% dev loss: 0.1052 F1: 36.44% 0.03 min
> Epoch: 578 Step: 43400, train loss: 0.0479 F1: 69.01% dev loss: 0.1049 F1: 37.41% 0.03 min
> Epoch: 581 Step: 43600, train loss: 0.0482 F1: 69.72% dev loss: 0.1050 F1: 37.62% 0.03 min
> Epoch: 583 Step: 43800, train loss: 0.0475 F1: 68.36% dev loss: 0.1050 F1: 37.55% 0.03 min
> Epoch: 586 Step: 44000, train loss: 0.0481 F1: 68.29% dev loss: 0.1049 F1: 37.45% 0.03 min
> Epoch: 589 Step: 44200, train loss: 0.0474 F1: 67.37% dev loss: 0.1055 F1: 36.30% 0.03 min
> Epoch: 591 Step: 44400, train loss: 0.0479 F1: 68.95% dev loss: 0.1048 F1: 37.63% 0.03 min
> Epoch: 594 Step: 44600, train loss: 0.0479 F1: 69.21% dev loss: 0.1050 F1: 37.45% 0.03 min
> Epoch: 597 Step: 44800, train loss: 0.0474 F1: 67.46% dev loss: 0.1054 F1: 37.44% 0.03 min
> Epoch: 599 Step: 45000, train loss: 0.0477 F1: 68.71% dev loss: 0.1051 F1: 37.50% 0.03 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 15:58:56.547103>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 600
>>> step: 200
>>> model_name: glove
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 0.5
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/glove-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 50000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]
> Epoch: 2 Step: 200, train loss: 0.7074 F1: 9.82% dev loss: 0.6265 F1: 9.98% 0.04 min
> Epoch: 5 Step: 400, train loss: 0.4530 F1: 9.43% dev loss: 0.2130 F1: 9.49% 0.03 min
> Epoch: 7 Step: 600, train loss: 0.1563 F1: 9.45% dev loss: 0.1229 F1: 9.49% 0.03 min
> Epoch: 10 Step: 800, train loss: 0.1274 F1: 9.43% dev loss: 0.1099 F1: 9.49% 0.03 min
> Epoch: 13 Step: 1000, train loss: 0.1154 F1: 9.43% dev loss: 0.1035 F1: 9.49% 0.03 min
> Epoch: 15 Step: 1200, train loss: 0.1108 F1: 10.07% dev loss: 0.0994 F1: 9.49% 0.03 min
> Epoch: 18 Step: 1400, train loss: 0.1081 F1: 10.72% dev loss: 0.0963 F1: 10.29% 0.03 min
> Epoch: 21 Step: 1600, train loss: 0.1024 F1: 11.70% dev loss: 0.0937 F1: 10.87% 0.03 min
> Epoch: 23 Step: 1800, train loss: 0.1007 F1: 12.00% dev loss: 0.0917 F1: 11.38% 0.03 min
> Epoch: 26 Step: 2000, train loss: 0.0997 F1: 12.92% dev loss: 0.0899 F1: 12.71% 0.03 min
> Epoch: 29 Step: 2200, train loss: 0.0952 F1: 13.43% dev loss: 0.0881 F1: 13.01% 0.03 min
> Epoch: 31 Step: 2400, train loss: 0.0943 F1: 13.34% dev loss: 0.0865 F1: 13.20% 0.03 min
> Epoch: 34 Step: 2600, train loss: 0.0936 F1: 13.53% dev loss: 0.0850 F1: 13.13% 0.03 min
> Epoch: 37 Step: 2800, train loss: 0.0894 F1: 13.87% dev loss: 0.0834 F1: 13.26% 0.03 min
> Epoch: 39 Step: 3000, train loss: 0.0886 F1: 13.61% dev loss: 0.0819 F1: 13.35% 0.03 min
> Epoch: 42 Step: 3200, train loss: 0.0880 F1: 13.82% dev loss: 0.0804 F1: 13.78% 0.03 min
> Epoch: 45 Step: 3400, train loss: 0.0840 F1: 14.37% dev loss: 0.0789 F1: 14.00% 0.03 min
> Epoch: 47 Step: 3600, train loss: 0.0832 F1: 17.71% dev loss: 0.0773 F1: 14.20% 0.03 min
> Epoch: 50 Step: 3800, train loss: 0.0824 F1: 20.92% dev loss: 0.0760 F1: 16.67% 0.03 min
> Epoch: 53 Step: 4000, train loss: 0.0787 F1: 19.19% dev loss: 0.0746 F1: 16.22% 0.03 min
> Epoch: 55 Step: 4200, train loss: 0.0778 F1: 22.29% dev loss: 0.0732 F1: 17.36% 0.03 min
> Epoch: 58 Step: 4400, train loss: 0.0773 F1: 21.92% dev loss: 0.0721 F1: 19.99% 0.03 min
> Epoch: 61 Step: 4600, train loss: 0.0739 F1: 22.72% dev loss: 0.0710 F1: 19.99% 0.03 min
> Epoch: 63 Step: 4800, train loss: 0.0735 F1: 25.13% dev loss: 0.0698 F1: 21.11% 0.03 min
> Epoch: 66 Step: 5000, train loss: 0.0730 F1: 24.83% dev loss: 0.0689 F1: 23.05% 0.03 min
> Epoch: 69 Step: 5200, train loss: 0.0701 F1: 26.03% dev loss: 0.0679 F1: 22.89% 0.03 min
> Epoch: 71 Step: 5400, train loss: 0.0697 F1: 26.23% dev loss: 0.0670 F1: 24.38% 0.03 min
> Epoch: 74 Step: 5600, train loss: 0.0693 F1: 26.44% dev loss: 0.0662 F1: 25.00% 0.03 min
> Epoch: 77 Step: 5800, train loss: 0.0664 F1: 26.77% dev loss: 0.0653 F1: 24.88% 0.03 min
> Epoch: 79 Step: 6000, train loss: 0.0663 F1: 27.78% dev loss: 0.0645 F1: 25.50% 0.03 min
> Epoch: 82 Step: 6200, train loss: 0.0659 F1: 27.47% dev loss: 0.0640 F1: 26.13% 0.03 min
> Epoch: 85 Step: 6400, train loss: 0.0633 F1: 26.94% dev loss: 0.0631 F1: 26.10% 0.03 min
> Epoch: 87 Step: 6600, train loss: 0.0631 F1: 28.93% dev loss: 0.0625 F1: 26.76% 0.03 min
> Epoch: 90 Step: 6800, train loss: 0.0627 F1: 28.38% dev loss: 0.0621 F1: 26.41% 0.03 min
> Epoch: 93 Step: 7000, train loss: 0.0603 F1: 29.06% dev loss: 0.0613 F1: 27.25% 0.03 min
> Epoch: 95 Step: 7200, train loss: 0.0604 F1: 30.16% dev loss: 0.0609 F1: 27.68% 0.03 min
> Epoch: 98 Step: 7400, train loss: 0.0602 F1: 29.45% dev loss: 0.0606 F1: 29.31% 0.03 min
> Epoch: 101 Step: 7600, train loss: 0.0578 F1: 30.18% dev loss: 0.0599 F1: 28.24% 0.03 min
> Epoch: 103 Step: 7800, train loss: 0.0580 F1: 31.02% dev loss: 0.0595 F1: 29.30% 0.03 min
> Epoch: 106 Step: 8000, train loss: 0.0577 F1: 30.91% dev loss: 0.0593 F1: 29.53% 0.03 min
> Epoch: 109 Step: 8200, train loss: 0.0558 F1: 30.03% dev loss: 0.0589 F1: 29.80% 0.03 min
> Epoch: 111 Step: 8400, train loss: 0.0559 F1: 30.88% dev loss: 0.0585 F1: 30.33% 0.03 min
> Epoch: 114 Step: 8600, train loss: 0.0555 F1: 31.13% dev loss: 0.0584 F1: 30.20% 0.03 min
> Epoch: 117 Step: 8800, train loss: 0.0537 F1: 34.99% dev loss: 0.0580 F1: 30.89% 0.03 min
> Epoch: 119 Step: 9000, train loss: 0.0540 F1: 32.33% dev loss: 0.0577 F1: 30.94% 0.03 min
> Epoch: 122 Step: 9200, train loss: 0.0536 F1: 35.63% dev loss: 0.0577 F1: 30.11% 0.03 min
> Epoch: 125 Step: 9400, train loss: 0.0520 F1: 37.17% dev loss: 0.0574 F1: 31.41% 0.03 min
> Epoch: 127 Step: 9600, train loss: 0.0524 F1: 34.16% dev loss: 0.0572 F1: 31.13% 0.03 min
> Epoch: 130 Step: 9800, train loss: 0.0519 F1: 40.08% dev loss: 0.0570 F1: 31.32% 0.03 min
> Epoch: 133 Step: 10000, train loss: 0.0503 F1: 38.79% dev loss: 0.0567 F1: 31.39% 0.03 min
> Epoch: 135 Step: 10200, train loss: 0.0505 F1: 40.37% dev loss: 0.0563 F1: 30.54% 0.03 min
> Epoch: 138 Step: 10400, train loss: 0.0502 F1: 41.47% dev loss: 0.0562 F1: 31.66% 0.03 min
> Epoch: 141 Step: 10600, train loss: 0.0491 F1: 42.17% dev loss: 0.0560 F1: 31.32% 0.03 min
> Epoch: 143 Step: 10800, train loss: 0.0492 F1: 41.40% dev loss: 0.0558 F1: 31.42% 0.03 min
> Epoch: 146 Step: 11000, train loss: 0.0490 F1: 40.71% dev loss: 0.0557 F1: 32.08% 0.03 min
> Epoch: 149 Step: 11200, train loss: 0.0474 F1: 39.75% dev loss: 0.0554 F1: 31.52% 0.03 min
> Epoch: 151 Step: 11400, train loss: 0.0481 F1: 42.27% dev loss: 0.0549 F1: 31.74% 0.03 min
> Epoch: 154 Step: 11600, train loss: 0.0474 F1: 42.14% dev loss: 0.0551 F1: 33.28% 0.03 min
> Epoch: 157 Step: 11800, train loss: 0.0466 F1: 44.66% dev loss: 0.0549 F1: 32.24% 0.03 min
> Epoch: 159 Step: 12000, train loss: 0.0466 F1: 43.23% dev loss: 0.0548 F1: 33.18% 0.03 min
> Epoch: 162 Step: 12200, train loss: 0.0463 F1: 42.94% dev loss: 0.0546 F1: 33.35% 0.03 min
> Epoch: 165 Step: 12400, train loss: 0.0451 F1: 41.50% dev loss: 0.0546 F1: 32.55% 0.03 min
> Epoch: 167 Step: 12600, train loss: 0.0453 F1: 43.93% dev loss: 0.0545 F1: 33.23% 0.03 min
> Epoch: 170 Step: 12800, train loss: 0.0449 F1: 45.06% dev loss: 0.0542 F1: 33.72% 0.03 min
> Epoch: 173 Step: 13000, train loss: 0.0440 F1: 44.40% dev loss: 0.0543 F1: 33.48% 0.03 min
> Epoch: 175 Step: 13200, train loss: 0.0439 F1: 45.49% dev loss: 0.0541 F1: 33.56% 0.03 min
> Epoch: 178 Step: 13400, train loss: 0.0441 F1: 44.98% dev loss: 0.0542 F1: 33.48% 0.03 min
> Epoch: 181 Step: 13600, train loss: 0.0430 F1: 46.68% dev loss: 0.0539 F1: 33.67% 0.03 min
> Epoch: 183 Step: 13800, train loss: 0.0433 F1: 46.42% dev loss: 0.0539 F1: 33.97% 0.03 min
> Epoch: 186 Step: 14000, train loss: 0.0430 F1: 45.76% dev loss: 0.0536 F1: 36.37% 0.03 min
> Epoch: 189 Step: 14200, train loss: 0.0419 F1: 47.64% dev loss: 0.0535 F1: 33.71% 0.03 min
> Epoch: 191 Step: 14400, train loss: 0.0421 F1: 45.90% dev loss: 0.0535 F1: 34.05% 0.03 min
> Epoch: 194 Step: 14600, train loss: 0.0419 F1: 47.14% dev loss: 0.0533 F1: 36.33% 0.03 min
> Epoch: 197 Step: 14800, train loss: 0.0410 F1: 49.60% dev loss: 0.0532 F1: 36.73% 0.03 min
> Epoch: 199 Step: 15000, train loss: 0.0412 F1: 47.32% dev loss: 0.0531 F1: 37.02% 0.03 min
> Epoch: 202 Step: 15200, train loss: 0.0409 F1: 48.62% dev loss: 0.0529 F1: 36.67% 0.03 min
> Epoch: 205 Step: 15400, train loss: 0.0401 F1: 48.57% dev loss: 0.0534 F1: 37.19% 0.03 min
> Epoch: 207 Step: 15600, train loss: 0.0402 F1: 47.81% dev loss: 0.0531 F1: 37.01% 0.03 min
> Epoch: 210 Step: 15800, train loss: 0.0403 F1: 49.74% dev loss: 0.0528 F1: 36.94% 0.03 min
> Epoch: 213 Step: 16000, train loss: 0.0394 F1: 50.63% dev loss: 0.0529 F1: 37.08% 0.03 min
> Epoch: 215 Step: 16200, train loss: 0.0393 F1: 47.48% dev loss: 0.0527 F1: 37.53% 0.03 min
> Epoch: 218 Step: 16400, train loss: 0.0395 F1: 48.89% dev loss: 0.0527 F1: 36.90% 0.03 min
> Epoch: 221 Step: 16600, train loss: 0.0385 F1: 51.42% dev loss: 0.0527 F1: 37.42% 0.03 min
> Epoch: 223 Step: 16800, train loss: 0.0388 F1: 50.18% dev loss: 0.0523 F1: 38.58% 0.03 min
> Epoch: 226 Step: 17000, train loss: 0.0390 F1: 50.72% dev loss: 0.0524 F1: 38.09% 0.03 min
> Epoch: 229 Step: 17200, train loss: 0.0378 F1: 51.19% dev loss: 0.0526 F1: 37.51% 0.03 min
> Epoch: 231 Step: 17400, train loss: 0.0382 F1: 50.56% dev loss: 0.0523 F1: 38.94% 0.03 min
> Epoch: 234 Step: 17600, train loss: 0.0378 F1: 52.05% dev loss: 0.0526 F1: 38.06% 0.03 min
> Epoch: 237 Step: 17800, train loss: 0.0370 F1: 50.11% dev loss: 0.0525 F1: 37.17% 0.03 min
> Epoch: 239 Step: 18000, train loss: 0.0374 F1: 51.37% dev loss: 0.0521 F1: 38.87% 0.03 min
> Epoch: 242 Step: 18200, train loss: 0.0374 F1: 51.81% dev loss: 0.0521 F1: 38.02% 0.03 min
> Epoch: 245 Step: 18400, train loss: 0.0365 F1: 52.52% dev loss: 0.0521 F1: 38.68% 0.03 min
> Epoch: 247 Step: 18600, train loss: 0.0364 F1: 54.42% dev loss: 0.0520 F1: 38.80% 0.03 min
> Epoch: 250 Step: 18800, train loss: 0.0363 F1: 54.76% dev loss: 0.0520 F1: 38.72% 0.03 min
> Epoch: 253 Step: 19000, train loss: 0.0357 F1: 55.31% dev loss: 0.0522 F1: 38.68% 0.03 min
> Epoch: 255 Step: 19200, train loss: 0.0360 F1: 54.38% dev loss: 0.0518 F1: 38.95% 0.03 min
> Epoch: 258 Step: 19400, train loss: 0.0359 F1: 55.16% dev loss: 0.0518 F1: 38.43% 0.03 min
> Epoch: 261 Step: 19600, train loss: 0.0353 F1: 52.93% dev loss: 0.0519 F1: 38.60% 0.03 min
> Epoch: 263 Step: 19800, train loss: 0.0352 F1: 52.17% dev loss: 0.0517 F1: 38.73% 0.03 min
> Epoch: 266 Step: 20000, train loss: 0.0353 F1: 52.34% dev loss: 0.0518 F1: 38.73% 0.03 min
> Epoch: 269 Step: 20200, train loss: 0.0347 F1: 53.37% dev loss: 0.0518 F1: 38.21% 0.03 min
> Epoch: 271 Step: 20400, train loss: 0.0348 F1: 55.50% dev loss: 0.0516 F1: 38.34% 0.03 min
> Epoch: 274 Step: 20600, train loss: 0.0346 F1: 56.57% dev loss: 0.0515 F1: 38.59% 0.03 min
> Epoch: 277 Step: 20800, train loss: 0.0340 F1: 54.27% dev loss: 0.0520 F1: 38.41% 0.03 min
> Epoch: 279 Step: 21000, train loss: 0.0343 F1: 55.08% dev loss: 0.0516 F1: 38.36% 0.03 min
> Epoch: 282 Step: 21200, train loss: 0.0339 F1: 56.41% dev loss: 0.0518 F1: 38.56% 0.03 min
> Epoch: 285 Step: 21400, train loss: 0.0335 F1: 56.38% dev loss: 0.0519 F1: 38.26% 0.03 min
> Epoch: 287 Step: 21600, train loss: 0.0338 F1: 54.70% dev loss: 0.0517 F1: 38.49% 0.03 min
> Epoch: 290 Step: 21800, train loss: 0.0336 F1: 59.38% dev loss: 0.0515 F1: 37.32% 0.03 min
> Epoch: 293 Step: 22000, train loss: 0.0330 F1: 55.28% dev loss: 0.0520 F1: 38.51% 0.03 min
> Epoch: 295 Step: 22200, train loss: 0.0329 F1: 58.91% dev loss: 0.0516 F1: 37.69% 0.03 min
> Epoch: 298 Step: 22400, train loss: 0.0335 F1: 57.95% dev loss: 0.0520 F1: 37.23% 0.03 min
> Epoch: 301 Step: 22600, train loss: 0.0324 F1: 55.88% dev loss: 0.0520 F1: 38.35% 0.03 min
> Epoch: 303 Step: 22800, train loss: 0.0328 F1: 57.16% dev loss: 0.0518 F1: 36.86% 0.03 min
> Epoch: 306 Step: 23000, train loss: 0.0324 F1: 58.67% dev loss: 0.0519 F1: 37.54% 0.03 min
> Epoch: 309 Step: 23200, train loss: 0.0320 F1: 59.65% dev loss: 0.0517 F1: 37.40% 0.03 min
> Epoch: 311 Step: 23400, train loss: 0.0320 F1: 59.47% dev loss: 0.0515 F1: 36.79% 0.03 min
> Epoch: 314 Step: 23600, train loss: 0.0320 F1: 60.36% dev loss: 0.0517 F1: 37.52% 0.03 min
> Epoch: 317 Step: 23800, train loss: 0.0317 F1: 57.11% dev loss: 0.0520 F1: 36.85% 0.03 min
> Epoch: 319 Step: 24000, train loss: 0.0318 F1: 59.06% dev loss: 0.0518 F1: 36.97% 0.03 min
> Epoch: 322 Step: 24200, train loss: 0.0315 F1: 59.62% dev loss: 0.0517 F1: 39.99% 0.03 min
> Epoch: 325 Step: 24400, train loss: 0.0313 F1: 61.75% dev loss: 0.0518 F1: 37.37% 0.03 min
> Epoch: 327 Step: 24600, train loss: 0.0312 F1: 58.93% dev loss: 0.0516 F1: 36.62% 0.03 min
> Epoch: 330 Step: 24800, train loss: 0.0313 F1: 59.58% dev loss: 0.0516 F1: 37.34% 0.03 min
> Epoch: 333 Step: 25000, train loss: 0.0308 F1: 61.63% dev loss: 0.0518 F1: 36.55% 0.03 min
> Epoch: 335 Step: 25200, train loss: 0.0309 F1: 60.41% dev loss: 0.0521 F1: 36.53% 0.03 min
> Epoch: 338 Step: 25400, train loss: 0.0311 F1: 58.42% dev loss: 0.0517 F1: 39.99% 0.03 min
> Epoch: 341 Step: 25600, train loss: 0.0306 F1: 59.43% dev loss: 0.0518 F1: 36.62% 0.03 min
> Epoch: 343 Step: 25800, train loss: 0.0304 F1: 60.44% dev loss: 0.0519 F1: 36.26% 0.03 min
> Epoch: 346 Step: 26000, train loss: 0.0303 F1: 60.79% dev loss: 0.0517 F1: 40.23% 0.03 min
> Epoch: 349 Step: 26200, train loss: 0.0300 F1: 60.07% dev loss: 0.0518 F1: 36.76% 0.03 min
> Epoch: 351 Step: 26400, train loss: 0.0300 F1: 59.82% dev loss: 0.0516 F1: 36.57% 0.03 min
> Epoch: 354 Step: 26600, train loss: 0.0298 F1: 62.50% dev loss: 0.0515 F1: 39.40% 0.03 min
> Epoch: 357 Step: 26800, train loss: 0.0294 F1: 57.66% dev loss: 0.0519 F1: 36.51% 0.03 min
> Epoch: 359 Step: 27000, train loss: 0.0298 F1: 61.26% dev loss: 0.0515 F1: 36.48% 0.03 min
> Epoch: 362 Step: 27200, train loss: 0.0296 F1: 62.87% dev loss: 0.0515 F1: 36.62% 0.03 min
> Epoch: 365 Step: 27400, train loss: 0.0290 F1: 59.79% dev loss: 0.0514 F1: 37.30% 0.03 min
> Epoch: 367 Step: 27600, train loss: 0.0292 F1: 62.79% dev loss: 0.0517 F1: 36.59% 0.03 min
> Epoch: 370 Step: 27800, train loss: 0.0292 F1: 61.12% dev loss: 0.0515 F1: 39.27% 0.03 min
> Epoch: 373 Step: 28000, train loss: 0.0288 F1: 60.95% dev loss: 0.0518 F1: 36.55% 0.03 min
> Epoch: 375 Step: 28200, train loss: 0.0296 F1: 61.83% dev loss: 0.0519 F1: 36.17% 0.03 min
> Epoch: 378 Step: 28400, train loss: 0.0290 F1: 61.75% dev loss: 0.0518 F1: 39.04% 0.03 min
> Epoch: 381 Step: 28600, train loss: 0.0286 F1: 61.43% dev loss: 0.0523 F1: 36.40% 0.03 min
> Epoch: 383 Step: 28800, train loss: 0.0289 F1: 61.81% dev loss: 0.0522 F1: 36.18% 0.03 min
> Epoch: 386 Step: 29000, train loss: 0.0288 F1: 62.84% dev loss: 0.0518 F1: 39.25% 0.03 min
> Epoch: 389 Step: 29200, train loss: 0.0281 F1: 63.05% dev loss: 0.0520 F1: 36.66% 0.03 min
> Epoch: 391 Step: 29400, train loss: 0.0282 F1: 62.15% dev loss: 0.0518 F1: 36.28% 0.03 min
> Epoch: 394 Step: 29600, train loss: 0.0284 F1: 64.30% dev loss: 0.0519 F1: 38.62% 0.03 min
> Epoch: 397 Step: 29800, train loss: 0.0278 F1: 62.81% dev loss: 0.0517 F1: 36.40% 0.03 min
> Epoch: 399 Step: 30000, train loss: 0.0278 F1: 63.61% dev loss: 0.0519 F1: 36.18% 0.03 min
> Epoch: 402 Step: 30200, train loss: 0.0280 F1: 63.02% dev loss: 0.0520 F1: 39.10% 0.03 min
> Epoch: 405 Step: 30400, train loss: 0.0280 F1: 66.14% dev loss: 0.0520 F1: 36.46% 0.03 min
> Epoch: 407 Step: 30600, train loss: 0.0279 F1: 62.77% dev loss: 0.0518 F1: 36.35% 0.03 min
> Epoch: 410 Step: 30800, train loss: 0.0277 F1: 64.97% dev loss: 0.0516 F1: 39.53% 0.03 min
> Epoch: 413 Step: 31000, train loss: 0.0272 F1: 64.51% dev loss: 0.0518 F1: 36.81% 0.03 min
> Epoch: 415 Step: 31200, train loss: 0.0276 F1: 64.47% dev loss: 0.0520 F1: 36.49% 0.03 min
> Epoch: 418 Step: 31400, train loss: 0.0275 F1: 64.91% dev loss: 0.0518 F1: 36.45% 0.03 min
> Epoch: 421 Step: 31600, train loss: 0.0272 F1: 63.23% dev loss: 0.0519 F1: 36.78% 0.03 min
> Epoch: 423 Step: 31800, train loss: 0.0274 F1: 63.40% dev loss: 0.0521 F1: 36.33% 0.03 min
> Epoch: 426 Step: 32000, train loss: 0.0274 F1: 65.32% dev loss: 0.0519 F1: 39.35% 0.03 min
> Epoch: 429 Step: 32200, train loss: 0.0269 F1: 63.43% dev loss: 0.0523 F1: 36.47% 0.03 min
> Epoch: 431 Step: 32400, train loss: 0.0272 F1: 63.20% dev loss: 0.0520 F1: 36.13% 0.03 min
> Epoch: 434 Step: 32600, train loss: 0.0272 F1: 65.59% dev loss: 0.0516 F1: 38.57% 0.03 min
> Epoch: 437 Step: 32800, train loss: 0.0267 F1: 64.87% dev loss: 0.0518 F1: 36.40% 0.03 min
> Epoch: 439 Step: 33000, train loss: 0.0265 F1: 64.77% dev loss: 0.0518 F1: 36.42% 0.03 min
> Epoch: 442 Step: 33200, train loss: 0.0268 F1: 65.73% dev loss: 0.0521 F1: 38.47% 0.03 min
> Epoch: 445 Step: 33400, train loss: 0.0264 F1: 67.00% dev loss: 0.0522 F1: 36.27% 0.03 min
> Epoch: 447 Step: 33600, train loss: 0.0264 F1: 65.65% dev loss: 0.0522 F1: 36.38% 0.03 min
> Epoch: 450 Step: 33800, train loss: 0.0265 F1: 66.57% dev loss: 0.0522 F1: 36.14% 0.03 min
> Epoch: 453 Step: 34000, train loss: 0.0262 F1: 66.00% dev loss: 0.0523 F1: 36.51% 0.03 min
> Epoch: 455 Step: 34200, train loss: 0.0262 F1: 66.01% dev loss: 0.0522 F1: 36.38% 0.03 min
> Epoch: 458 Step: 34400, train loss: 0.0264 F1: 66.22% dev loss: 0.0519 F1: 38.74% 0.03 min
> Epoch: 461 Step: 34600, train loss: 0.0258 F1: 66.14% dev loss: 0.0523 F1: 36.28% 0.03 min
> Epoch: 463 Step: 34800, train loss: 0.0260 F1: 65.64% dev loss: 0.0520 F1: 36.44% 0.03 min
> Epoch: 466 Step: 35000, train loss: 0.0261 F1: 67.20% dev loss: 0.0523 F1: 38.55% 0.03 min
> Epoch: 469 Step: 35200, train loss: 0.0257 F1: 66.59% dev loss: 0.0523 F1: 36.46% 0.03 min
> Epoch: 471 Step: 35400, train loss: 0.0257 F1: 66.88% dev loss: 0.0523 F1: 36.48% 0.03 min
> Epoch: 474 Step: 35600, train loss: 0.0259 F1: 67.18% dev loss: 0.0523 F1: 38.69% 0.03 min
> Epoch: 477 Step: 35800, train loss: 0.0256 F1: 68.44% dev loss: 0.0525 F1: 36.56% 0.03 min
> Epoch: 479 Step: 36000, train loss: 0.0256 F1: 65.97% dev loss: 0.0524 F1: 37.33% 0.03 min
> Epoch: 482 Step: 36200, train loss: 0.0255 F1: 67.75% dev loss: 0.0525 F1: 36.26% 0.03 min
> Epoch: 485 Step: 36400, train loss: 0.0253 F1: 67.04% dev loss: 0.0526 F1: 36.53% 0.03 min
> Epoch: 487 Step: 36600, train loss: 0.0255 F1: 65.04% dev loss: 0.0524 F1: 36.37% 0.03 min
> Epoch: 490 Step: 36800, train loss: 0.0255 F1: 66.47% dev loss: 0.0523 F1: 37.36% 0.03 min
> Epoch: 493 Step: 37000, train loss: 0.0252 F1: 64.59% dev loss: 0.0524 F1: 36.40% 0.03 min
> Epoch: 495 Step: 37200, train loss: 0.0254 F1: 66.20% dev loss: 0.0525 F1: 37.35% 0.03 min
> Epoch: 498 Step: 37400, train loss: 0.0254 F1: 68.56% dev loss: 0.0523 F1: 37.37% 0.03 min
> Epoch: 501 Step: 37600, train loss: 0.0251 F1: 66.90% dev loss: 0.0525 F1: 36.61% 0.03 min
> Epoch: 503 Step: 37800, train loss: 0.0253 F1: 66.93% dev loss: 0.0524 F1: 37.49% 0.03 min
> Epoch: 506 Step: 38000, train loss: 0.0253 F1: 67.41% dev loss: 0.0524 F1: 37.35% 0.03 min
> Epoch: 509 Step: 38200, train loss: 0.0248 F1: 68.43% dev loss: 0.0526 F1: 36.52% 0.03 min
> Epoch: 511 Step: 38400, train loss: 0.0253 F1: 66.65% dev loss: 0.0526 F1: 36.49% 0.03 min
> Epoch: 514 Step: 38600, train loss: 0.0251 F1: 68.34% dev loss: 0.0523 F1: 39.44% 0.03 min
> Epoch: 517 Step: 38800, train loss: 0.0247 F1: 66.07% dev loss: 0.0526 F1: 36.47% 0.03 min
> Epoch: 519 Step: 39000, train loss: 0.0248 F1: 67.10% dev loss: 0.0522 F1: 37.49% 0.03 min
> Epoch: 522 Step: 39200, train loss: 0.0250 F1: 68.11% dev loss: 0.0523 F1: 36.34% 0.03 min
> Epoch: 525 Step: 39400, train loss: 0.0246 F1: 67.82% dev loss: 0.0523 F1: 37.66% 0.03 min
> Epoch: 527 Step: 39600, train loss: 0.0247 F1: 67.06% dev loss: 0.0525 F1: 36.52% 0.03 min
> Epoch: 530 Step: 39800, train loss: 0.0246 F1: 67.04% dev loss: 0.0523 F1: 37.37% 0.03 min
> Epoch: 533 Step: 40000, train loss: 0.0247 F1: 68.89% dev loss: 0.0526 F1: 36.42% 0.03 min
> Epoch: 535 Step: 40200, train loss: 0.0248 F1: 67.48% dev loss: 0.0523 F1: 37.52% 0.03 min
> Epoch: 538 Step: 40400, train loss: 0.0246 F1: 67.24% dev loss: 0.0521 F1: 39.67% 0.03 min
> Epoch: 541 Step: 40600, train loss: 0.0243 F1: 67.09% dev loss: 0.0522 F1: 37.63% 0.03 min
> Epoch: 543 Step: 40800, train loss: 0.0243 F1: 67.15% dev loss: 0.0525 F1: 36.56% 0.03 min
> Epoch: 546 Step: 41000, train loss: 0.0243 F1: 68.31% dev loss: 0.0525 F1: 37.43% 0.03 min
> Epoch: 549 Step: 41200, train loss: 0.0241 F1: 67.54% dev loss: 0.0526 F1: 36.61% 0.03 min
> Epoch: 551 Step: 41400, train loss: 0.0243 F1: 67.78% dev loss: 0.0524 F1: 37.43% 0.03 min
> Epoch: 554 Step: 41600, train loss: 0.0244 F1: 69.33% dev loss: 0.0522 F1: 37.41% 0.03 min
> Epoch: 557 Step: 41800, train loss: 0.0242 F1: 69.14% dev loss: 0.0524 F1: 37.32% 0.03 min
> Epoch: 559 Step: 42000, train loss: 0.0242 F1: 69.18% dev loss: 0.0524 F1: 37.54% 0.03 min
> Epoch: 562 Step: 42200, train loss: 0.0241 F1: 68.30% dev loss: 0.0523 F1: 37.42% 0.03 min
> Epoch: 565 Step: 42400, train loss: 0.0241 F1: 65.68% dev loss: 0.0525 F1: 37.28% 0.03 min
> Epoch: 567 Step: 42600, train loss: 0.0240 F1: 68.62% dev loss: 0.0524 F1: 37.37% 0.03 min
> Epoch: 570 Step: 42800, train loss: 0.0242 F1: 68.01% dev loss: 0.0526 F1: 36.41% 0.03 min
> Epoch: 573 Step: 43000, train loss: 0.0241 F1: 67.51% dev loss: 0.0525 F1: 37.29% 0.03 min
> Epoch: 575 Step: 43200, train loss: 0.0239 F1: 67.81% dev loss: 0.0526 F1: 36.44% 0.03 min
> Epoch: 578 Step: 43400, train loss: 0.0240 F1: 69.00% dev loss: 0.0525 F1: 37.41% 0.03 min
> Epoch: 581 Step: 43600, train loss: 0.0241 F1: 69.72% dev loss: 0.0525 F1: 37.62% 0.03 min
> Epoch: 583 Step: 43800, train loss: 0.0237 F1: 68.36% dev loss: 0.0525 F1: 37.55% 0.03 min
> Epoch: 586 Step: 44000, train loss: 0.0240 F1: 68.29% dev loss: 0.0525 F1: 37.45% 0.03 min
> Epoch: 589 Step: 44200, train loss: 0.0237 F1: 67.37% dev loss: 0.0527 F1: 36.30% 0.03 min
> Epoch: 591 Step: 44400, train loss: 0.0240 F1: 68.95% dev loss: 0.0524 F1: 37.63% 0.03 min
> Epoch: 594 Step: 44600, train loss: 0.0239 F1: 69.21% dev loss: 0.0525 F1: 37.45% 0.03 min
> Epoch: 597 Step: 44800, train loss: 0.0237 F1: 67.46% dev loss: 0.0527 F1: 37.44% 0.03 min
> Epoch: 599 Step: 45000, train loss: 0.0238 F1: 68.71% dev loss: 0.0525 F1: 37.50% 0.03 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 16:06:46.073886>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 600
>>> step: 200
>>> model_name: glove
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 0.2
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/glove-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 50000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]
> Epoch: 2 Step: 200, train loss: 0.2830 F1: 9.82% dev loss: 0.2506 F1: 9.98% 0.04 min
> Epoch: 5 Step: 400, train loss: 0.1812 F1: 9.43% dev loss: 0.0852 F1: 9.49% 0.03 min
> Epoch: 7 Step: 600, train loss: 0.0625 F1: 9.45% dev loss: 0.0492 F1: 9.49% 0.03 min
> Epoch: 10 Step: 800, train loss: 0.0510 F1: 9.43% dev loss: 0.0440 F1: 9.49% 0.03 min
> Epoch: 13 Step: 1000, train loss: 0.0462 F1: 9.43% dev loss: 0.0414 F1: 9.49% 0.03 min
> Epoch: 15 Step: 1200, train loss: 0.0443 F1: 10.07% dev loss: 0.0398 F1: 9.49% 0.03 min
> Epoch: 18 Step: 1400, train loss: 0.0432 F1: 10.72% dev loss: 0.0385 F1: 10.29% 0.03 min
> Epoch: 21 Step: 1600, train loss: 0.0410 F1: 11.70% dev loss: 0.0375 F1: 10.87% 0.03 min
> Epoch: 23 Step: 1800, train loss: 0.0403 F1: 12.00% dev loss: 0.0367 F1: 11.38% 0.03 min
> Epoch: 26 Step: 2000, train loss: 0.0399 F1: 12.92% dev loss: 0.0359 F1: 12.71% 0.03 min
> Epoch: 29 Step: 2200, train loss: 0.0381 F1: 13.43% dev loss: 0.0352 F1: 13.01% 0.03 min
> Epoch: 31 Step: 2400, train loss: 0.0377 F1: 13.34% dev loss: 0.0346 F1: 13.20% 0.03 min
> Epoch: 34 Step: 2600, train loss: 0.0374 F1: 13.53% dev loss: 0.0340 F1: 13.13% 0.03 min
> Epoch: 37 Step: 2800, train loss: 0.0358 F1: 13.87% dev loss: 0.0334 F1: 13.26% 0.03 min
> Epoch: 39 Step: 3000, train loss: 0.0354 F1: 13.61% dev loss: 0.0328 F1: 13.35% 0.03 min
> Epoch: 42 Step: 3200, train loss: 0.0352 F1: 13.82% dev loss: 0.0322 F1: 13.78% 0.03 min
> Epoch: 45 Step: 3400, train loss: 0.0336 F1: 14.37% dev loss: 0.0315 F1: 14.00% 0.03 min
> Epoch: 47 Step: 3600, train loss: 0.0333 F1: 17.71% dev loss: 0.0309 F1: 14.20% 0.03 min
> Epoch: 50 Step: 3800, train loss: 0.0330 F1: 20.92% dev loss: 0.0304 F1: 16.67% 0.03 min
> Epoch: 53 Step: 4000, train loss: 0.0315 F1: 19.19% dev loss: 0.0298 F1: 16.22% 0.03 min
> Epoch: 55 Step: 4200, train loss: 0.0311 F1: 22.29% dev loss: 0.0293 F1: 17.36% 0.03 min
> Epoch: 58 Step: 4400, train loss: 0.0309 F1: 21.92% dev loss: 0.0289 F1: 19.99% 0.03 min
> Epoch: 61 Step: 4600, train loss: 0.0296 F1: 22.72% dev loss: 0.0284 F1: 19.99% 0.03 min
> Epoch: 63 Step: 4800, train loss: 0.0294 F1: 25.13% dev loss: 0.0279 F1: 21.11% 0.03 min
> Epoch: 66 Step: 5000, train loss: 0.0292 F1: 24.83% dev loss: 0.0276 F1: 23.05% 0.03 min
> Epoch: 69 Step: 5200, train loss: 0.0280 F1: 26.03% dev loss: 0.0272 F1: 22.89% 0.03 min
> Epoch: 71 Step: 5400, train loss: 0.0279 F1: 26.23% dev loss: 0.0268 F1: 24.38% 0.03 min
> Epoch: 74 Step: 5600, train loss: 0.0277 F1: 26.44% dev loss: 0.0265 F1: 25.00% 0.03 min
> Epoch: 77 Step: 5800, train loss: 0.0266 F1: 26.77% dev loss: 0.0261 F1: 24.88% 0.03 min
> Epoch: 79 Step: 6000, train loss: 0.0265 F1: 27.75% dev loss: 0.0258 F1: 25.50% 0.03 min
> Epoch: 82 Step: 6200, train loss: 0.0264 F1: 27.43% dev loss: 0.0256 F1: 26.13% 0.03 min
> Epoch: 85 Step: 6400, train loss: 0.0253 F1: 26.94% dev loss: 0.0253 F1: 26.10% 0.03 min
> Epoch: 87 Step: 6600, train loss: 0.0253 F1: 28.93% dev loss: 0.0250 F1: 26.76% 0.03 min
> Epoch: 90 Step: 6800, train loss: 0.0251 F1: 28.38% dev loss: 0.0249 F1: 26.41% 0.03 min
> Epoch: 93 Step: 7000, train loss: 0.0241 F1: 29.06% dev loss: 0.0245 F1: 27.25% 0.03 min
> Epoch: 95 Step: 7200, train loss: 0.0242 F1: 30.16% dev loss: 0.0244 F1: 27.68% 0.03 min
> Epoch: 98 Step: 7400, train loss: 0.0241 F1: 29.45% dev loss: 0.0242 F1: 29.31% 0.03 min
> Epoch: 101 Step: 7600, train loss: 0.0231 F1: 30.18% dev loss: 0.0240 F1: 28.24% 0.03 min
> Epoch: 103 Step: 7800, train loss: 0.0232 F1: 31.02% dev loss: 0.0238 F1: 29.30% 0.03 min
> Epoch: 106 Step: 8000, train loss: 0.0231 F1: 30.91% dev loss: 0.0237 F1: 29.53% 0.03 min
> Epoch: 109 Step: 8200, train loss: 0.0223 F1: 30.03% dev loss: 0.0236 F1: 29.80% 0.03 min
> Epoch: 111 Step: 8400, train loss: 0.0224 F1: 30.88% dev loss: 0.0234 F1: 30.33% 0.03 min
> Epoch: 114 Step: 8600, train loss: 0.0222 F1: 31.13% dev loss: 0.0234 F1: 30.20% 0.03 min
> Epoch: 117 Step: 8800, train loss: 0.0215 F1: 34.99% dev loss: 0.0232 F1: 30.89% 0.03 min
> Epoch: 119 Step: 9000, train loss: 0.0216 F1: 32.33% dev loss: 0.0231 F1: 30.94% 0.03 min
> Epoch: 122 Step: 9200, train loss: 0.0215 F1: 35.63% dev loss: 0.0231 F1: 30.11% 0.03 min
> Epoch: 125 Step: 9400, train loss: 0.0208 F1: 37.17% dev loss: 0.0229 F1: 31.41% 0.03 min
> Epoch: 127 Step: 9600, train loss: 0.0209 F1: 34.16% dev loss: 0.0229 F1: 31.13% 0.03 min
> Epoch: 130 Step: 9800, train loss: 0.0208 F1: 40.08% dev loss: 0.0228 F1: 31.32% 0.03 min
> Epoch: 133 Step: 10000, train loss: 0.0201 F1: 38.79% dev loss: 0.0227 F1: 31.39% 0.03 min
> Epoch: 135 Step: 10200, train loss: 0.0202 F1: 40.37% dev loss: 0.0225 F1: 30.54% 0.03 min
> Epoch: 138 Step: 10400, train loss: 0.0201 F1: 41.47% dev loss: 0.0225 F1: 31.66% 0.03 min
> Epoch: 141 Step: 10600, train loss: 0.0197 F1: 42.17% dev loss: 0.0224 F1: 31.32% 0.03 min
> Epoch: 143 Step: 10800, train loss: 0.0197 F1: 41.40% dev loss: 0.0223 F1: 31.42% 0.03 min
> Epoch: 146 Step: 11000, train loss: 0.0196 F1: 40.73% dev loss: 0.0223 F1: 32.08% 0.03 min
> Epoch: 149 Step: 11200, train loss: 0.0190 F1: 39.75% dev loss: 0.0221 F1: 31.52% 0.03 min
> Epoch: 151 Step: 11400, train loss: 0.0192 F1: 42.27% dev loss: 0.0220 F1: 31.74% 0.03 min
> Epoch: 154 Step: 11600, train loss: 0.0190 F1: 42.14% dev loss: 0.0220 F1: 33.28% 0.03 min
> Epoch: 157 Step: 11800, train loss: 0.0186 F1: 44.66% dev loss: 0.0220 F1: 32.24% 0.03 min
> Epoch: 159 Step: 12000, train loss: 0.0186 F1: 43.23% dev loss: 0.0219 F1: 33.18% 0.03 min
> Epoch: 162 Step: 12200, train loss: 0.0185 F1: 42.94% dev loss: 0.0218 F1: 33.35% 0.03 min
> Epoch: 165 Step: 12400, train loss: 0.0180 F1: 41.50% dev loss: 0.0218 F1: 32.55% 0.03 min
> Epoch: 167 Step: 12600, train loss: 0.0181 F1: 43.93% dev loss: 0.0218 F1: 33.23% 0.03 min
> Epoch: 170 Step: 12800, train loss: 0.0179 F1: 45.06% dev loss: 0.0217 F1: 33.72% 0.03 min
> Epoch: 173 Step: 13000, train loss: 0.0176 F1: 44.40% dev loss: 0.0217 F1: 33.48% 0.03 min
> Epoch: 175 Step: 13200, train loss: 0.0176 F1: 45.48% dev loss: 0.0217 F1: 33.56% 0.03 min
> Epoch: 178 Step: 13400, train loss: 0.0176 F1: 44.98% dev loss: 0.0217 F1: 33.48% 0.03 min
> Epoch: 181 Step: 13600, train loss: 0.0172 F1: 46.68% dev loss: 0.0216 F1: 33.67% 0.03 min
> Epoch: 183 Step: 13800, train loss: 0.0173 F1: 46.42% dev loss: 0.0215 F1: 33.97% 0.03 min
> Epoch: 186 Step: 14000, train loss: 0.0172 F1: 45.76% dev loss: 0.0214 F1: 36.37% 0.03 min
> Epoch: 189 Step: 14200, train loss: 0.0168 F1: 47.64% dev loss: 0.0214 F1: 33.71% 0.03 min
> Epoch: 191 Step: 14400, train loss: 0.0168 F1: 45.90% dev loss: 0.0214 F1: 34.05% 0.03 min
> Epoch: 194 Step: 14600, train loss: 0.0168 F1: 47.14% dev loss: 0.0213 F1: 36.33% 0.03 min
> Epoch: 197 Step: 14800, train loss: 0.0164 F1: 49.60% dev loss: 0.0213 F1: 36.73% 0.03 min
> Epoch: 199 Step: 15000, train loss: 0.0165 F1: 47.32% dev loss: 0.0212 F1: 37.02% 0.03 min
> Epoch: 202 Step: 15200, train loss: 0.0164 F1: 48.62% dev loss: 0.0212 F1: 36.67% 0.03 min
> Epoch: 205 Step: 15400, train loss: 0.0160 F1: 48.58% dev loss: 0.0214 F1: 37.19% 0.03 min
> Epoch: 207 Step: 15600, train loss: 0.0161 F1: 47.81% dev loss: 0.0212 F1: 37.01% 0.03 min
> Epoch: 210 Step: 15800, train loss: 0.0161 F1: 49.74% dev loss: 0.0211 F1: 36.94% 0.03 min
> Epoch: 213 Step: 16000, train loss: 0.0158 F1: 50.63% dev loss: 0.0212 F1: 37.08% 0.03 min
> Epoch: 215 Step: 16200, train loss: 0.0157 F1: 47.48% dev loss: 0.0211 F1: 37.53% 0.03 min
> Epoch: 218 Step: 16400, train loss: 0.0158 F1: 48.88% dev loss: 0.0211 F1: 36.90% 0.03 min
> Epoch: 221 Step: 16600, train loss: 0.0154 F1: 51.42% dev loss: 0.0211 F1: 37.42% 0.03 min
> Epoch: 223 Step: 16800, train loss: 0.0155 F1: 50.18% dev loss: 0.0209 F1: 38.58% 0.03 min
> Epoch: 226 Step: 17000, train loss: 0.0156 F1: 50.72% dev loss: 0.0210 F1: 38.09% 0.03 min
> Epoch: 229 Step: 17200, train loss: 0.0151 F1: 51.24% dev loss: 0.0210 F1: 37.45% 0.03 min
> Epoch: 231 Step: 17400, train loss: 0.0153 F1: 50.56% dev loss: 0.0209 F1: 38.94% 0.03 min
> Epoch: 234 Step: 17600, train loss: 0.0151 F1: 52.05% dev loss: 0.0210 F1: 38.06% 0.03 min
> Epoch: 237 Step: 17800, train loss: 0.0148 F1: 50.11% dev loss: 0.0210 F1: 37.17% 0.03 min
> Epoch: 239 Step: 18000, train loss: 0.0149 F1: 51.37% dev loss: 0.0209 F1: 38.87% 0.03 min
> Epoch: 242 Step: 18200, train loss: 0.0150 F1: 51.81% dev loss: 0.0208 F1: 38.02% 0.03 min
> Epoch: 245 Step: 18400, train loss: 0.0146 F1: 52.52% dev loss: 0.0208 F1: 38.68% 0.03 min
> Epoch: 247 Step: 18600, train loss: 0.0146 F1: 54.42% dev loss: 0.0208 F1: 38.80% 0.03 min
> Epoch: 250 Step: 18800, train loss: 0.0145 F1: 54.76% dev loss: 0.0208 F1: 38.72% 0.03 min
> Epoch: 253 Step: 19000, train loss: 0.0143 F1: 55.31% dev loss: 0.0209 F1: 38.68% 0.03 min
> Epoch: 255 Step: 19200, train loss: 0.0144 F1: 54.38% dev loss: 0.0207 F1: 38.95% 0.03 min
> Epoch: 258 Step: 19400, train loss: 0.0144 F1: 55.11% dev loss: 0.0207 F1: 38.43% 0.03 min
> Epoch: 261 Step: 19600, train loss: 0.0141 F1: 52.93% dev loss: 0.0207 F1: 38.60% 0.03 min
> Epoch: 263 Step: 19800, train loss: 0.0141 F1: 52.17% dev loss: 0.0207 F1: 38.73% 0.03 min
> Epoch: 266 Step: 20000, train loss: 0.0141 F1: 52.34% dev loss: 0.0207 F1: 38.73% 0.03 min
> Epoch: 269 Step: 20200, train loss: 0.0139 F1: 53.37% dev loss: 0.0207 F1: 38.21% 0.03 min
> Epoch: 271 Step: 20400, train loss: 0.0139 F1: 55.51% dev loss: 0.0206 F1: 38.34% 0.03 min
> Epoch: 274 Step: 20600, train loss: 0.0138 F1: 56.57% dev loss: 0.0206 F1: 38.59% 0.03 min
> Epoch: 277 Step: 20800, train loss: 0.0136 F1: 54.27% dev loss: 0.0208 F1: 38.41% 0.03 min
> Epoch: 279 Step: 21000, train loss: 0.0137 F1: 55.10% dev loss: 0.0206 F1: 38.36% 0.03 min
> Epoch: 282 Step: 21200, train loss: 0.0136 F1: 56.41% dev loss: 0.0207 F1: 38.59% 0.03 min
> Epoch: 285 Step: 21400, train loss: 0.0134 F1: 56.38% dev loss: 0.0208 F1: 38.26% 0.03 min
> Epoch: 287 Step: 21600, train loss: 0.0135 F1: 54.70% dev loss: 0.0207 F1: 38.49% 0.03 min
> Epoch: 290 Step: 21800, train loss: 0.0134 F1: 59.38% dev loss: 0.0206 F1: 37.32% 0.03 min
> Epoch: 293 Step: 22000, train loss: 0.0132 F1: 55.28% dev loss: 0.0208 F1: 38.51% 0.03 min
> Epoch: 295 Step: 22200, train loss: 0.0132 F1: 58.91% dev loss: 0.0206 F1: 37.69% 0.03 min
> Epoch: 298 Step: 22400, train loss: 0.0134 F1: 57.97% dev loss: 0.0208 F1: 37.23% 0.03 min
> Epoch: 301 Step: 22600, train loss: 0.0130 F1: 55.88% dev loss: 0.0208 F1: 38.35% 0.03 min
> Epoch: 303 Step: 22800, train loss: 0.0131 F1: 57.16% dev loss: 0.0207 F1: 36.86% 0.03 min
> Epoch: 306 Step: 23000, train loss: 0.0130 F1: 58.67% dev loss: 0.0207 F1: 37.54% 0.03 min
> Epoch: 309 Step: 23200, train loss: 0.0128 F1: 59.65% dev loss: 0.0207 F1: 37.40% 0.03 min
> Epoch: 311 Step: 23400, train loss: 0.0128 F1: 59.47% dev loss: 0.0206 F1: 36.79% 0.03 min
> Epoch: 314 Step: 23600, train loss: 0.0128 F1: 60.36% dev loss: 0.0207 F1: 37.52% 0.03 min
> Epoch: 317 Step: 23800, train loss: 0.0127 F1: 57.11% dev loss: 0.0208 F1: 36.85% 0.03 min
> Epoch: 319 Step: 24000, train loss: 0.0127 F1: 59.06% dev loss: 0.0207 F1: 36.96% 0.03 min
> Epoch: 322 Step: 24200, train loss: 0.0126 F1: 59.62% dev loss: 0.0207 F1: 39.99% 0.03 min
> Epoch: 325 Step: 24400, train loss: 0.0125 F1: 61.75% dev loss: 0.0207 F1: 37.37% 0.03 min
> Epoch: 327 Step: 24600, train loss: 0.0125 F1: 58.92% dev loss: 0.0206 F1: 36.62% 0.03 min
> Epoch: 330 Step: 24800, train loss: 0.0125 F1: 59.58% dev loss: 0.0206 F1: 37.34% 0.03 min
> Epoch: 333 Step: 25000, train loss: 0.0123 F1: 61.91% dev loss: 0.0207 F1: 36.55% 0.03 min
> Epoch: 335 Step: 25200, train loss: 0.0124 F1: 60.41% dev loss: 0.0208 F1: 36.53% 0.03 min
> Epoch: 338 Step: 25400, train loss: 0.0125 F1: 58.42% dev loss: 0.0207 F1: 39.99% 0.03 min
> Epoch: 341 Step: 25600, train loss: 0.0122 F1: 59.33% dev loss: 0.0207 F1: 36.62% 0.03 min
> Epoch: 343 Step: 25800, train loss: 0.0122 F1: 60.43% dev loss: 0.0208 F1: 36.26% 0.03 min
> Epoch: 346 Step: 26000, train loss: 0.0121 F1: 60.79% dev loss: 0.0207 F1: 40.23% 0.03 min
> Epoch: 349 Step: 26200, train loss: 0.0120 F1: 60.07% dev loss: 0.0207 F1: 36.76% 0.03 min
> Epoch: 351 Step: 26400, train loss: 0.0120 F1: 59.82% dev loss: 0.0206 F1: 36.57% 0.03 min
> Epoch: 354 Step: 26600, train loss: 0.0119 F1: 62.53% dev loss: 0.0206 F1: 39.40% 0.03 min
> Epoch: 357 Step: 26800, train loss: 0.0118 F1: 57.66% dev loss: 0.0208 F1: 36.51% 0.03 min
> Epoch: 359 Step: 27000, train loss: 0.0119 F1: 61.02% dev loss: 0.0206 F1: 36.48% 0.03 min
> Epoch: 362 Step: 27200, train loss: 0.0118 F1: 62.87% dev loss: 0.0206 F1: 36.62% 0.03 min
> Epoch: 365 Step: 27400, train loss: 0.0116 F1: 59.79% dev loss: 0.0206 F1: 37.30% 0.03 min
> Epoch: 367 Step: 27600, train loss: 0.0117 F1: 62.79% dev loss: 0.0207 F1: 36.59% 0.03 min
> Epoch: 370 Step: 27800, train loss: 0.0117 F1: 61.12% dev loss: 0.0206 F1: 39.27% 0.03 min
> Epoch: 373 Step: 28000, train loss: 0.0115 F1: 60.95% dev loss: 0.0207 F1: 36.55% 0.03 min
> Epoch: 375 Step: 28200, train loss: 0.0118 F1: 61.83% dev loss: 0.0208 F1: 36.17% 0.03 min
> Epoch: 378 Step: 28400, train loss: 0.0116 F1: 61.75% dev loss: 0.0207 F1: 39.04% 0.03 min
> Epoch: 381 Step: 28600, train loss: 0.0114 F1: 61.43% dev loss: 0.0209 F1: 36.40% 0.03 min
> Epoch: 383 Step: 28800, train loss: 0.0116 F1: 61.81% dev loss: 0.0209 F1: 36.18% 0.03 min
> Epoch: 386 Step: 29000, train loss: 0.0115 F1: 62.84% dev loss: 0.0207 F1: 39.25% 0.03 min
> Epoch: 389 Step: 29200, train loss: 0.0112 F1: 63.05% dev loss: 0.0208 F1: 36.66% 0.03 min
> Epoch: 391 Step: 29400, train loss: 0.0113 F1: 62.15% dev loss: 0.0207 F1: 36.28% 0.03 min
> Epoch: 394 Step: 29600, train loss: 0.0114 F1: 64.30% dev loss: 0.0208 F1: 38.62% 0.03 min
> Epoch: 397 Step: 29800, train loss: 0.0111 F1: 62.81% dev loss: 0.0207 F1: 36.40% 0.03 min
> Epoch: 399 Step: 30000, train loss: 0.0111 F1: 63.61% dev loss: 0.0207 F1: 36.18% 0.03 min
> Epoch: 402 Step: 30200, train loss: 0.0112 F1: 63.02% dev loss: 0.0208 F1: 39.10% 0.03 min
> Epoch: 405 Step: 30400, train loss: 0.0112 F1: 66.14% dev loss: 0.0208 F1: 36.46% 0.03 min
> Epoch: 407 Step: 30600, train loss: 0.0112 F1: 62.77% dev loss: 0.0207 F1: 36.35% 0.03 min
> Epoch: 410 Step: 30800, train loss: 0.0111 F1: 64.83% dev loss: 0.0207 F1: 39.53% 0.03 min
> Epoch: 413 Step: 31000, train loss: 0.0109 F1: 64.51% dev loss: 0.0207 F1: 36.81% 0.03 min
> Epoch: 415 Step: 31200, train loss: 0.0110 F1: 64.48% dev loss: 0.0208 F1: 36.49% 0.03 min
> Epoch: 418 Step: 31400, train loss: 0.0110 F1: 64.91% dev loss: 0.0207 F1: 36.45% 0.03 min
> Epoch: 421 Step: 31600, train loss: 0.0109 F1: 63.23% dev loss: 0.0207 F1: 36.78% 0.03 min
> Epoch: 423 Step: 31800, train loss: 0.0110 F1: 63.39% dev loss: 0.0208 F1: 36.33% 0.03 min
> Epoch: 426 Step: 32000, train loss: 0.0110 F1: 65.32% dev loss: 0.0207 F1: 39.35% 0.03 min
> Epoch: 429 Step: 32200, train loss: 0.0108 F1: 63.43% dev loss: 0.0209 F1: 36.47% 0.03 min
> Epoch: 431 Step: 32400, train loss: 0.0109 F1: 63.20% dev loss: 0.0208 F1: 36.13% 0.03 min
> Epoch: 434 Step: 32600, train loss: 0.0109 F1: 65.59% dev loss: 0.0206 F1: 38.57% 0.03 min
> Epoch: 437 Step: 32800, train loss: 0.0107 F1: 64.87% dev loss: 0.0207 F1: 36.40% 0.03 min
> Epoch: 439 Step: 33000, train loss: 0.0106 F1: 64.77% dev loss: 0.0207 F1: 36.42% 0.03 min
> Epoch: 442 Step: 33200, train loss: 0.0107 F1: 65.73% dev loss: 0.0208 F1: 38.47% 0.03 min
> Epoch: 445 Step: 33400, train loss: 0.0106 F1: 67.00% dev loss: 0.0209 F1: 36.27% 0.03 min
> Epoch: 447 Step: 33600, train loss: 0.0106 F1: 65.65% dev loss: 0.0209 F1: 36.38% 0.03 min
> Epoch: 450 Step: 33800, train loss: 0.0106 F1: 66.57% dev loss: 0.0209 F1: 36.14% 0.03 min
> Epoch: 453 Step: 34000, train loss: 0.0105 F1: 66.00% dev loss: 0.0209 F1: 36.51% 0.03 min
> Epoch: 455 Step: 34200, train loss: 0.0105 F1: 65.99% dev loss: 0.0209 F1: 36.38% 0.03 min
> Epoch: 458 Step: 34400, train loss: 0.0106 F1: 66.22% dev loss: 0.0208 F1: 38.74% 0.03 min
> Epoch: 461 Step: 34600, train loss: 0.0103 F1: 66.14% dev loss: 0.0209 F1: 36.28% 0.03 min
> Epoch: 463 Step: 34800, train loss: 0.0104 F1: 65.64% dev loss: 0.0208 F1: 36.44% 0.03 min
> Epoch: 466 Step: 35000, train loss: 0.0104 F1: 67.20% dev loss: 0.0209 F1: 38.52% 0.03 min
> Epoch: 469 Step: 35200, train loss: 0.0103 F1: 66.59% dev loss: 0.0209 F1: 36.46% 0.03 min
> Epoch: 471 Step: 35400, train loss: 0.0103 F1: 66.88% dev loss: 0.0209 F1: 36.49% 0.03 min
> Epoch: 474 Step: 35600, train loss: 0.0104 F1: 67.18% dev loss: 0.0209 F1: 38.69% 0.03 min
> Epoch: 477 Step: 35800, train loss: 0.0102 F1: 68.44% dev loss: 0.0210 F1: 36.56% 0.03 min
> Epoch: 479 Step: 36000, train loss: 0.0102 F1: 65.97% dev loss: 0.0210 F1: 37.33% 0.03 min
> Epoch: 482 Step: 36200, train loss: 0.0102 F1: 67.74% dev loss: 0.0210 F1: 36.26% 0.03 min
> Epoch: 485 Step: 36400, train loss: 0.0101 F1: 67.04% dev loss: 0.0210 F1: 36.53% 0.03 min
> Epoch: 487 Step: 36600, train loss: 0.0102 F1: 65.02% dev loss: 0.0210 F1: 36.37% 0.03 min
> Epoch: 490 Step: 36800, train loss: 0.0102 F1: 66.47% dev loss: 0.0209 F1: 37.36% 0.03 min
> Epoch: 493 Step: 37000, train loss: 0.0101 F1: 64.59% dev loss: 0.0210 F1: 36.40% 0.03 min
> Epoch: 495 Step: 37200, train loss: 0.0102 F1: 66.20% dev loss: 0.0210 F1: 37.35% 0.03 min
> Epoch: 498 Step: 37400, train loss: 0.0102 F1: 68.55% dev loss: 0.0209 F1: 37.37% 0.03 min
> Epoch: 501 Step: 37600, train loss: 0.0100 F1: 66.90% dev loss: 0.0210 F1: 36.53% 0.03 min
> Epoch: 503 Step: 37800, train loss: 0.0101 F1: 66.92% dev loss: 0.0210 F1: 37.49% 0.03 min
> Epoch: 506 Step: 38000, train loss: 0.0101 F1: 67.54% dev loss: 0.0210 F1: 37.35% 0.03 min
> Epoch: 509 Step: 38200, train loss: 0.0099 F1: 68.43% dev loss: 0.0210 F1: 36.52% 0.03 min
> Epoch: 511 Step: 38400, train loss: 0.0101 F1: 66.65% dev loss: 0.0210 F1: 36.49% 0.03 min
> Epoch: 514 Step: 38600, train loss: 0.0100 F1: 68.34% dev loss: 0.0209 F1: 39.44% 0.03 min
> Epoch: 517 Step: 38800, train loss: 0.0099 F1: 66.07% dev loss: 0.0211 F1: 36.47% 0.03 min
> Epoch: 519 Step: 39000, train loss: 0.0099 F1: 67.08% dev loss: 0.0209 F1: 37.49% 0.03 min
> Epoch: 522 Step: 39200, train loss: 0.0100 F1: 68.11% dev loss: 0.0209 F1: 36.34% 0.03 min
> Epoch: 525 Step: 39400, train loss: 0.0098 F1: 67.82% dev loss: 0.0209 F1: 37.66% 0.03 min
> Epoch: 527 Step: 39600, train loss: 0.0099 F1: 67.06% dev loss: 0.0210 F1: 36.52% 0.03 min
> Epoch: 530 Step: 39800, train loss: 0.0098 F1: 67.04% dev loss: 0.0209 F1: 37.37% 0.03 min
> Epoch: 533 Step: 40000, train loss: 0.0099 F1: 68.88% dev loss: 0.0210 F1: 36.42% 0.03 min
> Epoch: 535 Step: 40200, train loss: 0.0099 F1: 67.48% dev loss: 0.0209 F1: 37.52% 0.03 min
> Epoch: 538 Step: 40400, train loss: 0.0098 F1: 67.24% dev loss: 0.0209 F1: 39.67% 0.03 min
> Epoch: 541 Step: 40600, train loss: 0.0097 F1: 67.09% dev loss: 0.0209 F1: 37.63% 0.03 min
> Epoch: 543 Step: 40800, train loss: 0.0097 F1: 67.16% dev loss: 0.0210 F1: 36.56% 0.03 min
> Epoch: 546 Step: 41000, train loss: 0.0097 F1: 68.31% dev loss: 0.0210 F1: 37.43% 0.03 min
> Epoch: 549 Step: 41200, train loss: 0.0097 F1: 67.54% dev loss: 0.0210 F1: 36.61% 0.03 min
> Epoch: 551 Step: 41400, train loss: 0.0097 F1: 67.78% dev loss: 0.0210 F1: 37.43% 0.03 min
> Epoch: 554 Step: 41600, train loss: 0.0098 F1: 69.32% dev loss: 0.0209 F1: 37.41% 0.03 min
> Epoch: 557 Step: 41800, train loss: 0.0097 F1: 69.14% dev loss: 0.0210 F1: 37.32% 0.03 min
> Epoch: 559 Step: 42000, train loss: 0.0097 F1: 69.18% dev loss: 0.0210 F1: 37.54% 0.03 min
> Epoch: 562 Step: 42200, train loss: 0.0096 F1: 68.31% dev loss: 0.0209 F1: 37.42% 0.03 min
> Epoch: 565 Step: 42400, train loss: 0.0097 F1: 65.70% dev loss: 0.0210 F1: 37.28% 0.03 min
> Epoch: 567 Step: 42600, train loss: 0.0096 F1: 68.62% dev loss: 0.0210 F1: 37.37% 0.03 min
> Epoch: 570 Step: 42800, train loss: 0.0097 F1: 68.01% dev loss: 0.0210 F1: 36.41% 0.03 min
> Epoch: 573 Step: 43000, train loss: 0.0096 F1: 67.50% dev loss: 0.0210 F1: 36.41% 0.03 min
> Epoch: 575 Step: 43200, train loss: 0.0095 F1: 67.81% dev loss: 0.0210 F1: 36.44% 0.03 min
> Epoch: 578 Step: 43400, train loss: 0.0096 F1: 69.00% dev loss: 0.0210 F1: 37.41% 0.03 min
> Epoch: 581 Step: 43600, train loss: 0.0096 F1: 69.72% dev loss: 0.0210 F1: 37.62% 0.03 min
> Epoch: 583 Step: 43800, train loss: 0.0095 F1: 68.34% dev loss: 0.0210 F1: 37.55% 0.03 min
> Epoch: 586 Step: 44000, train loss: 0.0096 F1: 68.29% dev loss: 0.0210 F1: 37.45% 0.03 min
> Epoch: 589 Step: 44200, train loss: 0.0095 F1: 67.37% dev loss: 0.0211 F1: 36.30% 0.03 min
> Epoch: 591 Step: 44400, train loss: 0.0096 F1: 68.93% dev loss: 0.0210 F1: 37.63% 0.03 min
> Epoch: 594 Step: 44600, train loss: 0.0096 F1: 69.21% dev loss: 0.0210 F1: 37.45% 0.03 min
> Epoch: 597 Step: 44800, train loss: 0.0095 F1: 67.47% dev loss: 0.0211 F1: 37.44% 0.03 min
> Epoch: 599 Step: 45000, train loss: 0.0095 F1: 68.70% dev loss: 0.0210 F1: 37.50% 0.03 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 16:13:35.258475>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 400
>>> step: 200
>>> model_name: glove
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 0.1
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/glove-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 50000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
> Epoch: 2 Step: 200, train loss: 0.1415 F1: 9.82% dev loss: 0.1253 F1: 9.98% 0.04 min
> Epoch: 5 Step: 400, train loss: 0.0906 F1: 9.43% dev loss: 0.0426 F1: 9.49% 0.03 min
> Epoch: 7 Step: 600, train loss: 0.0313 F1: 9.45% dev loss: 0.0246 F1: 9.49% 0.03 min
> Epoch: 10 Step: 800, train loss: 0.0255 F1: 9.43% dev loss: 0.0220 F1: 9.49% 0.03 min
> Epoch: 13 Step: 1000, train loss: 0.0231 F1: 9.43% dev loss: 0.0207 F1: 9.49% 0.03 min
> Epoch: 15 Step: 1200, train loss: 0.0222 F1: 10.07% dev loss: 0.0199 F1: 9.49% 0.03 min
> Epoch: 18 Step: 1400, train loss: 0.0216 F1: 10.72% dev loss: 0.0193 F1: 10.29% 0.03 min
> Epoch: 21 Step: 1600, train loss: 0.0205 F1: 11.70% dev loss: 0.0187 F1: 10.87% 0.03 min
> Epoch: 23 Step: 1800, train loss: 0.0201 F1: 12.00% dev loss: 0.0183 F1: 11.38% 0.03 min
> Epoch: 26 Step: 2000, train loss: 0.0199 F1: 12.92% dev loss: 0.0180 F1: 12.71% 0.03 min
> Epoch: 29 Step: 2200, train loss: 0.0190 F1: 13.43% dev loss: 0.0176 F1: 13.01% 0.03 min
> Epoch: 31 Step: 2400, train loss: 0.0189 F1: 13.34% dev loss: 0.0173 F1: 13.20% 0.03 min
> Epoch: 34 Step: 2600, train loss: 0.0187 F1: 13.52% dev loss: 0.0170 F1: 13.13% 0.03 min
> Epoch: 37 Step: 2800, train loss: 0.0179 F1: 13.87% dev loss: 0.0167 F1: 13.26% 0.03 min
> Epoch: 39 Step: 3000, train loss: 0.0177 F1: 13.60% dev loss: 0.0164 F1: 13.35% 0.03 min
> Epoch: 42 Step: 3200, train loss: 0.0176 F1: 13.82% dev loss: 0.0161 F1: 13.79% 0.03 min
> Epoch: 45 Step: 3400, train loss: 0.0168 F1: 14.37% dev loss: 0.0158 F1: 14.00% 0.03 min
> Epoch: 47 Step: 3600, train loss: 0.0166 F1: 17.71% dev loss: 0.0155 F1: 14.20% 0.03 min
> Epoch: 50 Step: 3800, train loss: 0.0165 F1: 20.92% dev loss: 0.0152 F1: 16.67% 0.03 min
> Epoch: 53 Step: 4000, train loss: 0.0157 F1: 19.19% dev loss: 0.0149 F1: 16.22% 0.03 min
> Epoch: 55 Step: 4200, train loss: 0.0156 F1: 22.29% dev loss: 0.0146 F1: 17.36% 0.03 min
> Epoch: 58 Step: 4400, train loss: 0.0155 F1: 21.89% dev loss: 0.0144 F1: 19.99% 0.03 min
> Epoch: 61 Step: 4600, train loss: 0.0148 F1: 22.72% dev loss: 0.0142 F1: 19.99% 0.03 min
> Epoch: 63 Step: 4800, train loss: 0.0147 F1: 25.13% dev loss: 0.0140 F1: 21.11% 0.03 min
> Epoch: 66 Step: 5000, train loss: 0.0146 F1: 24.82% dev loss: 0.0138 F1: 23.05% 0.03 min
> Epoch: 69 Step: 5200, train loss: 0.0140 F1: 26.03% dev loss: 0.0136 F1: 22.89% 0.03 min
> Epoch: 71 Step: 5400, train loss: 0.0139 F1: 26.23% dev loss: 0.0134 F1: 24.38% 0.03 min
> Epoch: 74 Step: 5600, train loss: 0.0139 F1: 26.44% dev loss: 0.0132 F1: 25.00% 0.03 min
> Epoch: 77 Step: 5800, train loss: 0.0133 F1: 26.77% dev loss: 0.0131 F1: 24.88% 0.03 min
> Epoch: 79 Step: 6000, train loss: 0.0133 F1: 27.75% dev loss: 0.0129 F1: 25.50% 0.03 min
> Epoch: 82 Step: 6200, train loss: 0.0132 F1: 27.43% dev loss: 0.0128 F1: 26.13% 0.03 min
> Epoch: 85 Step: 6400, train loss: 0.0127 F1: 26.94% dev loss: 0.0126 F1: 26.10% 0.03 min
> Epoch: 87 Step: 6600, train loss: 0.0126 F1: 28.93% dev loss: 0.0125 F1: 26.76% 0.03 min
> Epoch: 90 Step: 6800, train loss: 0.0125 F1: 28.37% dev loss: 0.0124 F1: 26.41% 0.03 min
> Epoch: 93 Step: 7000, train loss: 0.0121 F1: 29.06% dev loss: 0.0123 F1: 27.25% 0.03 min
> Epoch: 95 Step: 7200, train loss: 0.0121 F1: 30.15% dev loss: 0.0122 F1: 27.68% 0.03 min
> Epoch: 98 Step: 7400, train loss: 0.0120 F1: 29.45% dev loss: 0.0121 F1: 29.31% 0.03 min
> Epoch: 101 Step: 7600, train loss: 0.0116 F1: 30.18% dev loss: 0.0120 F1: 28.24% 0.03 min
> Epoch: 103 Step: 7800, train loss: 0.0116 F1: 31.02% dev loss: 0.0119 F1: 29.30% 0.03 min
> Epoch: 106 Step: 8000, train loss: 0.0115 F1: 30.91% dev loss: 0.0119 F1: 29.53% 0.03 min
> Epoch: 109 Step: 8200, train loss: 0.0112 F1: 30.03% dev loss: 0.0118 F1: 29.80% 0.03 min
> Epoch: 111 Step: 8400, train loss: 0.0112 F1: 30.88% dev loss: 0.0117 F1: 30.33% 0.03 min
> Epoch: 114 Step: 8600, train loss: 0.0111 F1: 31.13% dev loss: 0.0117 F1: 30.20% 0.03 min
> Epoch: 117 Step: 8800, train loss: 0.0108 F1: 34.99% dev loss: 0.0116 F1: 30.89% 0.03 min
> Epoch: 119 Step: 9000, train loss: 0.0108 F1: 32.33% dev loss: 0.0115 F1: 30.94% 0.03 min
> Epoch: 122 Step: 9200, train loss: 0.0107 F1: 35.63% dev loss: 0.0115 F1: 30.11% 0.03 min
> Epoch: 125 Step: 9400, train loss: 0.0104 F1: 37.17% dev loss: 0.0115 F1: 31.41% 0.03 min
> Epoch: 127 Step: 9600, train loss: 0.0105 F1: 34.16% dev loss: 0.0114 F1: 31.13% 0.03 min
> Epoch: 130 Step: 9800, train loss: 0.0104 F1: 40.08% dev loss: 0.0114 F1: 31.32% 0.03 min
> Epoch: 133 Step: 10000, train loss: 0.0101 F1: 38.79% dev loss: 0.0113 F1: 31.39% 0.03 min
> Epoch: 135 Step: 10200, train loss: 0.0101 F1: 40.37% dev loss: 0.0113 F1: 30.54% 0.03 min
> Epoch: 138 Step: 10400, train loss: 0.0100 F1: 41.47% dev loss: 0.0112 F1: 31.66% 0.03 min
> Epoch: 141 Step: 10600, train loss: 0.0098 F1: 42.17% dev loss: 0.0112 F1: 31.32% 0.03 min
> Epoch: 143 Step: 10800, train loss: 0.0098 F1: 41.38% dev loss: 0.0112 F1: 31.42% 0.03 min
> Epoch: 146 Step: 11000, train loss: 0.0098 F1: 40.73% dev loss: 0.0111 F1: 32.08% 0.03 min
> Epoch: 149 Step: 11200, train loss: 0.0095 F1: 39.75% dev loss: 0.0111 F1: 31.52% 0.03 min
> Epoch: 151 Step: 11400, train loss: 0.0096 F1: 42.27% dev loss: 0.0110 F1: 31.74% 0.03 min
> Epoch: 154 Step: 11600, train loss: 0.0095 F1: 42.11% dev loss: 0.0110 F1: 33.28% 0.03 min
> Epoch: 157 Step: 11800, train loss: 0.0093 F1: 44.66% dev loss: 0.0110 F1: 32.24% 0.03 min
> Epoch: 159 Step: 12000, train loss: 0.0093 F1: 43.23% dev loss: 0.0110 F1: 33.18% 0.03 min
> Epoch: 162 Step: 12200, train loss: 0.0093 F1: 42.94% dev loss: 0.0109 F1: 33.36% 0.03 min
> Epoch: 165 Step: 12400, train loss: 0.0090 F1: 41.50% dev loss: 0.0109 F1: 32.55% 0.03 min
> Epoch: 167 Step: 12600, train loss: 0.0091 F1: 43.93% dev loss: 0.0109 F1: 33.23% 0.03 min
> Epoch: 170 Step: 12800, train loss: 0.0090 F1: 45.06% dev loss: 0.0108 F1: 33.72% 0.03 min
> Epoch: 173 Step: 13000, train loss: 0.0088 F1: 44.40% dev loss: 0.0109 F1: 33.48% 0.03 min
> Epoch: 175 Step: 13200, train loss: 0.0088 F1: 45.48% dev loss: 0.0108 F1: 33.56% 0.03 min
> Epoch: 178 Step: 13400, train loss: 0.0088 F1: 44.98% dev loss: 0.0108 F1: 33.48% 0.03 min
> Epoch: 181 Step: 13600, train loss: 0.0086 F1: 46.68% dev loss: 0.0108 F1: 33.64% 0.03 min
> Epoch: 183 Step: 13800, train loss: 0.0087 F1: 46.42% dev loss: 0.0108 F1: 33.97% 0.03 min
> Epoch: 186 Step: 14000, train loss: 0.0086 F1: 45.76% dev loss: 0.0107 F1: 36.55% 0.03 min
> Epoch: 189 Step: 14200, train loss: 0.0084 F1: 47.64% dev loss: 0.0107 F1: 33.71% 0.03 min
> Epoch: 191 Step: 14400, train loss: 0.0084 F1: 45.90% dev loss: 0.0107 F1: 34.05% 0.03 min
> Epoch: 194 Step: 14600, train loss: 0.0084 F1: 47.14% dev loss: 0.0107 F1: 36.33% 0.03 min
> Epoch: 197 Step: 14800, train loss: 0.0082 F1: 49.60% dev loss: 0.0106 F1: 36.73% 0.03 min
> Epoch: 199 Step: 15000, train loss: 0.0082 F1: 47.38% dev loss: 0.0106 F1: 37.02% 0.03 min
> Epoch: 202 Step: 15200, train loss: 0.0082 F1: 48.62% dev loss: 0.0106 F1: 36.67% 0.03 min
> Epoch: 205 Step: 15400, train loss: 0.0080 F1: 48.52% dev loss: 0.0107 F1: 37.19% 0.03 min
> Epoch: 207 Step: 15600, train loss: 0.0080 F1: 47.82% dev loss: 0.0106 F1: 37.01% 0.03 min
> Epoch: 210 Step: 15800, train loss: 0.0081 F1: 49.74% dev loss: 0.0106 F1: 36.94% 0.03 min
> Epoch: 213 Step: 16000, train loss: 0.0079 F1: 50.63% dev loss: 0.0106 F1: 37.08% 0.03 min
> Epoch: 215 Step: 16200, train loss: 0.0079 F1: 47.46% dev loss: 0.0105 F1: 37.53% 0.03 min
> Epoch: 218 Step: 16400, train loss: 0.0079 F1: 48.89% dev loss: 0.0105 F1: 36.90% 0.03 min
> Epoch: 221 Step: 16600, train loss: 0.0077 F1: 51.45% dev loss: 0.0105 F1: 37.42% 0.03 min
> Epoch: 223 Step: 16800, train loss: 0.0078 F1: 50.22% dev loss: 0.0105 F1: 38.58% 0.03 min
> Epoch: 226 Step: 17000, train loss: 0.0078 F1: 50.72% dev loss: 0.0105 F1: 38.09% 0.03 min
> Epoch: 229 Step: 17200, train loss: 0.0076 F1: 51.23% dev loss: 0.0105 F1: 37.45% 0.03 min
> Epoch: 231 Step: 17400, train loss: 0.0076 F1: 50.56% dev loss: 0.0105 F1: 38.94% 0.03 min
> Epoch: 234 Step: 17600, train loss: 0.0076 F1: 52.00% dev loss: 0.0105 F1: 38.00% 0.03 min
> Epoch: 237 Step: 17800, train loss: 0.0074 F1: 50.11% dev loss: 0.0105 F1: 37.17% 0.03 min
> Epoch: 239 Step: 18000, train loss: 0.0075 F1: 51.39% dev loss: 0.0104 F1: 38.87% 0.03 min
> Epoch: 242 Step: 18200, train loss: 0.0075 F1: 51.81% dev loss: 0.0104 F1: 38.02% 0.03 min
> Epoch: 245 Step: 18400, train loss: 0.0073 F1: 52.52% dev loss: 0.0104 F1: 38.68% 0.03 min
> Epoch: 247 Step: 18600, train loss: 0.0073 F1: 54.42% dev loss: 0.0104 F1: 38.80% 0.03 min
> Epoch: 250 Step: 18800, train loss: 0.0073 F1: 54.75% dev loss: 0.0104 F1: 38.72% 0.03 min
> Epoch: 253 Step: 19000, train loss: 0.0071 F1: 55.31% dev loss: 0.0104 F1: 38.70% 0.03 min
> Epoch: 255 Step: 19200, train loss: 0.0072 F1: 54.38% dev loss: 0.0104 F1: 38.95% 0.03 min
> Epoch: 258 Step: 19400, train loss: 0.0072 F1: 55.09% dev loss: 0.0104 F1: 38.43% 0.03 min
> Epoch: 261 Step: 19600, train loss: 0.0071 F1: 52.93% dev loss: 0.0104 F1: 38.60% 0.03 min
> Epoch: 263 Step: 19800, train loss: 0.0070 F1: 52.15% dev loss: 0.0103 F1: 38.73% 0.03 min
> Epoch: 266 Step: 20000, train loss: 0.0071 F1: 52.34% dev loss: 0.0104 F1: 38.73% 0.03 min
> Epoch: 269 Step: 20200, train loss: 0.0069 F1: 53.37% dev loss: 0.0104 F1: 38.21% 0.03 min
> Epoch: 271 Step: 20400, train loss: 0.0070 F1: 55.51% dev loss: 0.0103 F1: 38.34% 0.03 min
> Epoch: 274 Step: 20600, train loss: 0.0069 F1: 56.57% dev loss: 0.0103 F1: 38.59% 0.03 min
> Epoch: 277 Step: 20800, train loss: 0.0068 F1: 54.27% dev loss: 0.0104 F1: 38.41% 0.03 min
> Epoch: 279 Step: 21000, train loss: 0.0069 F1: 55.10% dev loss: 0.0103 F1: 38.36% 0.03 min
> Epoch: 282 Step: 21200, train loss: 0.0068 F1: 56.42% dev loss: 0.0104 F1: 38.59% 0.03 min
> Epoch: 285 Step: 21400, train loss: 0.0067 F1: 56.38% dev loss: 0.0104 F1: 38.26% 0.03 min
> Epoch: 287 Step: 21600, train loss: 0.0068 F1: 54.67% dev loss: 0.0103 F1: 38.49% 0.03 min
> Epoch: 290 Step: 21800, train loss: 0.0067 F1: 59.39% dev loss: 0.0103 F1: 37.32% 0.03 min
> Epoch: 293 Step: 22000, train loss: 0.0066 F1: 55.28% dev loss: 0.0104 F1: 38.51% 0.03 min
> Epoch: 295 Step: 22200, train loss: 0.0066 F1: 58.91% dev loss: 0.0103 F1: 37.69% 0.03 min
> Epoch: 298 Step: 22400, train loss: 0.0067 F1: 57.97% dev loss: 0.0104 F1: 37.23% 0.03 min
> Epoch: 301 Step: 22600, train loss: 0.0065 F1: 55.88% dev loss: 0.0104 F1: 38.35% 0.03 min
> Epoch: 303 Step: 22800, train loss: 0.0066 F1: 57.15% dev loss: 0.0104 F1: 36.86% 0.03 min
> Epoch: 306 Step: 23000, train loss: 0.0065 F1: 58.67% dev loss: 0.0104 F1: 37.54% 0.03 min
> Epoch: 309 Step: 23200, train loss: 0.0064 F1: 59.65% dev loss: 0.0103 F1: 37.40% 0.03 min
> Epoch: 311 Step: 23400, train loss: 0.0064 F1: 59.47% dev loss: 0.0103 F1: 36.79% 0.03 min
> Epoch: 314 Step: 23600, train loss: 0.0064 F1: 60.36% dev loss: 0.0103 F1: 37.52% 0.03 min
> Epoch: 317 Step: 23800, train loss: 0.0063 F1: 57.05% dev loss: 0.0104 F1: 36.85% 0.03 min
> Epoch: 319 Step: 24000, train loss: 0.0064 F1: 59.06% dev loss: 0.0104 F1: 36.96% 0.03 min
> Epoch: 322 Step: 24200, train loss: 0.0063 F1: 59.62% dev loss: 0.0103 F1: 39.99% 0.03 min
> Epoch: 325 Step: 24400, train loss: 0.0063 F1: 61.69% dev loss: 0.0104 F1: 37.37% 0.03 min
> Epoch: 327 Step: 24600, train loss: 0.0062 F1: 58.89% dev loss: 0.0103 F1: 36.62% 0.03 min
> Epoch: 330 Step: 24800, train loss: 0.0063 F1: 59.58% dev loss: 0.0103 F1: 37.34% 0.03 min
> Epoch: 333 Step: 25000, train loss: 0.0062 F1: 61.91% dev loss: 0.0104 F1: 36.55% 0.03 min
> Epoch: 335 Step: 25200, train loss: 0.0062 F1: 60.41% dev loss: 0.0104 F1: 36.53% 0.03 min
> Epoch: 338 Step: 25400, train loss: 0.0062 F1: 58.35% dev loss: 0.0103 F1: 39.99% 0.03 min
> Epoch: 341 Step: 25600, train loss: 0.0061 F1: 59.33% dev loss: 0.0104 F1: 36.62% 0.03 min
> Epoch: 343 Step: 25800, train loss: 0.0061 F1: 60.43% dev loss: 0.0104 F1: 36.26% 0.03 min
> Epoch: 346 Step: 26000, train loss: 0.0061 F1: 60.79% dev loss: 0.0103 F1: 40.23% 0.03 min
> Epoch: 349 Step: 26200, train loss: 0.0060 F1: 60.07% dev loss: 0.0104 F1: 36.76% 0.03 min
> Epoch: 351 Step: 26400, train loss: 0.0060 F1: 59.80% dev loss: 0.0103 F1: 36.57% 0.03 min
> Epoch: 354 Step: 26600, train loss: 0.0060 F1: 62.53% dev loss: 0.0103 F1: 39.40% 0.03 min
> Epoch: 357 Step: 26800, train loss: 0.0059 F1: 57.66% dev loss: 0.0104 F1: 36.51% 0.03 min
> Epoch: 359 Step: 27000, train loss: 0.0060 F1: 61.02% dev loss: 0.0103 F1: 36.56% 0.03 min
> Epoch: 362 Step: 27200, train loss: 0.0059 F1: 62.87% dev loss: 0.0103 F1: 36.62% 0.03 min
> Epoch: 365 Step: 27400, train loss: 0.0058 F1: 59.79% dev loss: 0.0103 F1: 37.30% 0.03 min
> Epoch: 367 Step: 27600, train loss: 0.0058 F1: 62.79% dev loss: 0.0103 F1: 36.59% 0.03 min
> Epoch: 370 Step: 27800, train loss: 0.0058 F1: 61.12% dev loss: 0.0103 F1: 39.27% 0.03 min
> Epoch: 373 Step: 28000, train loss: 0.0058 F1: 60.90% dev loss: 0.0104 F1: 36.55% 0.03 min
> Epoch: 375 Step: 28200, train loss: 0.0059 F1: 61.83% dev loss: 0.0104 F1: 36.17% 0.03 min
> Epoch: 378 Step: 28400, train loss: 0.0058 F1: 61.74% dev loss: 0.0104 F1: 39.04% 0.03 min
> Epoch: 381 Step: 28600, train loss: 0.0057 F1: 61.43% dev loss: 0.0105 F1: 36.40% 0.03 min
> Epoch: 383 Step: 28800, train loss: 0.0058 F1: 61.81% dev loss: 0.0104 F1: 36.18% 0.03 min
> Epoch: 386 Step: 29000, train loss: 0.0058 F1: 62.84% dev loss: 0.0104 F1: 39.25% 0.03 min
> Epoch: 389 Step: 29200, train loss: 0.0056 F1: 62.99% dev loss: 0.0104 F1: 36.66% 0.03 min
> Epoch: 391 Step: 29400, train loss: 0.0056 F1: 62.15% dev loss: 0.0104 F1: 36.28% 0.03 min
> Epoch: 394 Step: 29600, train loss: 0.0057 F1: 64.30% dev loss: 0.0104 F1: 38.62% 0.03 min
> Epoch: 397 Step: 29800, train loss: 0.0056 F1: 62.81% dev loss: 0.0103 F1: 36.40% 0.03 min
> Epoch: 399 Step: 30000, train loss: 0.0056 F1: 63.60% dev loss: 0.0104 F1: 36.18% 0.03 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 16:18:46.008843>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 0.0001
>>> epochs: 400
>>> step: 200
>>> model_name: glove
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 0.1
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/glove-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 50000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 0.0001, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
> Epoch: 2 Step: 200, train loss: 0.1338 F1: 10.01% dev loss: 0.0993 F1: 9.49% 0.04 min
> Epoch: 5 Step: 400, train loss: 0.0497 F1: 9.43% dev loss: 0.0252 F1: 9.49% 0.03 min
> Epoch: 7 Step: 600, train loss: 0.0247 F1: 9.45% dev loss: 0.0210 F1: 9.49% 0.03 min
> Epoch: 10 Step: 800, train loss: 0.0223 F1: 10.41% dev loss: 0.0195 F1: 9.50% 0.03 min
> Epoch: 13 Step: 1000, train loss: 0.0205 F1: 11.96% dev loss: 0.0186 F1: 11.23% 0.03 min
> Epoch: 15 Step: 1200, train loss: 0.0198 F1: 12.65% dev loss: 0.0179 F1: 12.81% 0.03 min
> Epoch: 18 Step: 1400, train loss: 0.0193 F1: 13.30% dev loss: 0.0174 F1: 13.14% 0.03 min
> Epoch: 21 Step: 1600, train loss: 0.0182 F1: 13.84% dev loss: 0.0168 F1: 13.32% 0.03 min
> Epoch: 23 Step: 1800, train loss: 0.0178 F1: 13.57% dev loss: 0.0163 F1: 13.30% 0.03 min
> Epoch: 26 Step: 2000, train loss: 0.0174 F1: 19.56% dev loss: 0.0158 F1: 13.92% 0.03 min
> Epoch: 29 Step: 2200, train loss: 0.0163 F1: 15.57% dev loss: 0.0153 F1: 14.50% 0.03 min
> Epoch: 31 Step: 2400, train loss: 0.0160 F1: 22.50% dev loss: 0.0148 F1: 17.25% 0.03 min
> Epoch: 34 Step: 2600, train loss: 0.0156 F1: 22.78% dev loss: 0.0144 F1: 19.66% 0.03 min
> Epoch: 37 Step: 2800, train loss: 0.0147 F1: 24.08% dev loss: 0.0140 F1: 20.59% 0.03 min
> Epoch: 39 Step: 3000, train loss: 0.0144 F1: 26.19% dev loss: 0.0137 F1: 22.39% 0.03 min
> Epoch: 42 Step: 3200, train loss: 0.0141 F1: 26.60% dev loss: 0.0133 F1: 25.47% 0.03 min
> Epoch: 45 Step: 3400, train loss: 0.0133 F1: 25.79% dev loss: 0.0130 F1: 25.02% 0.03 min
> Epoch: 47 Step: 3600, train loss: 0.0131 F1: 27.85% dev loss: 0.0127 F1: 25.69% 0.03 min
> Epoch: 50 Step: 3800, train loss: 0.0129 F1: 28.39% dev loss: 0.0125 F1: 26.99% 0.03 min
> Epoch: 53 Step: 4000, train loss: 0.0122 F1: 29.05% dev loss: 0.0123 F1: 27.17% 0.03 min
> Epoch: 55 Step: 4200, train loss: 0.0120 F1: 30.25% dev loss: 0.0121 F1: 26.96% 0.03 min
> Epoch: 58 Step: 4400, train loss: 0.0118 F1: 30.64% dev loss: 0.0120 F1: 29.20% 0.03 min
> Epoch: 61 Step: 4600, train loss: 0.0113 F1: 30.24% dev loss: 0.0117 F1: 29.86% 0.03 min
> Epoch: 63 Step: 4800, train loss: 0.0111 F1: 31.88% dev loss: 0.0116 F1: 29.33% 0.03 min
> Epoch: 66 Step: 5000, train loss: 0.0109 F1: 31.88% dev loss: 0.0115 F1: 30.42% 0.03 min
> Epoch: 69 Step: 5200, train loss: 0.0105 F1: 32.81% dev loss: 0.0114 F1: 30.66% 0.03 min
> Epoch: 71 Step: 5400, train loss: 0.0104 F1: 33.89% dev loss: 0.0113 F1: 30.59% 0.03 min
> Epoch: 74 Step: 5600, train loss: 0.0103 F1: 34.93% dev loss: 0.0112 F1: 31.47% 0.03 min
> Epoch: 77 Step: 5800, train loss: 0.0098 F1: 39.52% dev loss: 0.0110 F1: 31.88% 0.03 min
> Epoch: 79 Step: 6000, train loss: 0.0097 F1: 41.28% dev loss: 0.0110 F1: 31.11% 0.03 min
> Epoch: 82 Step: 6200, train loss: 0.0096 F1: 41.34% dev loss: 0.0110 F1: 32.35% 0.03 min
> Epoch: 85 Step: 6400, train loss: 0.0092 F1: 42.65% dev loss: 0.0108 F1: 33.58% 0.03 min
> Epoch: 87 Step: 6600, train loss: 0.0092 F1: 43.26% dev loss: 0.0108 F1: 32.51% 0.03 min
> Epoch: 90 Step: 6800, train loss: 0.0090 F1: 45.87% dev loss: 0.0108 F1: 33.23% 0.03 min
> Epoch: 93 Step: 7000, train loss: 0.0087 F1: 46.31% dev loss: 0.0107 F1: 35.55% 0.03 min
> Epoch: 95 Step: 7200, train loss: 0.0086 F1: 44.77% dev loss: 0.0107 F1: 36.41% 0.03 min
> Epoch: 98 Step: 7400, train loss: 0.0085 F1: 46.11% dev loss: 0.0106 F1: 38.32% 0.03 min
> Epoch: 101 Step: 7600, train loss: 0.0082 F1: 48.49% dev loss: 0.0105 F1: 38.07% 0.03 min
> Epoch: 103 Step: 7800, train loss: 0.0082 F1: 47.94% dev loss: 0.0105 F1: 38.47% 0.03 min
> Epoch: 106 Step: 8000, train loss: 0.0081 F1: 48.04% dev loss: 0.0104 F1: 38.63% 0.03 min
> Epoch: 109 Step: 8200, train loss: 0.0079 F1: 46.98% dev loss: 0.0105 F1: 38.11% 0.03 min
> Epoch: 111 Step: 8400, train loss: 0.0078 F1: 48.45% dev loss: 0.0104 F1: 38.14% 0.03 min
> Epoch: 114 Step: 8600, train loss: 0.0077 F1: 50.55% dev loss: 0.0104 F1: 38.08% 0.03 min
> Epoch: 117 Step: 8800, train loss: 0.0075 F1: 53.73% dev loss: 0.0104 F1: 38.39% 0.03 min
> Epoch: 119 Step: 9000, train loss: 0.0074 F1: 53.50% dev loss: 0.0104 F1: 38.80% 0.03 min
> Epoch: 122 Step: 9200, train loss: 0.0073 F1: 53.74% dev loss: 0.0103 F1: 38.03% 0.03 min
> Epoch: 125 Step: 9400, train loss: 0.0071 F1: 53.41% dev loss: 0.0103 F1: 38.30% 0.03 min
> Epoch: 127 Step: 9600, train loss: 0.0071 F1: 53.50% dev loss: 0.0104 F1: 38.57% 0.03 min
> Epoch: 130 Step: 9800, train loss: 0.0070 F1: 53.89% dev loss: 0.0103 F1: 39.06% 0.03 min
> Epoch: 133 Step: 10000, train loss: 0.0068 F1: 56.43% dev loss: 0.0102 F1: 38.51% 0.03 min
> Epoch: 135 Step: 10200, train loss: 0.0068 F1: 54.46% dev loss: 0.0103 F1: 38.45% 0.03 min
> Epoch: 138 Step: 10400, train loss: 0.0067 F1: 55.38% dev loss: 0.0102 F1: 39.61% 0.03 min
> Epoch: 141 Step: 10600, train loss: 0.0066 F1: 57.69% dev loss: 0.0101 F1: 39.55% 0.03 min
> Epoch: 143 Step: 10800, train loss: 0.0065 F1: 54.93% dev loss: 0.0102 F1: 39.54% 0.03 min
> Epoch: 146 Step: 11000, train loss: 0.0064 F1: 60.43% dev loss: 0.0101 F1: 39.48% 0.03 min
> Epoch: 149 Step: 11200, train loss: 0.0063 F1: 59.00% dev loss: 0.0101 F1: 39.19% 0.03 min
> Epoch: 151 Step: 11400, train loss: 0.0063 F1: 58.16% dev loss: 0.0100 F1: 39.50% 0.03 min
> Epoch: 154 Step: 11600, train loss: 0.0061 F1: 59.77% dev loss: 0.0100 F1: 38.31% 0.03 min
> Epoch: 157 Step: 11800, train loss: 0.0061 F1: 57.57% dev loss: 0.0103 F1: 37.23% 0.03 min
> Epoch: 159 Step: 12000, train loss: 0.0060 F1: 60.91% dev loss: 0.0103 F1: 38.08% 0.03 min
> Epoch: 162 Step: 12200, train loss: 0.0059 F1: 62.03% dev loss: 0.0101 F1: 39.02% 0.03 min
> Epoch: 165 Step: 12400, train loss: 0.0058 F1: 60.92% dev loss: 0.0102 F1: 38.63% 0.03 min
> Epoch: 167 Step: 12600, train loss: 0.0057 F1: 62.28% dev loss: 0.0103 F1: 37.53% 0.03 min
> Epoch: 170 Step: 12800, train loss: 0.0057 F1: 63.12% dev loss: 0.0102 F1: 38.51% 0.03 min
> Epoch: 173 Step: 13000, train loss: 0.0056 F1: 62.72% dev loss: 0.0102 F1: 38.60% 0.03 min
> Epoch: 175 Step: 13200, train loss: 0.0055 F1: 65.15% dev loss: 0.0103 F1: 37.03% 0.03 min
> Epoch: 178 Step: 13400, train loss: 0.0055 F1: 66.20% dev loss: 0.0102 F1: 38.07% 0.03 min
> Epoch: 181 Step: 13600, train loss: 0.0054 F1: 60.87% dev loss: 0.0102 F1: 36.90% 0.03 min
> Epoch: 183 Step: 13800, train loss: 0.0054 F1: 66.06% dev loss: 0.0103 F1: 37.42% 0.03 min
> Epoch: 186 Step: 14000, train loss: 0.0052 F1: 66.98% dev loss: 0.0102 F1: 38.77% 0.03 min
> Epoch: 189 Step: 14200, train loss: 0.0052 F1: 67.14% dev loss: 0.0102 F1: 37.16% 0.03 min
> Epoch: 191 Step: 14400, train loss: 0.0051 F1: 66.55% dev loss: 0.0104 F1: 37.17% 0.03 min
> Epoch: 194 Step: 14600, train loss: 0.0051 F1: 69.67% dev loss: 0.0102 F1: 36.90% 0.03 min
> Epoch: 197 Step: 14800, train loss: 0.0050 F1: 65.96% dev loss: 0.0102 F1: 37.12% 0.03 min
> Epoch: 199 Step: 15000, train loss: 0.0050 F1: 68.68% dev loss: 0.0103 F1: 37.25% 0.03 min
> Epoch: 202 Step: 15200, train loss: 0.0049 F1: 70.41% dev loss: 0.0102 F1: 36.83% 0.03 min
> Epoch: 205 Step: 15400, train loss: 0.0048 F1: 67.13% dev loss: 0.0104 F1: 37.07% 0.03 min
> Epoch: 207 Step: 15600, train loss: 0.0048 F1: 69.79% dev loss: 0.0103 F1: 37.48% 0.03 min
> Epoch: 210 Step: 15800, train loss: 0.0048 F1: 70.27% dev loss: 0.0103 F1: 37.12% 0.03 min
> Epoch: 213 Step: 16000, train loss: 0.0047 F1: 68.91% dev loss: 0.0104 F1: 37.03% 0.03 min
> Epoch: 215 Step: 16200, train loss: 0.0046 F1: 69.19% dev loss: 0.0104 F1: 37.24% 0.03 min
> Epoch: 218 Step: 16400, train loss: 0.0046 F1: 70.00% dev loss: 0.0103 F1: 37.22% 0.03 min
> Epoch: 221 Step: 16600, train loss: 0.0045 F1: 68.70% dev loss: 0.0102 F1: 37.35% 0.03 min
> Epoch: 223 Step: 16800, train loss: 0.0045 F1: 71.22% dev loss: 0.0104 F1: 37.24% 0.03 min
> Epoch: 226 Step: 17000, train loss: 0.0045 F1: 71.93% dev loss: 0.0102 F1: 36.98% 0.03 min
> Epoch: 229 Step: 17200, train loss: 0.0044 F1: 70.29% dev loss: 0.0103 F1: 37.24% 0.03 min
> Epoch: 231 Step: 17400, train loss: 0.0044 F1: 71.32% dev loss: 0.0105 F1: 36.86% 0.03 min
> Epoch: 234 Step: 17600, train loss: 0.0043 F1: 72.93% dev loss: 0.0103 F1: 36.67% 0.03 min
> Epoch: 237 Step: 17800, train loss: 0.0042 F1: 68.44% dev loss: 0.0104 F1: 37.16% 0.03 min
> Epoch: 239 Step: 18000, train loss: 0.0042 F1: 72.27% dev loss: 0.0105 F1: 37.00% 0.03 min
> Epoch: 242 Step: 18200, train loss: 0.0042 F1: 72.58% dev loss: 0.0103 F1: 36.98% 0.03 min
> Epoch: 245 Step: 18400, train loss: 0.0041 F1: 70.83% dev loss: 0.0105 F1: 36.95% 0.03 min
> Epoch: 247 Step: 18600, train loss: 0.0041 F1: 72.73% dev loss: 0.0106 F1: 37.24% 0.03 min
> Epoch: 250 Step: 18800, train loss: 0.0040 F1: 74.29% dev loss: 0.0105 F1: 37.02% 0.03 min
> Epoch: 253 Step: 19000, train loss: 0.0039 F1: 73.25% dev loss: 0.0106 F1: 37.08% 0.03 min
> Epoch: 255 Step: 19200, train loss: 0.0040 F1: 73.99% dev loss: 0.0106 F1: 37.29% 0.03 min
> Epoch: 258 Step: 19400, train loss: 0.0039 F1: 74.71% dev loss: 0.0106 F1: 36.72% 0.03 min
> Epoch: 261 Step: 19600, train loss: 0.0039 F1: 74.87% dev loss: 0.0106 F1: 37.27% 0.03 min
> Epoch: 263 Step: 19800, train loss: 0.0038 F1: 75.08% dev loss: 0.0105 F1: 37.98% 0.03 min
> Epoch: 266 Step: 20000, train loss: 0.0038 F1: 74.87% dev loss: 0.0105 F1: 36.59% 0.03 min
> Epoch: 269 Step: 20200, train loss: 0.0038 F1: 73.13% dev loss: 0.0106 F1: 37.18% 0.03 min
> Epoch: 271 Step: 20400, train loss: 0.0037 F1: 75.08% dev loss: 0.0108 F1: 37.50% 0.03 min
> Epoch: 274 Step: 20600, train loss: 0.0037 F1: 75.92% dev loss: 0.0105 F1: 37.29% 0.03 min
> Epoch: 277 Step: 20800, train loss: 0.0037 F1: 72.95% dev loss: 0.0107 F1: 37.30% 0.03 min
> Epoch: 279 Step: 21000, train loss: 0.0036 F1: 76.39% dev loss: 0.0108 F1: 37.22% 0.03 min
> Epoch: 282 Step: 21200, train loss: 0.0036 F1: 76.15% dev loss: 0.0106 F1: 37.65% 0.03 min
> Epoch: 285 Step: 21400, train loss: 0.0035 F1: 76.38% dev loss: 0.0107 F1: 37.05% 0.03 min
> Epoch: 287 Step: 21600, train loss: 0.0036 F1: 76.85% dev loss: 0.0108 F1: 37.62% 0.03 min
> Epoch: 290 Step: 21800, train loss: 0.0035 F1: 78.32% dev loss: 0.0107 F1: 37.37% 0.03 min
> Epoch: 293 Step: 22000, train loss: 0.0034 F1: 79.02% dev loss: 0.0109 F1: 36.69% 0.03 min
> Epoch: 295 Step: 22200, train loss: 0.0034 F1: 77.76% dev loss: 0.0108 F1: 37.47% 0.03 min
> Epoch: 298 Step: 22400, train loss: 0.0035 F1: 77.78% dev loss: 0.0108 F1: 37.04% 0.03 min
> Epoch: 301 Step: 22600, train loss: 0.0033 F1: 76.10% dev loss: 0.0108 F1: 37.33% 0.03 min
> Epoch: 303 Step: 22800, train loss: 0.0034 F1: 78.71% dev loss: 0.0109 F1: 37.60% 0.03 min
> Epoch: 306 Step: 23000, train loss: 0.0033 F1: 80.45% dev loss: 0.0108 F1: 36.96% 0.03 min
> Epoch: 309 Step: 23200, train loss: 0.0032 F1: 78.94% dev loss: 0.0108 F1: 37.44% 0.03 min
> Epoch: 311 Step: 23400, train loss: 0.0033 F1: 80.38% dev loss: 0.0110 F1: 37.80% 0.03 min
> Epoch: 314 Step: 23600, train loss: 0.0032 F1: 79.06% dev loss: 0.0108 F1: 37.13% 0.03 min
> Epoch: 317 Step: 23800, train loss: 0.0032 F1: 78.84% dev loss: 0.0109 F1: 37.33% 0.03 min
> Epoch: 319 Step: 24000, train loss: 0.0032 F1: 80.23% dev loss: 0.0111 F1: 37.72% 0.03 min
> Epoch: 322 Step: 24200, train loss: 0.0031 F1: 80.65% dev loss: 0.0108 F1: 36.75% 0.03 min
> Epoch: 325 Step: 24400, train loss: 0.0031 F1: 79.38% dev loss: 0.0110 F1: 37.71% 0.03 min
> Epoch: 327 Step: 24600, train loss: 0.0031 F1: 79.98% dev loss: 0.0111 F1: 37.56% 0.03 min
> Epoch: 330 Step: 24800, train loss: 0.0031 F1: 79.78% dev loss: 0.0109 F1: 36.66% 0.03 min
> Epoch: 333 Step: 25000, train loss: 0.0031 F1: 80.15% dev loss: 0.0111 F1: 37.31% 0.03 min
> Epoch: 335 Step: 25200, train loss: 0.0030 F1: 81.74% dev loss: 0.0111 F1: 37.81% 0.03 min
> Epoch: 338 Step: 25400, train loss: 0.0030 F1: 80.27% dev loss: 0.0109 F1: 36.16% 0.03 min
> Epoch: 341 Step: 25600, train loss: 0.0030 F1: 77.74% dev loss: 0.0111 F1: 37.35% 0.03 min
> Epoch: 343 Step: 25800, train loss: 0.0029 F1: 81.72% dev loss: 0.0112 F1: 37.99% 0.03 min
> Epoch: 346 Step: 26000, train loss: 0.0029 F1: 82.27% dev loss: 0.0109 F1: 36.58% 0.03 min
> Epoch: 349 Step: 26200, train loss: 0.0029 F1: 81.76% dev loss: 0.0112 F1: 37.85% 0.03 min
> Epoch: 351 Step: 26400, train loss: 0.0029 F1: 81.86% dev loss: 0.0112 F1: 37.87% 0.03 min
> Epoch: 354 Step: 26600, train loss: 0.0028 F1: 82.16% dev loss: 0.0110 F1: 37.00% 0.03 min
> Epoch: 357 Step: 26800, train loss: 0.0028 F1: 81.38% dev loss: 0.0112 F1: 37.55% 0.03 min
> Epoch: 359 Step: 27000, train loss: 0.0028 F1: 81.98% dev loss: 0.0112 F1: 37.50% 0.03 min
> Epoch: 362 Step: 27200, train loss: 0.0028 F1: 81.71% dev loss: 0.0110 F1: 36.84% 0.03 min
> Epoch: 365 Step: 27400, train loss: 0.0027 F1: 81.24% dev loss: 0.0112 F1: 37.39% 0.03 min
> Epoch: 367 Step: 27600, train loss: 0.0027 F1: 82.76% dev loss: 0.0114 F1: 37.10% 0.03 min
> Epoch: 370 Step: 27800, train loss: 0.0027 F1: 83.48% dev loss: 0.0111 F1: 36.74% 0.03 min
> Epoch: 373 Step: 28000, train loss: 0.0027 F1: 84.31% dev loss: 0.0115 F1: 37.09% 0.03 min
> Epoch: 375 Step: 28200, train loss: 0.0028 F1: 81.75% dev loss: 0.0113 F1: 37.29% 0.03 min
> Epoch: 378 Step: 28400, train loss: 0.0027 F1: 81.89% dev loss: 0.0113 F1: 36.68% 0.03 min
> Epoch: 381 Step: 28600, train loss: 0.0026 F1: 82.12% dev loss: 0.0115 F1: 36.83% 0.03 min
> Epoch: 383 Step: 28800, train loss: 0.0027 F1: 82.76% dev loss: 0.0115 F1: 36.94% 0.03 min
> Epoch: 386 Step: 29000, train loss: 0.0026 F1: 81.80% dev loss: 0.0111 F1: 36.73% 0.03 min
> Epoch: 389 Step: 29200, train loss: 0.0026 F1: 84.88% dev loss: 0.0114 F1: 36.95% 0.03 min
> Epoch: 391 Step: 29400, train loss: 0.0026 F1: 84.40% dev loss: 0.0114 F1: 37.06% 0.03 min
> Epoch: 394 Step: 29600, train loss: 0.0026 F1: 84.15% dev loss: 0.0112 F1: 36.90% 0.03 min
> Epoch: 397 Step: 29800, train loss: 0.0025 F1: 84.47% dev loss: 0.0114 F1: 37.60% 0.03 min
> Epoch: 399 Step: 30000, train loss: 0.0025 F1: 83.68% dev loss: 0.0113 F1: 37.40% 0.03 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 16:23:21.528356>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 0.0005
>>> epochs: 400
>>> step: 200
>>> model_name: glove
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 0.1
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/glove-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 50000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 0.0005, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
> Epoch: 2 Step: 200, train loss: 0.0864 F1: 9.43% dev loss: 0.0241 F1: 9.49% 0.04 min
> Epoch: 5 Step: 400, train loss: 0.0223 F1: 12.04% dev loss: 0.0185 F1: 11.55% 0.03 min
> Epoch: 7 Step: 600, train loss: 0.0188 F1: 13.58% dev loss: 0.0162 F1: 13.81% 0.03 min
> Epoch: 10 Step: 800, train loss: 0.0165 F1: 21.87% dev loss: 0.0144 F1: 22.47% 0.03 min
> Epoch: 13 Step: 1000, train loss: 0.0140 F1: 25.82% dev loss: 0.0131 F1: 25.18% 0.03 min
> Epoch: 15 Step: 1200, train loss: 0.0126 F1: 29.95% dev loss: 0.0121 F1: 27.63% 0.03 min
> Epoch: 18 Step: 1400, train loss: 0.0114 F1: 31.49% dev loss: 0.0114 F1: 29.27% 0.03 min
> Epoch: 21 Step: 1600, train loss: 0.0103 F1: 37.16% dev loss: 0.0109 F1: 31.54% 0.03 min
> Epoch: 23 Step: 1800, train loss: 0.0096 F1: 38.02% dev loss: 0.0104 F1: 31.81% 0.03 min
> Epoch: 26 Step: 2000, train loss: 0.0089 F1: 45.47% dev loss: 0.0103 F1: 36.42% 0.03 min
> Epoch: 29 Step: 2200, train loss: 0.0081 F1: 47.00% dev loss: 0.0102 F1: 38.79% 0.03 min
> Epoch: 31 Step: 2400, train loss: 0.0077 F1: 48.66% dev loss: 0.0101 F1: 36.27% 0.03 min
> Epoch: 34 Step: 2600, train loss: 0.0071 F1: 52.30% dev loss: 0.0101 F1: 37.65% 0.03 min
> Epoch: 37 Step: 2800, train loss: 0.0066 F1: 56.04% dev loss: 0.0099 F1: 37.11% 0.03 min
> Epoch: 39 Step: 3000, train loss: 0.0062 F1: 58.56% dev loss: 0.0100 F1: 36.24% 0.03 min
> Epoch: 42 Step: 3200, train loss: 0.0058 F1: 61.21% dev loss: 0.0101 F1: 35.86% 0.03 min
> Epoch: 45 Step: 3400, train loss: 0.0055 F1: 63.93% dev loss: 0.0100 F1: 41.52% 0.03 min
> Epoch: 47 Step: 3600, train loss: 0.0051 F1: 67.46% dev loss: 0.0107 F1: 35.80% 0.03 min
> Epoch: 50 Step: 3800, train loss: 0.0048 F1: 69.75% dev loss: 0.0105 F1: 34.95% 0.03 min
> Epoch: 53 Step: 4000, train loss: 0.0044 F1: 72.06% dev loss: 0.0101 F1: 38.67% 0.03 min
> Epoch: 55 Step: 4200, train loss: 0.0043 F1: 72.55% dev loss: 0.0107 F1: 35.31% 0.03 min
> Epoch: 58 Step: 4400, train loss: 0.0040 F1: 75.38% dev loss: 0.0105 F1: 34.76% 0.03 min
> Epoch: 61 Step: 4600, train loss: 0.0037 F1: 74.05% dev loss: 0.0103 F1: 38.32% 0.03 min
> Epoch: 63 Step: 4800, train loss: 0.0035 F1: 77.66% dev loss: 0.0110 F1: 35.41% 0.03 min
> Epoch: 66 Step: 5000, train loss: 0.0033 F1: 80.94% dev loss: 0.0109 F1: 37.56% 0.03 min
> Epoch: 69 Step: 5200, train loss: 0.0031 F1: 80.21% dev loss: 0.0112 F1: 35.64% 0.03 min
> Epoch: 71 Step: 5400, train loss: 0.0029 F1: 81.73% dev loss: 0.0114 F1: 35.38% 0.03 min
> Epoch: 74 Step: 5600, train loss: 0.0027 F1: 83.72% dev loss: 0.0111 F1: 38.35% 0.03 min
> Epoch: 77 Step: 5800, train loss: 0.0025 F1: 85.23% dev loss: 0.0112 F1: 36.21% 0.03 min
> Epoch: 79 Step: 6000, train loss: 0.0024 F1: 85.38% dev loss: 0.0115 F1: 34.79% 0.03 min
> Epoch: 82 Step: 6200, train loss: 0.0023 F1: 86.51% dev loss: 0.0116 F1: 39.60% 0.03 min
> Epoch: 85 Step: 6400, train loss: 0.0022 F1: 86.09% dev loss: 0.0115 F1: 39.47% 0.03 min
> Epoch: 87 Step: 6600, train loss: 0.0021 F1: 85.83% dev loss: 0.0121 F1: 35.99% 0.03 min
> Epoch: 90 Step: 6800, train loss: 0.0019 F1: 88.65% dev loss: 0.0118 F1: 40.64% 0.03 min
> Epoch: 93 Step: 7000, train loss: 0.0018 F1: 89.89% dev loss: 0.0119 F1: 40.24% 0.03 min
> Epoch: 95 Step: 7200, train loss: 0.0017 F1: 89.64% dev loss: 0.0126 F1: 37.55% 0.03 min
> Epoch: 98 Step: 7400, train loss: 0.0016 F1: 90.26% dev loss: 0.0127 F1: 40.35% 0.03 min
> Epoch: 101 Step: 7600, train loss: 0.0016 F1: 90.87% dev loss: 0.0124 F1: 43.62% 0.03 min
> Epoch: 103 Step: 7800, train loss: 0.0015 F1: 91.32% dev loss: 0.0130 F1: 37.92% 0.03 min
> Epoch: 106 Step: 8000, train loss: 0.0014 F1: 91.50% dev loss: 0.0131 F1: 39.62% 0.03 min
> Epoch: 109 Step: 8200, train loss: 0.0013 F1: 92.30% dev loss: 0.0132 F1: 35.14% 0.03 min
> Epoch: 111 Step: 8400, train loss: 0.0013 F1: 92.19% dev loss: 0.0136 F1: 36.35% 0.03 min
> Epoch: 114 Step: 8600, train loss: 0.0012 F1: 93.28% dev loss: 0.0135 F1: 34.89% 0.03 min
> Epoch: 117 Step: 8800, train loss: 0.0011 F1: 93.01% dev loss: 0.0133 F1: 40.79% 0.03 min
> Epoch: 119 Step: 9000, train loss: 0.0011 F1: 93.44% dev loss: 0.0140 F1: 39.38% 0.03 min
> Epoch: 122 Step: 9200, train loss: 0.0010 F1: 93.46% dev loss: 0.0136 F1: 39.73% 0.03 min
> Epoch: 125 Step: 9400, train loss: 0.0010 F1: 94.68% dev loss: 0.0137 F1: 39.21% 0.03 min
> Epoch: 127 Step: 9600, train loss: 0.0010 F1: 93.40% dev loss: 0.0138 F1: 35.47% 0.03 min
> Epoch: 130 Step: 9800, train loss: 0.0009 F1: 94.78% dev loss: 0.0139 F1: 39.32% 0.03 min
> Epoch: 133 Step: 10000, train loss: 0.0008 F1: 94.31% dev loss: 0.0145 F1: 41.62% 0.03 min
> Epoch: 135 Step: 10200, train loss: 0.0008 F1: 95.03% dev loss: 0.0143 F1: 38.20% 0.03 min
> Epoch: 138 Step: 10400, train loss: 0.0008 F1: 94.86% dev loss: 0.0145 F1: 35.08% 0.03 min
> Epoch: 141 Step: 10600, train loss: 0.0008 F1: 95.99% dev loss: 0.0146 F1: 39.72% 0.03 min
> Epoch: 143 Step: 10800, train loss: 0.0007 F1: 95.77% dev loss: 0.0157 F1: 35.07% 0.03 min
> Epoch: 146 Step: 11000, train loss: 0.0007 F1: 95.94% dev loss: 0.0148 F1: 38.53% 0.03 min
> Epoch: 149 Step: 11200, train loss: 0.0006 F1: 96.29% dev loss: 0.0153 F1: 38.34% 0.03 min
> Epoch: 151 Step: 11400, train loss: 0.0007 F1: 95.99% dev loss: 0.0154 F1: 35.32% 0.03 min
> Epoch: 154 Step: 11600, train loss: 0.0006 F1: 95.80% dev loss: 0.0149 F1: 38.05% 0.03 min
> Epoch: 157 Step: 11800, train loss: 0.0005 F1: 97.00% dev loss: 0.0161 F1: 37.06% 0.03 min
> Epoch: 159 Step: 12000, train loss: 0.0005 F1: 96.53% dev loss: 0.0155 F1: 33.32% 0.03 min
> Epoch: 162 Step: 12200, train loss: 0.0005 F1: 96.70% dev loss: 0.0155 F1: 40.27% 0.03 min
> Epoch: 165 Step: 12400, train loss: 0.0005 F1: 96.93% dev loss: 0.0158 F1: 39.60% 0.03 min
> Epoch: 167 Step: 12600, train loss: 0.0005 F1: 97.44% dev loss: 0.0160 F1: 36.83% 0.03 min
> Epoch: 170 Step: 12800, train loss: 0.0005 F1: 96.49% dev loss: 0.0157 F1: 38.81% 0.03 min
> Epoch: 173 Step: 13000, train loss: 0.0004 F1: 97.75% dev loss: 0.0161 F1: 38.59% 0.03 min
> Epoch: 175 Step: 13200, train loss: 0.0005 F1: 96.75% dev loss: 0.0168 F1: 35.85% 0.03 min
> Epoch: 178 Step: 13400, train loss: 0.0005 F1: 94.78% dev loss: 0.0154 F1: 38.14% 0.03 min
> Epoch: 181 Step: 13600, train loss: 0.0005 F1: 96.22% dev loss: 0.0170 F1: 37.74% 0.03 min
> Epoch: 183 Step: 13800, train loss: 0.0004 F1: 97.61% dev loss: 0.0166 F1: 36.64% 0.03 min
> Epoch: 186 Step: 14000, train loss: 0.0004 F1: 97.63% dev loss: 0.0161 F1: 38.61% 0.03 min
> Epoch: 189 Step: 14200, train loss: 0.0003 F1: 97.54% dev loss: 0.0168 F1: 39.64% 0.03 min
> Epoch: 191 Step: 14400, train loss: 0.0004 F1: 97.69% dev loss: 0.0174 F1: 38.25% 0.03 min
> Epoch: 194 Step: 14600, train loss: 0.0003 F1: 97.89% dev loss: 0.0168 F1: 40.38% 0.03 min
> Epoch: 197 Step: 14800, train loss: 0.0003 F1: 97.14% dev loss: 0.0167 F1: 39.47% 0.03 min
> Epoch: 199 Step: 15000, train loss: 0.0003 F1: 98.37% dev loss: 0.0176 F1: 36.13% 0.03 min
> Epoch: 202 Step: 15200, train loss: 0.0003 F1: 98.43% dev loss: 0.0167 F1: 36.32% 0.03 min
> Epoch: 205 Step: 15400, train loss: 0.0003 F1: 98.26% dev loss: 0.0172 F1: 37.93% 0.03 min
> Epoch: 207 Step: 15600, train loss: 0.0003 F1: 98.07% dev loss: 0.0176 F1: 41.17% 0.03 min
> Epoch: 210 Step: 15800, train loss: 0.0003 F1: 97.62% dev loss: 0.0169 F1: 39.57% 0.03 min
> Epoch: 213 Step: 16000, train loss: 0.0003 F1: 98.67% dev loss: 0.0172 F1: 40.05% 0.03 min
> Epoch: 215 Step: 16200, train loss: 0.0003 F1: 98.30% dev loss: 0.0184 F1: 37.81% 0.03 min
> Epoch: 218 Step: 16400, train loss: 0.0002 F1: 98.22% dev loss: 0.0176 F1: 38.79% 0.03 min
> Epoch: 221 Step: 16600, train loss: 0.0002 F1: 97.96% dev loss: 0.0179 F1: 39.72% 0.03 min
> Epoch: 223 Step: 16800, train loss: 0.0003 F1: 98.30% dev loss: 0.0180 F1: 38.61% 0.03 min
> Epoch: 226 Step: 17000, train loss: 0.0002 F1: 98.87% dev loss: 0.0179 F1: 39.20% 0.03 min
> Epoch: 229 Step: 17200, train loss: 0.0002 F1: 97.77% dev loss: 0.0179 F1: 40.85% 0.03 min
> Epoch: 231 Step: 17400, train loss: 0.0002 F1: 98.66% dev loss: 0.0182 F1: 36.60% 0.03 min
> Epoch: 234 Step: 17600, train loss: 0.0002 F1: 98.93% dev loss: 0.0181 F1: 37.02% 0.03 min
> Epoch: 237 Step: 17800, train loss: 0.0002 F1: 98.40% dev loss: 0.0181 F1: 40.93% 0.03 min
> Epoch: 239 Step: 18000, train loss: 0.0002 F1: 98.94% dev loss: 0.0186 F1: 36.22% 0.03 min
> Epoch: 242 Step: 18200, train loss: 0.0002 F1: 99.16% dev loss: 0.0179 F1: 39.49% 0.03 min
> Epoch: 245 Step: 18400, train loss: 0.0002 F1: 98.48% dev loss: 0.0188 F1: 35.65% 0.03 min
> Epoch: 247 Step: 18600, train loss: 0.0002 F1: 98.70% dev loss: 0.0193 F1: 39.29% 0.03 min
> Epoch: 250 Step: 18800, train loss: 0.0002 F1: 98.65% dev loss: 0.0183 F1: 40.15% 0.03 min
> Epoch: 253 Step: 19000, train loss: 0.0002 F1: 98.63% dev loss: 0.0182 F1: 36.62% 0.03 min
> Epoch: 255 Step: 19200, train loss: 0.0002 F1: 98.87% dev loss: 0.0193 F1: 36.90% 0.03 min
> Epoch: 258 Step: 19400, train loss: 0.0002 F1: 99.14% dev loss: 0.0188 F1: 41.14% 0.03 min
> Epoch: 261 Step: 19600, train loss: 0.0002 F1: 98.82% dev loss: 0.0193 F1: 35.81% 0.03 min
> Epoch: 263 Step: 19800, train loss: 0.0001 F1: 99.21% dev loss: 0.0195 F1: 37.11% 0.03 min
> Epoch: 266 Step: 20000, train loss: 0.0001 F1: 99.14% dev loss: 0.0186 F1: 38.14% 0.03 min
> Epoch: 269 Step: 20200, train loss: 0.0002 F1: 98.55% dev loss: 0.0196 F1: 36.21% 0.03 min
> Epoch: 271 Step: 20400, train loss: 0.0002 F1: 99.00% dev loss: 0.0198 F1: 34.73% 0.03 min
> Epoch: 274 Step: 20600, train loss: 0.0001 F1: 99.29% dev loss: 0.0189 F1: 36.43% 0.03 min
> Epoch: 277 Step: 20800, train loss: 0.0002 F1: 99.34% dev loss: 0.0194 F1: 36.75% 0.03 min
> Epoch: 279 Step: 21000, train loss: 0.0001 F1: 98.92% dev loss: 0.0199 F1: 36.45% 0.03 min
> Epoch: 282 Step: 21200, train loss: 0.0001 F1: 99.23% dev loss: 0.0195 F1: 40.55% 0.03 min
> Epoch: 285 Step: 21400, train loss: 0.0001 F1: 99.27% dev loss: 0.0199 F1: 36.99% 0.03 min
> Epoch: 287 Step: 21600, train loss: 0.0001 F1: 99.29% dev loss: 0.0204 F1: 35.45% 0.03 min
> Epoch: 290 Step: 21800, train loss: 0.0001 F1: 99.06% dev loss: 0.0193 F1: 40.26% 0.03 min
> Epoch: 293 Step: 22000, train loss: 0.0001 F1: 98.75% dev loss: 0.0199 F1: 37.86% 0.03 min
> Epoch: 295 Step: 22200, train loss: 0.0001 F1: 99.39% dev loss: 0.0199 F1: 37.10% 0.03 min
> Epoch: 298 Step: 22400, train loss: 0.0001 F1: 98.84% dev loss: 0.0201 F1: 39.48% 0.03 min
> Epoch: 301 Step: 22600, train loss: 0.0001 F1: 98.22% dev loss: 0.0205 F1: 38.25% 0.03 min
> Epoch: 303 Step: 22800, train loss: 0.0001 F1: 99.29% dev loss: 0.0202 F1: 37.24% 0.03 min
> Epoch: 306 Step: 23000, train loss: 0.0001 F1: 99.40% dev loss: 0.0200 F1: 36.56% 0.03 min
> Epoch: 309 Step: 23200, train loss: 0.0001 F1: 99.29% dev loss: 0.0204 F1: 37.50% 0.03 min
> Epoch: 311 Step: 23400, train loss: 0.0001 F1: 99.50% dev loss: 0.0207 F1: 36.41% 0.03 min
> Epoch: 314 Step: 23600, train loss: 0.0001 F1: 99.28% dev loss: 0.0189 F1: 36.25% 0.03 min
> Epoch: 317 Step: 23800, train loss: 0.0001 F1: 99.26% dev loss: 0.0201 F1: 37.89% 0.03 min
> Epoch: 319 Step: 24000, train loss: 0.0001 F1: 99.34% dev loss: 0.0204 F1: 35.44% 0.03 min
> Epoch: 322 Step: 24200, train loss: 0.0001 F1: 99.20% dev loss: 0.0198 F1: 39.60% 0.03 min
> Epoch: 325 Step: 24400, train loss: 0.0001 F1: 99.25% dev loss: 0.0207 F1: 41.30% 0.03 min
> Epoch: 327 Step: 24600, train loss: 0.0001 F1: 99.10% dev loss: 0.0202 F1: 38.16% 0.03 min
> Epoch: 330 Step: 24800, train loss: 0.0001 F1: 99.19% dev loss: 0.0199 F1: 40.52% 0.03 min
> Epoch: 333 Step: 25000, train loss: 0.0001 F1: 99.62% dev loss: 0.0207 F1: 40.75% 0.03 min
> Epoch: 335 Step: 25200, train loss: 0.0001 F1: 98.93% dev loss: 0.0206 F1: 36.76% 0.03 min
> Epoch: 338 Step: 25400, train loss: 0.0001 F1: 99.68% dev loss: 0.0205 F1: 40.86% 0.03 min
> Epoch: 341 Step: 25600, train loss: 0.0001 F1: 99.46% dev loss: 0.0207 F1: 41.61% 0.03 min
> Epoch: 343 Step: 25800, train loss: 0.0001 F1: 99.22% dev loss: 0.0203 F1: 39.32% 0.03 min
> Epoch: 346 Step: 26000, train loss: 0.0001 F1: 99.57% dev loss: 0.0203 F1: 39.87% 0.03 min
> Epoch: 349 Step: 26200, train loss: 0.0001 F1: 99.20% dev loss: 0.0206 F1: 41.65% 0.03 min
> Epoch: 351 Step: 26400, train loss: 0.0001 F1: 99.44% dev loss: 0.0220 F1: 34.20% 0.03 min
> Epoch: 354 Step: 26600, train loss: 0.0001 F1: 99.27% dev loss: 0.0211 F1: 39.58% 0.03 min
> Epoch: 357 Step: 26800, train loss: 0.0001 F1: 99.37% dev loss: 0.0216 F1: 39.40% 0.03 min
> Epoch: 359 Step: 27000, train loss: 0.0001 F1: 99.40% dev loss: 0.0211 F1: 40.12% 0.03 min
> Epoch: 362 Step: 27200, train loss: 0.0001 F1: 99.38% dev loss: 0.0208 F1: 40.77% 0.03 min
> Epoch: 365 Step: 27400, train loss: 0.0001 F1: 99.88% dev loss: 0.0212 F1: 37.12% 0.03 min
> Epoch: 367 Step: 27600, train loss: 0.0001 F1: 99.58% dev loss: 0.0212 F1: 36.87% 0.03 min
> Epoch: 370 Step: 27800, train loss: 0.0001 F1: 99.48% dev loss: 0.0211 F1: 40.65% 0.03 min
> Epoch: 373 Step: 28000, train loss: 0.0001 F1: 99.37% dev loss: 0.0221 F1: 35.95% 0.03 min
> Epoch: 375 Step: 28200, train loss: 0.0001 F1: 99.32% dev loss: 0.0211 F1: 35.40% 0.03 min
> Epoch: 378 Step: 28400, train loss: 0.0001 F1: 99.38% dev loss: 0.0209 F1: 39.64% 0.03 min
> Epoch: 381 Step: 28600, train loss: 0.0001 F1: 99.44% dev loss: 0.0216 F1: 40.79% 0.03 min
> Epoch: 383 Step: 28800, train loss: 0.0001 F1: 99.51% dev loss: 0.0213 F1: 36.55% 0.03 min
> Epoch: 386 Step: 29000, train loss: 0.0001 F1: 99.50% dev loss: 0.0206 F1: 38.62% 0.03 min
> Epoch: 389 Step: 29200, train loss: 0.0001 F1: 99.14% dev loss: 0.0215 F1: 37.13% 0.03 min
> Epoch: 391 Step: 29400, train loss: 0.0001 F1: 99.66% dev loss: 0.0218 F1: 36.92% 0.03 min
> Epoch: 394 Step: 29600, train loss: 0.0001 F1: 99.72% dev loss: 0.0211 F1: 39.81% 0.03 min
> Epoch: 397 Step: 29800, train loss: 0.0001 F1: 99.66% dev loss: 0.0219 F1: 36.48% 0.03 min
> Epoch: 399 Step: 30000, train loss: 0.0001 F1: 99.46% dev loss: 0.0209 F1: 39.69% 0.03 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 16:29:44.453939>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 0.001
>>> epochs: 400
>>> step: 200
>>> model_name: glove
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 0.1
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/glove-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 50000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 0.001, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
> Epoch: 2 Step: 200, train loss: 0.0689 F1: 9.43% dev loss: 0.0207 F1: 9.49% 0.04 min
> Epoch: 5 Step: 400, train loss: 0.0198 F1: 13.83% dev loss: 0.0168 F1: 13.60% 0.03 min
> Epoch: 7 Step: 600, train loss: 0.0160 F1: 24.36% dev loss: 0.0135 F1: 22.87% 0.03 min
> Epoch: 10 Step: 800, train loss: 0.0128 F1: 29.48% dev loss: 0.0116 F1: 29.88% 0.03 min
> Epoch: 13 Step: 1000, train loss: 0.0106 F1: 34.45% dev loss: 0.0108 F1: 31.54% 0.03 min
> Epoch: 15 Step: 1200, train loss: 0.0093 F1: 40.65% dev loss: 0.0099 F1: 34.80% 0.03 min
> Epoch: 18 Step: 1400, train loss: 0.0082 F1: 47.54% dev loss: 0.0101 F1: 37.91% 0.03 min
> Epoch: 21 Step: 1600, train loss: 0.0073 F1: 49.99% dev loss: 0.0098 F1: 36.36% 0.03 min
> Epoch: 23 Step: 1800, train loss: 0.0066 F1: 57.00% dev loss: 0.0098 F1: 34.88% 0.03 min
> Epoch: 26 Step: 2000, train loss: 0.0058 F1: 63.83% dev loss: 0.0098 F1: 39.17% 0.03 min
> Epoch: 29 Step: 2200, train loss: 0.0051 F1: 64.65% dev loss: 0.0100 F1: 38.30% 0.03 min
> Epoch: 31 Step: 2400, train loss: 0.0047 F1: 69.75% dev loss: 0.0101 F1: 35.08% 0.03 min
> Epoch: 34 Step: 2600, train loss: 0.0042 F1: 71.62% dev loss: 0.0105 F1: 36.04% 0.03 min
> Epoch: 37 Step: 2800, train loss: 0.0037 F1: 74.42% dev loss: 0.0105 F1: 38.55% 0.03 min
> Epoch: 39 Step: 3000, train loss: 0.0034 F1: 76.94% dev loss: 0.0108 F1: 36.25% 0.03 min
> Epoch: 42 Step: 3200, train loss: 0.0031 F1: 79.75% dev loss: 0.0110 F1: 36.32% 0.03 min
> Epoch: 45 Step: 3400, train loss: 0.0028 F1: 82.17% dev loss: 0.0112 F1: 35.45% 0.03 min
> Epoch: 47 Step: 3600, train loss: 0.0025 F1: 84.74% dev loss: 0.0116 F1: 35.71% 0.03 min
> Epoch: 50 Step: 3800, train loss: 0.0023 F1: 85.62% dev loss: 0.0117 F1: 37.31% 0.03 min
> Epoch: 53 Step: 4000, train loss: 0.0021 F1: 88.82% dev loss: 0.0117 F1: 37.22% 0.03 min
> Epoch: 55 Step: 4200, train loss: 0.0019 F1: 87.61% dev loss: 0.0125 F1: 36.87% 0.03 min
> Epoch: 58 Step: 4400, train loss: 0.0018 F1: 89.95% dev loss: 0.0121 F1: 37.17% 0.03 min
> Epoch: 61 Step: 4600, train loss: 0.0016 F1: 89.59% dev loss: 0.0120 F1: 40.47% 0.03 min
> Epoch: 63 Step: 4800, train loss: 0.0015 F1: 89.74% dev loss: 0.0128 F1: 34.21% 0.03 min
> Epoch: 66 Step: 5000, train loss: 0.0014 F1: 91.13% dev loss: 0.0127 F1: 38.63% 0.03 min
> Epoch: 69 Step: 5200, train loss: 0.0013 F1: 91.76% dev loss: 0.0127 F1: 38.17% 0.03 min
> Epoch: 71 Step: 5400, train loss: 0.0011 F1: 92.33% dev loss: 0.0133 F1: 34.58% 0.03 min
> Epoch: 74 Step: 5600, train loss: 0.0010 F1: 93.72% dev loss: 0.0132 F1: 40.84% 0.03 min
> Epoch: 77 Step: 5800, train loss: 0.0009 F1: 94.16% dev loss: 0.0130 F1: 43.02% 0.03 min
> Epoch: 79 Step: 6000, train loss: 0.0009 F1: 94.60% dev loss: 0.0137 F1: 41.81% 0.03 min
> Epoch: 82 Step: 6200, train loss: 0.0009 F1: 94.98% dev loss: 0.0141 F1: 41.52% 0.03 min
> Epoch: 85 Step: 6400, train loss: 0.0008 F1: 92.87% dev loss: 0.0138 F1: 43.84% 0.03 min
> Epoch: 87 Step: 6600, train loss: 0.0007 F1: 95.83% dev loss: 0.0144 F1: 42.90% 0.03 min
> Epoch: 90 Step: 6800, train loss: 0.0006 F1: 96.33% dev loss: 0.0144 F1: 43.76% 0.03 min
> Epoch: 93 Step: 7000, train loss: 0.0006 F1: 96.93% dev loss: 0.0144 F1: 45.55% 0.03 min
> Epoch: 95 Step: 7200, train loss: 0.0006 F1: 96.09% dev loss: 0.0144 F1: 43.97% 0.03 min
> Epoch: 98 Step: 7400, train loss: 0.0006 F1: 95.35% dev loss: 0.0148 F1: 43.74% 0.03 min
> Epoch: 101 Step: 7600, train loss: 0.0005 F1: 96.96% dev loss: 0.0148 F1: 43.25% 0.03 min
> Epoch: 103 Step: 7800, train loss: 0.0005 F1: 97.05% dev loss: 0.0155 F1: 43.72% 0.03 min
> Epoch: 106 Step: 8000, train loss: 0.0004 F1: 97.61% dev loss: 0.0161 F1: 42.66% 0.03 min
> Epoch: 109 Step: 8200, train loss: 0.0004 F1: 96.97% dev loss: 0.0162 F1: 38.77% 0.03 min
> Epoch: 111 Step: 8400, train loss: 0.0004 F1: 97.11% dev loss: 0.0165 F1: 42.95% 0.03 min
> Epoch: 114 Step: 8600, train loss: 0.0004 F1: 97.94% dev loss: 0.0162 F1: 41.51% 0.03 min
> Epoch: 117 Step: 8800, train loss: 0.0003 F1: 97.71% dev loss: 0.0165 F1: 44.90% 0.03 min
> Epoch: 119 Step: 9000, train loss: 0.0004 F1: 97.78% dev loss: 0.0165 F1: 43.64% 0.03 min
> Epoch: 122 Step: 9200, train loss: 0.0003 F1: 97.97% dev loss: 0.0161 F1: 44.40% 0.03 min
> Epoch: 125 Step: 9400, train loss: 0.0003 F1: 97.89% dev loss: 0.0171 F1: 43.18% 0.03 min
> Epoch: 127 Step: 9600, train loss: 0.0003 F1: 98.28% dev loss: 0.0172 F1: 42.53% 0.03 min
> Epoch: 130 Step: 9800, train loss: 0.0003 F1: 98.43% dev loss: 0.0169 F1: 41.66% 0.03 min
> Epoch: 133 Step: 10000, train loss: 0.0002 F1: 98.06% dev loss: 0.0174 F1: 40.38% 0.03 min
> Epoch: 135 Step: 10200, train loss: 0.0003 F1: 97.81% dev loss: 0.0166 F1: 44.23% 0.03 min
> Epoch: 138 Step: 10400, train loss: 0.0002 F1: 98.97% dev loss: 0.0174 F1: 38.71% 0.03 min
> Epoch: 141 Step: 10600, train loss: 0.0002 F1: 98.68% dev loss: 0.0179 F1: 42.00% 0.03 min
> Epoch: 143 Step: 10800, train loss: 0.0002 F1: 98.86% dev loss: 0.0182 F1: 41.44% 0.03 min
> Epoch: 146 Step: 11000, train loss: 0.0002 F1: 98.79% dev loss: 0.0178 F1: 42.00% 0.03 min
> Epoch: 149 Step: 11200, train loss: 0.0002 F1: 99.02% dev loss: 0.0183 F1: 42.15% 0.03 min
> Epoch: 151 Step: 11400, train loss: 0.0002 F1: 98.39% dev loss: 0.0186 F1: 42.87% 0.03 min
> Epoch: 154 Step: 11600, train loss: 0.0002 F1: 99.04% dev loss: 0.0179 F1: 42.29% 0.03 min
> Epoch: 157 Step: 11800, train loss: 0.0001 F1: 99.25% dev loss: 0.0193 F1: 42.18% 0.03 min
> Epoch: 159 Step: 12000, train loss: 0.0002 F1: 98.26% dev loss: 0.0196 F1: 39.49% 0.03 min
> Epoch: 162 Step: 12200, train loss: 0.0002 F1: 98.66% dev loss: 0.0181 F1: 41.42% 0.03 min
> Epoch: 165 Step: 12400, train loss: 0.0002 F1: 97.68% dev loss: 0.0186 F1: 45.19% 0.03 min
> Epoch: 167 Step: 12600, train loss: 0.0002 F1: 98.91% dev loss: 0.0197 F1: 38.80% 0.03 min
> Epoch: 170 Step: 12800, train loss: 0.0001 F1: 99.05% dev loss: 0.0189 F1: 43.38% 0.03 min
> Epoch: 173 Step: 13000, train loss: 0.0001 F1: 98.86% dev loss: 0.0188 F1: 42.33% 0.03 min
> Epoch: 175 Step: 13200, train loss: 0.0002 F1: 98.73% dev loss: 0.0194 F1: 39.17% 0.03 min
> Epoch: 178 Step: 13400, train loss: 0.0001 F1: 98.75% dev loss: 0.0190 F1: 43.11% 0.03 min
> Epoch: 181 Step: 13600, train loss: 0.0001 F1: 98.98% dev loss: 0.0201 F1: 41.99% 0.03 min
> Epoch: 183 Step: 13800, train loss: 0.0001 F1: 99.19% dev loss: 0.0193 F1: 41.24% 0.03 min
> Epoch: 186 Step: 14000, train loss: 0.0001 F1: 99.12% dev loss: 0.0193 F1: 45.24% 0.03 min
> Epoch: 189 Step: 14200, train loss: 0.0001 F1: 99.10% dev loss: 0.0193 F1: 40.47% 0.03 min
> Epoch: 191 Step: 14400, train loss: 0.0001 F1: 99.11% dev loss: 0.0196 F1: 37.59% 0.03 min
> Epoch: 194 Step: 14600, train loss: 0.0003 F1: 98.25% dev loss: 0.0192 F1: 41.88% 0.03 min
> Epoch: 197 Step: 14800, train loss: 0.0001 F1: 99.02% dev loss: 0.0199 F1: 44.38% 0.03 min
> Epoch: 199 Step: 15000, train loss: 0.0001 F1: 99.11% dev loss: 0.0200 F1: 45.75% 0.03 min
> Epoch: 202 Step: 15200, train loss: 0.0001 F1: 99.51% dev loss: 0.0198 F1: 47.07% 0.03 min
> Epoch: 205 Step: 15400, train loss: 0.0001 F1: 98.74% dev loss: 0.0199 F1: 42.27% 0.03 min
> Epoch: 207 Step: 15600, train loss: 0.0001 F1: 99.47% dev loss: 0.0197 F1: 45.69% 0.03 min
> Epoch: 210 Step: 15800, train loss: 0.0001 F1: 99.32% dev loss: 0.0197 F1: 45.67% 0.03 min
> Epoch: 213 Step: 16000, train loss: 0.0001 F1: 99.22% dev loss: 0.0204 F1: 41.59% 0.03 min
> Epoch: 215 Step: 16200, train loss: 0.0001 F1: 99.43% dev loss: 0.0199 F1: 42.61% 0.03 min
> Epoch: 218 Step: 16400, train loss: 0.0001 F1: 99.06% dev loss: 0.0199 F1: 45.86% 0.03 min
> Epoch: 221 Step: 16600, train loss: 0.0001 F1: 99.22% dev loss: 0.0198 F1: 40.97% 0.03 min
> Epoch: 223 Step: 16800, train loss: 0.0001 F1: 99.09% dev loss: 0.0205 F1: 46.32% 0.03 min
> Epoch: 226 Step: 17000, train loss: 0.0001 F1: 99.25% dev loss: 0.0203 F1: 45.25% 0.03 min
> Epoch: 229 Step: 17200, train loss: 0.0001 F1: 99.56% dev loss: 0.0211 F1: 42.42% 0.03 min
> Epoch: 231 Step: 17400, train loss: 0.0001 F1: 99.68% dev loss: 0.0208 F1: 42.61% 0.03 min
> Epoch: 234 Step: 17600, train loss: 0.0001 F1: 99.70% dev loss: 0.0207 F1: 44.36% 0.03 min
> Epoch: 237 Step: 17800, train loss: 0.0001 F1: 98.30% dev loss: 0.0206 F1: 42.63% 0.03 min
> Epoch: 239 Step: 18000, train loss: 0.0001 F1: 99.76% dev loss: 0.0203 F1: 46.32% 0.03 min
> Epoch: 242 Step: 18200, train loss: 0.0001 F1: 99.20% dev loss: 0.0204 F1: 45.35% 0.03 min
> Epoch: 245 Step: 18400, train loss: 0.0001 F1: 99.37% dev loss: 0.0207 F1: 43.54% 0.03 min
> Epoch: 247 Step: 18600, train loss: 0.0001 F1: 99.31% dev loss: 0.0209 F1: 42.81% 0.03 min
> Epoch: 250 Step: 18800, train loss: 0.0001 F1: 99.48% dev loss: 0.0209 F1: 41.81% 0.03 min
> Epoch: 253 Step: 19000, train loss: 0.0001 F1: 99.61% dev loss: 0.0208 F1: 43.06% 0.03 min
> Epoch: 255 Step: 19200, train loss: 0.0001 F1: 99.45% dev loss: 0.0213 F1: 43.66% 0.03 min
> Epoch: 258 Step: 19400, train loss: 0.0001 F1: 99.74% dev loss: 0.0208 F1: 43.55% 0.03 min
> Epoch: 261 Step: 19600, train loss: 0.0001 F1: 99.86% dev loss: 0.0216 F1: 43.13% 0.03 min
> Epoch: 263 Step: 19800, train loss: 0.0000 F1: 99.68% dev loss: 0.0215 F1: 44.50% 0.03 min
> Epoch: 266 Step: 20000, train loss: 0.0001 F1: 99.24% dev loss: 0.0205 F1: 43.59% 0.03 min
> Epoch: 269 Step: 20200, train loss: 0.0001 F1: 99.67% dev loss: 0.0219 F1: 41.91% 0.03 min
> Epoch: 271 Step: 20400, train loss: 0.0001 F1: 99.49% dev loss: 0.0220 F1: 42.42% 0.03 min
> Epoch: 274 Step: 20600, train loss: 0.0001 F1: 99.64% dev loss: 0.0219 F1: 42.92% 0.03 min
> Epoch: 277 Step: 20800, train loss: 0.0000 F1: 99.88% dev loss: 0.0220 F1: 43.37% 0.03 min
> Epoch: 279 Step: 21000, train loss: 0.0000 F1: 99.77% dev loss: 0.0214 F1: 44.39% 0.03 min
> Epoch: 282 Step: 21200, train loss: 0.0001 F1: 99.65% dev loss: 0.0217 F1: 43.74% 0.03 min
> Epoch: 285 Step: 21400, train loss: 0.0001 F1: 99.48% dev loss: 0.0225 F1: 43.90% 0.03 min
> Epoch: 287 Step: 21600, train loss: 0.0000 F1: 99.63% dev loss: 0.0221 F1: 44.62% 0.03 min
> Epoch: 290 Step: 21800, train loss: 0.0000 F1: 99.74% dev loss: 0.0218 F1: 43.07% 0.03 min
> Epoch: 293 Step: 22000, train loss: 0.0000 F1: 99.69% dev loss: 0.0224 F1: 41.75% 0.03 min
> Epoch: 295 Step: 22200, train loss: 0.0001 F1: 99.71% dev loss: 0.0217 F1: 43.52% 0.03 min
> Epoch: 298 Step: 22400, train loss: 0.0000 F1: 99.65% dev loss: 0.0223 F1: 42.83% 0.03 min
> Epoch: 301 Step: 22600, train loss: 0.0001 F1: 99.77% dev loss: 0.0230 F1: 42.03% 0.03 min
> Epoch: 303 Step: 22800, train loss: 0.0001 F1: 99.72% dev loss: 0.0221 F1: 47.01% 0.03 min
> Epoch: 306 Step: 23000, train loss: 0.0001 F1: 99.69% dev loss: 0.0225 F1: 42.64% 0.03 min
> Epoch: 309 Step: 23200, train loss: 0.0001 F1: 99.66% dev loss: 0.0223 F1: 40.74% 0.03 min
> Epoch: 311 Step: 23400, train loss: 0.0000 F1: 99.81% dev loss: 0.0221 F1: 40.93% 0.03 min
> Epoch: 314 Step: 23600, train loss: 0.0000 F1: 99.57% dev loss: 0.0222 F1: 41.63% 0.03 min
> Epoch: 317 Step: 23800, train loss: 0.0000 F1: 99.54% dev loss: 0.0225 F1: 42.92% 0.03 min
> Epoch: 319 Step: 24000, train loss: 0.0001 F1: 99.58% dev loss: 0.0224 F1: 43.72% 0.03 min
> Epoch: 322 Step: 24200, train loss: 0.0000 F1: 99.77% dev loss: 0.0224 F1: 43.62% 0.03 min
> Epoch: 325 Step: 24400, train loss: 0.0001 F1: 99.51% dev loss: 0.0226 F1: 44.66% 0.03 min
> Epoch: 327 Step: 24600, train loss: 0.0000 F1: 99.50% dev loss: 0.0224 F1: 42.72% 0.03 min
> Epoch: 330 Step: 24800, train loss: 0.0000 F1: 99.40% dev loss: 0.0224 F1: 43.32% 0.03 min
> Epoch: 333 Step: 25000, train loss: 0.0000 F1: 99.85% dev loss: 0.0228 F1: 44.40% 0.03 min
> Epoch: 335 Step: 25200, train loss: 0.0000 F1: 99.83% dev loss: 0.0224 F1: 42.86% 0.03 min
> Epoch: 338 Step: 25400, train loss: 0.0000 F1: 99.73% dev loss: 0.0221 F1: 43.83% 0.03 min
> Epoch: 341 Step: 25600, train loss: 0.0000 F1: 99.40% dev loss: 0.0215 F1: 42.67% 0.03 min
> Epoch: 343 Step: 25800, train loss: 0.0000 F1: 99.72% dev loss: 0.0228 F1: 45.22% 0.03 min
> Epoch: 346 Step: 26000, train loss: 0.0001 F1: 99.64% dev loss: 0.0222 F1: 43.67% 0.03 min
> Epoch: 349 Step: 26200, train loss: 0.0000 F1: 99.82% dev loss: 0.0230 F1: 42.51% 0.03 min
> Epoch: 351 Step: 26400, train loss: 0.0000 F1: 99.81% dev loss: 0.0226 F1: 43.09% 0.03 min
> Epoch: 354 Step: 26600, train loss: 0.0000 F1: 99.68% dev loss: 0.0228 F1: 43.13% 0.03 min
> Epoch: 357 Step: 26800, train loss: 0.0000 F1: 99.85% dev loss: 0.0231 F1: 45.02% 0.03 min
> Epoch: 359 Step: 27000, train loss: 0.0000 F1: 99.72% dev loss: 0.0229 F1: 43.08% 0.03 min
> Epoch: 362 Step: 27200, train loss: 0.0000 F1: 99.62% dev loss: 0.0227 F1: 42.77% 0.03 min
> Epoch: 365 Step: 27400, train loss: 0.0000 F1: 99.82% dev loss: 0.0226 F1: 42.38% 0.03 min
> Epoch: 367 Step: 27600, train loss: 0.0000 F1: 99.69% dev loss: 0.0227 F1: 44.05% 0.03 min
> Epoch: 370 Step: 27800, train loss: 0.0000 F1: 99.86% dev loss: 0.0235 F1: 43.61% 0.03 min
> Epoch: 373 Step: 28000, train loss: 0.0000 F1: 99.68% dev loss: 0.0228 F1: 43.14% 0.03 min
> Epoch: 375 Step: 28200, train loss: 0.0000 F1: 99.75% dev loss: 0.0218 F1: 44.63% 0.03 min
> Epoch: 378 Step: 28400, train loss: 0.0000 F1: 99.71% dev loss: 0.0229 F1: 43.06% 0.03 min
> Epoch: 381 Step: 28600, train loss: 0.0000 F1: 99.70% dev loss: 0.0230 F1: 42.66% 0.03 min
> Epoch: 383 Step: 28800, train loss: 0.0000 F1: 99.87% dev loss: 0.0235 F1: 43.17% 0.03 min
> Epoch: 386 Step: 29000, train loss: 0.0000 F1: 99.71% dev loss: 0.0230 F1: 43.70% 0.03 min
> Epoch: 389 Step: 29200, train loss: 0.0000 F1: 99.23% dev loss: 0.0233 F1: 43.55% 0.03 min
> Epoch: 391 Step: 29400, train loss: 0.0000 F1: 99.49% dev loss: 0.0235 F1: 44.86% 0.03 min
> Epoch: 394 Step: 29600, train loss: 0.0000 F1: 99.96% dev loss: 0.0232 F1: 43.93% 0.03 min
> Epoch: 397 Step: 29800, train loss: 0.0000 F1: 99.71% dev loss: 0.0233 F1: 43.28% 0.03 min
> Epoch: 399 Step: 30000, train loss: 0.0000 F1: 99.92% dev loss: 0.0233 F1: 43.71% 0.03 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 17:52:04.272097>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 0.005
>>> epochs: 400
>>> step: 100
>>> model_name: glove
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 0.1
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/glove-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 5000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 0.005, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
> Epoch: 1 Step: 100, train loss: 0.0660 F1: 11.57% dev loss: 0.0204 F1: 9.50% 0.03 min
> Epoch: 2 Step: 200, train loss: 0.0202 F1: 13.46% dev loss: 0.0164 F1: 13.50% 0.01 min
> Epoch: 3 Step: 300, train loss: 0.0153 F1: 23.68% dev loss: 0.0131 F1: 26.51% 0.01 min
> Epoch: 5 Step: 400, train loss: 0.0121 F1: 30.47% dev loss: 0.0110 F1: 30.19% 0.01 min
> Epoch: 6 Step: 500, train loss: 0.0104 F1: 40.33% dev loss: 0.0107 F1: 34.99% 0.01 min
> Epoch: 7 Step: 600, train loss: 0.0086 F1: 44.08% dev loss: 0.0103 F1: 41.68% 0.01 min
> Epoch: 9 Step: 700, train loss: 0.0076 F1: 56.29% dev loss: 0.0100 F1: 41.42% 0.01 min
> Epoch: 10 Step: 800, train loss: 0.0066 F1: 58.02% dev loss: 0.0104 F1: 34.00% 0.01 min
> Epoch: 11 Step: 900, train loss: 0.0057 F1: 61.47% dev loss: 0.0102 F1: 34.72% 0.01 min
> Epoch: 13 Step: 1000, train loss: 0.0049 F1: 67.62% dev loss: 0.0111 F1: 38.55% 0.01 min
> Epoch: 14 Step: 1100, train loss: 0.0043 F1: 72.89% dev loss: 0.0103 F1: 37.16% 0.01 min
> Epoch: 15 Step: 1200, train loss: 0.0037 F1: 74.63% dev loss: 0.0107 F1: 37.61% 0.01 min
> Epoch: 17 Step: 1300, train loss: 0.0032 F1: 77.85% dev loss: 0.0112 F1: 35.46% 0.01 min
> Epoch: 18 Step: 1400, train loss: 0.0028 F1: 82.78% dev loss: 0.0119 F1: 35.97% 0.01 min
> Epoch: 19 Step: 1500, train loss: 0.0025 F1: 82.86% dev loss: 0.0117 F1: 40.82% 0.01 min
> Epoch: 21 Step: 1600, train loss: 0.0023 F1: 86.23% dev loss: 0.0120 F1: 36.18% 0.01 min
> Epoch: 22 Step: 1700, train loss: 0.0020 F1: 87.12% dev loss: 0.0123 F1: 40.94% 0.01 min
> Epoch: 23 Step: 1800, train loss: 0.0018 F1: 88.51% dev loss: 0.0128 F1: 34.00% 0.01 min
> Epoch: 25 Step: 1900, train loss: 0.0015 F1: 90.76% dev loss: 0.0124 F1: 38.99% 0.01 min
> Epoch: 26 Step: 2000, train loss: 0.0014 F1: 91.61% dev loss: 0.0126 F1: 40.09% 0.01 min
> Epoch: 27 Step: 2100, train loss: 0.0013 F1: 91.44% dev loss: 0.0132 F1: 40.62% 0.01 min
> Epoch: 29 Step: 2200, train loss: 0.0011 F1: 94.01% dev loss: 0.0135 F1: 40.56% 0.01 min
> Epoch: 30 Step: 2300, train loss: 0.0010 F1: 94.19% dev loss: 0.0135 F1: 41.48% 0.01 min
> Epoch: 31 Step: 2400, train loss: 0.0010 F1: 92.98% dev loss: 0.0148 F1: 35.87% 0.01 min
> Epoch: 33 Step: 2500, train loss: 0.0008 F1: 94.18% dev loss: 0.0142 F1: 43.38% 0.01 min
> Epoch: 34 Step: 2600, train loss: 0.0008 F1: 94.30% dev loss: 0.0138 F1: 42.41% 0.01 min
> Epoch: 35 Step: 2700, train loss: 0.0007 F1: 95.46% dev loss: 0.0146 F1: 40.19% 0.01 min
> Epoch: 37 Step: 2800, train loss: 0.0006 F1: 96.20% dev loss: 0.0142 F1: 43.01% 0.01 min
> Epoch: 38 Step: 2900, train loss: 0.0006 F1: 95.95% dev loss: 0.0151 F1: 41.03% 0.01 min
> Epoch: 39 Step: 3000, train loss: 0.0006 F1: 96.23% dev loss: 0.0149 F1: 41.11% 0.01 min
> Epoch: 41 Step: 3100, train loss: 0.0006 F1: 96.07% dev loss: 0.0147 F1: 40.77% 0.01 min
> Epoch: 42 Step: 3200, train loss: 0.0005 F1: 97.25% dev loss: 0.0155 F1: 38.42% 0.01 min
> Epoch: 43 Step: 3300, train loss: 0.0005 F1: 97.03% dev loss: 0.0153 F1: 37.98% 0.01 min
> Epoch: 45 Step: 3400, train loss: 0.0004 F1: 97.49% dev loss: 0.0151 F1: 37.41% 0.01 min
> Epoch: 46 Step: 3500, train loss: 0.0004 F1: 97.40% dev loss: 0.0156 F1: 42.15% 0.01 min
> Epoch: 47 Step: 3600, train loss: 0.0004 F1: 97.98% dev loss: 0.0156 F1: 38.11% 0.01 min
> Epoch: 49 Step: 3700, train loss: 0.0003 F1: 98.08% dev loss: 0.0157 F1: 37.93% 0.01 min
> Epoch: 50 Step: 3800, train loss: 0.0003 F1: 98.57% dev loss: 0.0155 F1: 41.17% 0.01 min
> Epoch: 51 Step: 3900, train loss: 0.0003 F1: 98.15% dev loss: 0.0156 F1: 38.64% 0.01 min
> Epoch: 53 Step: 4000, train loss: 0.0003 F1: 98.10% dev loss: 0.0160 F1: 39.12% 0.01 min
> Epoch: 54 Step: 4100, train loss: 0.0003 F1: 98.57% dev loss: 0.0159 F1: 40.86% 0.01 min
> Epoch: 55 Step: 4200, train loss: 0.0002 F1: 98.84% dev loss: 0.0161 F1: 41.24% 0.01 min
> Epoch: 57 Step: 4300, train loss: 0.0002 F1: 97.98% dev loss: 0.0161 F1: 39.89% 0.01 min
> Epoch: 58 Step: 4400, train loss: 0.0002 F1: 98.97% dev loss: 0.0160 F1: 40.75% 0.01 min
> Epoch: 59 Step: 4500, train loss: 0.0002 F1: 98.35% dev loss: 0.0162 F1: 41.14% 0.01 min
> Epoch: 61 Step: 4600, train loss: 0.0002 F1: 99.06% dev loss: 0.0165 F1: 41.06% 0.01 min
> Epoch: 62 Step: 4700, train loss: 0.0002 F1: 99.01% dev loss: 0.0163 F1: 42.12% 0.01 min
> Epoch: 63 Step: 4800, train loss: 0.0002 F1: 99.30% dev loss: 0.0163 F1: 42.38% 0.01 min
> Epoch: 65 Step: 4900, train loss: 0.0002 F1: 99.23% dev loss: 0.0163 F1: 42.14% 0.01 min
> Epoch: 66 Step: 5000, train loss: 0.0002 F1: 99.27% dev loss: 0.0163 F1: 42.15% 0.01 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 17:53:24.228577>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 0.01
>>> epochs: 400
>>> step: 100
>>> model_name: glove
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 0.1
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/glove-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 5000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 0.01, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
> Epoch: 1 Step: 100, train loss: 0.0540 F1: 11.57% dev loss: 0.0186 F1: 12.28% 0.03 min
> Epoch: 2 Step: 200, train loss: 0.0181 F1: 18.84% dev loss: 0.0139 F1: 24.52% 0.01 min
> Epoch: 3 Step: 300, train loss: 0.0125 F1: 28.96% dev loss: 0.0112 F1: 31.19% 0.01 min
> Epoch: 5 Step: 400, train loss: 0.0102 F1: 40.81% dev loss: 0.0102 F1: 38.12% 0.01 min
> Epoch: 6 Step: 500, train loss: 0.0087 F1: 48.50% dev loss: 0.0102 F1: 36.88% 0.01 min
> Epoch: 7 Step: 600, train loss: 0.0073 F1: 51.93% dev loss: 0.0100 F1: 34.81% 0.01 min
> Epoch: 9 Step: 700, train loss: 0.0059 F1: 63.42% dev loss: 0.0107 F1: 39.95% 0.01 min
> Epoch: 10 Step: 800, train loss: 0.0049 F1: 69.49% dev loss: 0.0107 F1: 33.56% 0.02 min
> Epoch: 11 Step: 900, train loss: 0.0040 F1: 72.98% dev loss: 0.0108 F1: 38.34% 0.01 min
> Epoch: 13 Step: 1000, train loss: 0.0034 F1: 76.51% dev loss: 0.0120 F1: 37.00% 0.01 min
> Epoch: 14 Step: 1100, train loss: 0.0030 F1: 81.42% dev loss: 0.0115 F1: 43.70% 0.01 min
> Epoch: 15 Step: 1200, train loss: 0.0026 F1: 83.15% dev loss: 0.0119 F1: 39.56% 0.01 min
> Epoch: 17 Step: 1300, train loss: 0.0021 F1: 87.56% dev loss: 0.0120 F1: 41.73% 0.01 min
> Epoch: 18 Step: 1400, train loss: 0.0017 F1: 88.10% dev loss: 0.0137 F1: 43.30% 0.01 min
> Epoch: 19 Step: 1500, train loss: 0.0015 F1: 90.66% dev loss: 0.0127 F1: 47.63% 0.01 min
> Epoch: 21 Step: 1600, train loss: 0.0015 F1: 90.53% dev loss: 0.0135 F1: 41.57% 0.01 min
> Epoch: 22 Step: 1700, train loss: 0.0013 F1: 90.92% dev loss: 0.0136 F1: 44.79% 0.01 min
> Epoch: 23 Step: 1800, train loss: 0.0011 F1: 91.85% dev loss: 0.0136 F1: 45.03% 0.01 min
> Epoch: 25 Step: 1900, train loss: 0.0009 F1: 93.11% dev loss: 0.0140 F1: 42.26% 0.01 min
> Epoch: 26 Step: 2000, train loss: 0.0008 F1: 94.94% dev loss: 0.0146 F1: 41.47% 0.01 min
> Epoch: 27 Step: 2100, train loss: 0.0008 F1: 94.42% dev loss: 0.0150 F1: 39.81% 0.01 min
> Epoch: 29 Step: 2200, train loss: 0.0007 F1: 94.96% dev loss: 0.0139 F1: 47.44% 0.01 min
> Epoch: 30 Step: 2300, train loss: 0.0007 F1: 95.31% dev loss: 0.0148 F1: 42.30% 0.01 min
> Epoch: 31 Step: 2400, train loss: 0.0007 F1: 94.88% dev loss: 0.0162 F1: 43.77% 0.01 min
> Epoch: 33 Step: 2500, train loss: 0.0006 F1: 95.05% dev loss: 0.0143 F1: 45.15% 0.01 min
> Epoch: 34 Step: 2600, train loss: 0.0006 F1: 96.35% dev loss: 0.0154 F1: 43.52% 0.01 min
> Epoch: 35 Step: 2700, train loss: 0.0005 F1: 96.51% dev loss: 0.0161 F1: 43.43% 0.01 min
> Epoch: 37 Step: 2800, train loss: 0.0004 F1: 97.33% dev loss: 0.0153 F1: 42.99% 0.01 min
> Epoch: 38 Step: 2900, train loss: 0.0004 F1: 97.21% dev loss: 0.0159 F1: 41.22% 0.01 min
> Epoch: 39 Step: 3000, train loss: 0.0004 F1: 96.89% dev loss: 0.0158 F1: 41.45% 0.01 min
> Epoch: 41 Step: 3100, train loss: 0.0004 F1: 97.61% dev loss: 0.0154 F1: 43.70% 0.01 min
> Epoch: 42 Step: 3200, train loss: 0.0004 F1: 97.67% dev loss: 0.0156 F1: 43.30% 0.01 min
> Epoch: 43 Step: 3300, train loss: 0.0003 F1: 97.49% dev loss: 0.0162 F1: 46.14% 0.01 min
> Epoch: 45 Step: 3400, train loss: 0.0003 F1: 98.59% dev loss: 0.0165 F1: 42.44% 0.01 min
> Epoch: 46 Step: 3500, train loss: 0.0003 F1: 98.25% dev loss: 0.0163 F1: 43.28% 0.01 min
> Epoch: 47 Step: 3600, train loss: 0.0003 F1: 98.01% dev loss: 0.0163 F1: 45.92% 0.01 min
> Epoch: 49 Step: 3700, train loss: 0.0002 F1: 98.86% dev loss: 0.0160 F1: 45.43% 0.01 min
> Epoch: 50 Step: 3800, train loss: 0.0002 F1: 98.95% dev loss: 0.0166 F1: 45.67% 0.01 min
> Epoch: 51 Step: 3900, train loss: 0.0002 F1: 98.65% dev loss: 0.0169 F1: 46.27% 0.01 min
> Epoch: 53 Step: 4000, train loss: 0.0002 F1: 99.29% dev loss: 0.0168 F1: 44.42% 0.01 min
> Epoch: 54 Step: 4100, train loss: 0.0002 F1: 98.56% dev loss: 0.0165 F1: 45.63% 0.01 min
> Epoch: 55 Step: 4200, train loss: 0.0002 F1: 99.15% dev loss: 0.0164 F1: 45.38% 0.01 min
> Epoch: 57 Step: 4300, train loss: 0.0001 F1: 98.80% dev loss: 0.0167 F1: 44.59% 0.01 min
> Epoch: 58 Step: 4400, train loss: 0.0001 F1: 99.50% dev loss: 0.0167 F1: 44.42% 0.01 min
> Epoch: 59 Step: 4500, train loss: 0.0001 F1: 99.15% dev loss: 0.0169 F1: 46.60% 0.01 min
> Epoch: 61 Step: 4600, train loss: 0.0001 F1: 99.18% dev loss: 0.0170 F1: 50.54% 0.01 min
> Epoch: 62 Step: 4700, train loss: 0.0001 F1: 99.39% dev loss: 0.0168 F1: 48.68% 0.01 min
> Epoch: 63 Step: 4800, train loss: 0.0001 F1: 99.30% dev loss: 0.0169 F1: 49.24% 0.01 min
> Epoch: 65 Step: 4900, train loss: 0.0001 F1: 99.59% dev loss: 0.0170 F1: 49.64% 0.01 min
> Epoch: 66 Step: 5000, train loss: 0.0001 F1: 99.72% dev loss: 0.0171 F1: 49.63% 0.01 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 17:54:31.232049>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 0.05
>>> epochs: 400
>>> step: 100
>>> model_name: glove
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 0.1
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/glove-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 5000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 0.05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
> Epoch: 1 Step: 100, train loss: 0.0365 F1: 17.19% dev loss: 0.0150 F1: 17.15% 0.03 min
> Epoch: 2 Step: 200, train loss: 0.0135 F1: 29.49% dev loss: 0.0107 F1: 36.49% 0.01 min
> Epoch: 3 Step: 300, train loss: 0.0101 F1: 38.47% dev loss: 0.0101 F1: 36.71% 0.01 min
> Epoch: 5 Step: 400, train loss: 0.0088 F1: 46.55% dev loss: 0.0116 F1: 36.88% 0.01 min
> Epoch: 6 Step: 500, train loss: 0.0088 F1: 46.32% dev loss: 0.0125 F1: 34.69% 0.01 min
> Epoch: 7 Step: 600, train loss: 0.0078 F1: 49.90% dev loss: 0.0115 F1: 33.37% 0.01 min
> Epoch: 9 Step: 700, train loss: 0.0073 F1: 56.05% dev loss: 0.0118 F1: 35.98% 0.01 min
> Epoch: 10 Step: 800, train loss: 0.0070 F1: 59.46% dev loss: 0.0119 F1: 36.97% 0.01 min
> Epoch: 11 Step: 900, train loss: 0.0064 F1: 57.72% dev loss: 0.0119 F1: 40.87% 0.01 min
> Epoch: 13 Step: 1000, train loss: 0.0061 F1: 60.84% dev loss: 0.0120 F1: 35.38% 0.01 min
> Epoch: 14 Step: 1100, train loss: 0.0059 F1: 62.01% dev loss: 0.0115 F1: 39.08% 0.01 min
> Epoch: 15 Step: 1200, train loss: 0.0052 F1: 63.89% dev loss: 0.0120 F1: 35.90% 0.01 min
> Epoch: 17 Step: 1300, train loss: 0.0052 F1: 62.67% dev loss: 0.0122 F1: 37.90% 0.01 min
> Epoch: 18 Step: 1400, train loss: 0.0053 F1: 62.95% dev loss: 0.0124 F1: 44.65% 0.01 min
> Epoch: 19 Step: 1500, train loss: 0.0048 F1: 64.70% dev loss: 0.0117 F1: 37.81% 0.01 min
> Epoch: 21 Step: 1600, train loss: 0.0047 F1: 65.65% dev loss: 0.0122 F1: 38.74% 0.01 min
> Epoch: 22 Step: 1700, train loss: 0.0046 F1: 69.51% dev loss: 0.0125 F1: 44.38% 0.01 min
> Epoch: 23 Step: 1800, train loss: 0.0047 F1: 68.59% dev loss: 0.0126 F1: 38.19% 0.01 min
> Epoch: 25 Step: 1900, train loss: 0.0039 F1: 71.91% dev loss: 0.0128 F1: 38.70% 0.01 min
> Epoch: 26 Step: 2000, train loss: 0.0039 F1: 74.92% dev loss: 0.0130 F1: 43.96% 0.01 min
> Epoch: 27 Step: 2100, train loss: 0.0037 F1: 73.82% dev loss: 0.0125 F1: 41.95% 0.01 min
> Epoch: 29 Step: 2200, train loss: 0.0036 F1: 76.61% dev loss: 0.0130 F1: 41.74% 0.01 min
> Epoch: 30 Step: 2300, train loss: 0.0037 F1: 76.09% dev loss: 0.0126 F1: 45.94% 0.01 min
> Epoch: 31 Step: 2400, train loss: 0.0033 F1: 76.84% dev loss: 0.0128 F1: 41.98% 0.01 min
> Epoch: 33 Step: 2500, train loss: 0.0032 F1: 78.01% dev loss: 0.0137 F1: 37.26% 0.01 min
> Epoch: 34 Step: 2600, train loss: 0.0032 F1: 80.24% dev loss: 0.0121 F1: 42.17% 0.01 min
> Epoch: 35 Step: 2700, train loss: 0.0031 F1: 78.80% dev loss: 0.0132 F1: 46.96% 0.01 min
> Epoch: 37 Step: 2800, train loss: 0.0028 F1: 78.74% dev loss: 0.0140 F1: 40.68% 0.01 min
> Epoch: 38 Step: 2900, train loss: 0.0030 F1: 81.64% dev loss: 0.0131 F1: 41.98% 0.01 min
> Epoch: 39 Step: 3000, train loss: 0.0030 F1: 79.55% dev loss: 0.0138 F1: 43.53% 0.01 min
> Epoch: 41 Step: 3100, train loss: 0.0026 F1: 80.95% dev loss: 0.0140 F1: 41.86% 0.01 min
> Epoch: 42 Step: 3200, train loss: 0.0026 F1: 82.62% dev loss: 0.0135 F1: 43.84% 0.01 min
> Epoch: 43 Step: 3300, train loss: 0.0026 F1: 81.16% dev loss: 0.0145 F1: 41.52% 0.01 min
> Epoch: 45 Step: 3400, train loss: 0.0024 F1: 84.83% dev loss: 0.0144 F1: 40.69% 0.01 min
> Epoch: 46 Step: 3500, train loss: 0.0023 F1: 85.28% dev loss: 0.0140 F1: 41.02% 0.01 min
> Epoch: 47 Step: 3600, train loss: 0.0023 F1: 84.07% dev loss: 0.0147 F1: 37.83% 0.01 min
> Epoch: 49 Step: 3700, train loss: 0.0019 F1: 85.33% dev loss: 0.0149 F1: 41.57% 0.01 min
> Epoch: 50 Step: 3800, train loss: 0.0018 F1: 87.71% dev loss: 0.0148 F1: 41.59% 0.01 min
> Epoch: 51 Step: 3900, train loss: 0.0019 F1: 87.22% dev loss: 0.0149 F1: 43.26% 0.01 min
> Epoch: 53 Step: 4000, train loss: 0.0016 F1: 88.30% dev loss: 0.0141 F1: 40.24% 0.01 min
> Epoch: 54 Step: 4100, train loss: 0.0016 F1: 89.15% dev loss: 0.0143 F1: 43.08% 0.01 min
> Epoch: 55 Step: 4200, train loss: 0.0015 F1: 89.49% dev loss: 0.0148 F1: 40.41% 0.01 min
> Epoch: 57 Step: 4300, train loss: 0.0015 F1: 90.21% dev loss: 0.0152 F1: 41.91% 0.01 min
> Epoch: 58 Step: 4400, train loss: 0.0014 F1: 89.52% dev loss: 0.0150 F1: 40.13% 0.01 min
> Epoch: 59 Step: 4500, train loss: 0.0014 F1: 90.42% dev loss: 0.0149 F1: 41.13% 0.01 min
> Epoch: 61 Step: 4600, train loss: 0.0013 F1: 91.08% dev loss: 0.0152 F1: 39.22% 0.01 min
> Epoch: 62 Step: 4700, train loss: 0.0013 F1: 91.53% dev loss: 0.0151 F1: 38.83% 0.02 min
> Epoch: 63 Step: 4800, train loss: 0.0013 F1: 91.42% dev loss: 0.0152 F1: 39.82% 0.02 min
> Epoch: 65 Step: 4900, train loss: 0.0011 F1: 94.27% dev loss: 0.0152 F1: 38.58% 0.01 min
> Epoch: 66 Step: 5000, train loss: 0.0011 F1: 91.67% dev loss: 0.0152 F1: 39.82% 0.01 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 17:56:16.823179>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 0.02
>>> epochs: 400
>>> step: 100
>>> model_name: glove
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 0.1
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/glove-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 5000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 0.02, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
> Epoch: 1 Step: 100, train loss: 0.0452 F1: 13.57% dev loss: 0.0175 F1: 13.57% 0.03 min
> Epoch: 2 Step: 200, train loss: 0.0158 F1: 25.88% dev loss: 0.0123 F1: 29.85% 0.01 min
> Epoch: 3 Step: 300, train loss: 0.0109 F1: 36.21% dev loss: 0.0105 F1: 35.56% 0.01 min
> Epoch: 5 Step: 400, train loss: 0.0092 F1: 42.08% dev loss: 0.0100 F1: 39.41% 0.01 min
> Epoch: 6 Step: 500, train loss: 0.0079 F1: 52.09% dev loss: 0.0110 F1: 38.43% 0.02 min
> Epoch: 7 Step: 600, train loss: 0.0068 F1: 53.42% dev loss: 0.0106 F1: 39.61% 0.02 min
> Epoch: 9 Step: 700, train loss: 0.0055 F1: 63.82% dev loss: 0.0115 F1: 37.13% 0.01 min
> Epoch: 10 Step: 800, train loss: 0.0047 F1: 68.91% dev loss: 0.0111 F1: 41.66% 0.01 min
> Epoch: 11 Step: 900, train loss: 0.0041 F1: 72.05% dev loss: 0.0117 F1: 35.84% 0.01 min
> Epoch: 13 Step: 1000, train loss: 0.0034 F1: 77.47% dev loss: 0.0123 F1: 40.08% 0.01 min
> Epoch: 14 Step: 1100, train loss: 0.0033 F1: 78.77% dev loss: 0.0114 F1: 45.00% 0.01 min
> Epoch: 15 Step: 1200, train loss: 0.0027 F1: 81.23% dev loss: 0.0123 F1: 37.73% 0.01 min
> Epoch: 17 Step: 1300, train loss: 0.0024 F1: 82.78% dev loss: 0.0126 F1: 43.02% 0.01 min
> Epoch: 18 Step: 1400, train loss: 0.0021 F1: 86.51% dev loss: 0.0129 F1: 43.60% 0.01 min
> Epoch: 19 Step: 1500, train loss: 0.0018 F1: 87.16% dev loss: 0.0144 F1: 34.49% 0.01 min
> Epoch: 21 Step: 1600, train loss: 0.0016 F1: 89.18% dev loss: 0.0143 F1: 40.08% 0.01 min
> Epoch: 22 Step: 1700, train loss: 0.0018 F1: 87.62% dev loss: 0.0152 F1: 39.99% 0.01 min
> Epoch: 23 Step: 1800, train loss: 0.0016 F1: 88.04% dev loss: 0.0148 F1: 38.55% 0.01 min
> Epoch: 25 Step: 1900, train loss: 0.0015 F1: 87.24% dev loss: 0.0154 F1: 39.72% 0.01 min
> Epoch: 26 Step: 2000, train loss: 0.0015 F1: 88.33% dev loss: 0.0161 F1: 39.21% 0.01 min
> Epoch: 27 Step: 2100, train loss: 0.0013 F1: 90.97% dev loss: 0.0160 F1: 39.54% 0.01 min
> Epoch: 29 Step: 2200, train loss: 0.0011 F1: 91.46% dev loss: 0.0164 F1: 38.14% 0.01 min
> Epoch: 30 Step: 2300, train loss: 0.0010 F1: 91.80% dev loss: 0.0168 F1: 38.78% 0.01 min
> Epoch: 31 Step: 2400, train loss: 0.0010 F1: 91.74% dev loss: 0.0169 F1: 38.06% 0.01 min
> Epoch: 33 Step: 2500, train loss: 0.0008 F1: 94.22% dev loss: 0.0166 F1: 38.49% 0.01 min
> Epoch: 34 Step: 2600, train loss: 0.0009 F1: 93.53% dev loss: 0.0164 F1: 44.59% 0.01 min
> Epoch: 35 Step: 2700, train loss: 0.0009 F1: 93.05% dev loss: 0.0173 F1: 43.59% 0.01 min
> Epoch: 37 Step: 2800, train loss: 0.0008 F1: 93.47% dev loss: 0.0177 F1: 39.70% 0.01 min
> Epoch: 38 Step: 2900, train loss: 0.0009 F1: 93.79% dev loss: 0.0180 F1: 44.01% 0.01 min
> Epoch: 39 Step: 3000, train loss: 0.0008 F1: 93.26% dev loss: 0.0184 F1: 40.55% 0.01 min
> Epoch: 41 Step: 3100, train loss: 0.0006 F1: 95.97% dev loss: 0.0179 F1: 41.37% 0.01 min
> Epoch: 42 Step: 3200, train loss: 0.0006 F1: 95.60% dev loss: 0.0174 F1: 42.25% 0.01 min
> Epoch: 43 Step: 3300, train loss: 0.0006 F1: 95.80% dev loss: 0.0189 F1: 42.94% 0.01 min
> Epoch: 45 Step: 3400, train loss: 0.0005 F1: 96.68% dev loss: 0.0182 F1: 42.01% 0.01 min
> Epoch: 46 Step: 3500, train loss: 0.0005 F1: 96.19% dev loss: 0.0184 F1: 43.61% 0.01 min
> Epoch: 47 Step: 3600, train loss: 0.0005 F1: 96.60% dev loss: 0.0190 F1: 43.40% 0.01 min
> Epoch: 49 Step: 3700, train loss: 0.0004 F1: 96.25% dev loss: 0.0183 F1: 41.23% 0.01 min
> Epoch: 50 Step: 3800, train loss: 0.0003 F1: 96.90% dev loss: 0.0188 F1: 43.10% 0.01 min
> Epoch: 51 Step: 3900, train loss: 0.0003 F1: 97.99% dev loss: 0.0185 F1: 42.81% 0.01 min
> Epoch: 53 Step: 4000, train loss: 0.0003 F1: 97.52% dev loss: 0.0191 F1: 40.29% 0.01 min
> Epoch: 54 Step: 4100, train loss: 0.0003 F1: 97.55% dev loss: 0.0186 F1: 43.13% 0.01 min
> Epoch: 55 Step: 4200, train loss: 0.0002 F1: 98.62% dev loss: 0.0192 F1: 42.65% 0.01 min
> Epoch: 57 Step: 4300, train loss: 0.0003 F1: 97.20% dev loss: 0.0196 F1: 41.06% 0.01 min
> Epoch: 58 Step: 4400, train loss: 0.0002 F1: 98.84% dev loss: 0.0193 F1: 42.32% 0.01 min
> Epoch: 59 Step: 4500, train loss: 0.0002 F1: 98.15% dev loss: 0.0192 F1: 40.82% 0.01 min
> Epoch: 61 Step: 4600, train loss: 0.0002 F1: 98.81% dev loss: 0.0196 F1: 42.19% 0.01 min
> Epoch: 62 Step: 4700, train loss: 0.0002 F1: 98.81% dev loss: 0.0195 F1: 42.78% 0.01 min
> Epoch: 63 Step: 4800, train loss: 0.0002 F1: 98.93% dev loss: 0.0194 F1: 43.04% 0.01 min
> Epoch: 65 Step: 4900, train loss: 0.0002 F1: 99.17% dev loss: 0.0194 F1: 42.71% 0.01 min
> Epoch: 66 Step: 5000, train loss: 0.0002 F1: 98.71% dev loss: 0.0194 F1: 43.23% 0.01 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 17:57:59.042752>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 0.006
>>> epochs: 400
>>> step: 100
>>> model_name: glove
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 0.1
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/glove-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 5000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 0.006, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
> Epoch: 1 Step: 100, train loss: 0.0625 F1: 11.27% dev loss: 0.0197 F1: 10.10% 0.03 min
> Epoch: 2 Step: 200, train loss: 0.0197 F1: 13.66% dev loss: 0.0158 F1: 13.86% 0.01 min
> Epoch: 3 Step: 300, train loss: 0.0145 F1: 25.06% dev loss: 0.0124 F1: 27.23% 0.01 min
> Epoch: 5 Step: 400, train loss: 0.0115 F1: 31.33% dev loss: 0.0107 F1: 31.15% 0.01 min
> Epoch: 6 Step: 500, train loss: 0.0098 F1: 40.54% dev loss: 0.0106 F1: 43.17% 0.01 min
> Epoch: 7 Step: 600, train loss: 0.0082 F1: 44.02% dev loss: 0.0101 F1: 38.22% 0.01 min
> Epoch: 9 Step: 700, train loss: 0.0070 F1: 54.29% dev loss: 0.0100 F1: 41.47% 0.01 min
> Epoch: 10 Step: 800, train loss: 0.0060 F1: 62.71% dev loss: 0.0106 F1: 33.30% 0.01 min
> Epoch: 11 Step: 900, train loss: 0.0051 F1: 65.54% dev loss: 0.0102 F1: 37.00% 0.01 min
> Epoch: 13 Step: 1000, train loss: 0.0044 F1: 71.01% dev loss: 0.0117 F1: 43.82% 0.01 min
> Epoch: 14 Step: 1100, train loss: 0.0039 F1: 75.69% dev loss: 0.0110 F1: 36.22% 0.01 min
> Epoch: 15 Step: 1200, train loss: 0.0033 F1: 77.35% dev loss: 0.0117 F1: 33.68% 0.01 min
> Epoch: 17 Step: 1300, train loss: 0.0028 F1: 80.88% dev loss: 0.0117 F1: 36.80% 0.01 min
> Epoch: 18 Step: 1400, train loss: 0.0024 F1: 85.42% dev loss: 0.0123 F1: 35.66% 0.01 min
> Epoch: 19 Step: 1500, train loss: 0.0021 F1: 86.36% dev loss: 0.0129 F1: 34.35% 0.01 min
> Epoch: 21 Step: 1600, train loss: 0.0019 F1: 89.29% dev loss: 0.0126 F1: 38.04% 0.01 min
> Epoch: 22 Step: 1700, train loss: 0.0017 F1: 89.68% dev loss: 0.0137 F1: 36.96% 0.01 min
> Epoch: 23 Step: 1800, train loss: 0.0017 F1: 89.99% dev loss: 0.0128 F1: 39.57% 0.01 min
> Epoch: 25 Step: 1900, train loss: 0.0013 F1: 92.33% dev loss: 0.0132 F1: 35.92% 0.01 min
> Epoch: 26 Step: 2000, train loss: 0.0011 F1: 92.79% dev loss: 0.0139 F1: 36.96% 0.01 min
> Epoch: 27 Step: 2100, train loss: 0.0011 F1: 92.02% dev loss: 0.0140 F1: 39.18% 0.01 min
> Epoch: 29 Step: 2200, train loss: 0.0010 F1: 94.68% dev loss: 0.0137 F1: 36.88% 0.01 min
> Epoch: 30 Step: 2300, train loss: 0.0008 F1: 94.67% dev loss: 0.0141 F1: 37.63% 0.01 min
> Epoch: 31 Step: 2400, train loss: 0.0008 F1: 95.03% dev loss: 0.0155 F1: 37.33% 0.01 min
> Epoch: 33 Step: 2500, train loss: 0.0007 F1: 95.79% dev loss: 0.0145 F1: 38.88% 0.01 min
> Epoch: 34 Step: 2600, train loss: 0.0006 F1: 95.42% dev loss: 0.0157 F1: 37.75% 0.01 min
> Epoch: 35 Step: 2700, train loss: 0.0006 F1: 96.44% dev loss: 0.0159 F1: 36.83% 0.01 min
> Epoch: 37 Step: 2800, train loss: 0.0005 F1: 95.85% dev loss: 0.0153 F1: 35.72% 0.01 min
> Epoch: 38 Step: 2900, train loss: 0.0004 F1: 97.06% dev loss: 0.0159 F1: 39.58% 0.01 min
> Epoch: 39 Step: 3000, train loss: 0.0005 F1: 96.30% dev loss: 0.0164 F1: 35.95% 0.01 min
> Epoch: 41 Step: 3100, train loss: 0.0004 F1: 97.08% dev loss: 0.0159 F1: 37.37% 0.01 min
> Epoch: 42 Step: 3200, train loss: 0.0004 F1: 97.07% dev loss: 0.0162 F1: 37.12% 0.01 min
> Epoch: 43 Step: 3300, train loss: 0.0004 F1: 97.96% dev loss: 0.0161 F1: 35.82% 0.01 min
> Epoch: 45 Step: 3400, train loss: 0.0004 F1: 97.55% dev loss: 0.0164 F1: 36.68% 0.01 min
> Epoch: 46 Step: 3500, train loss: 0.0003 F1: 97.82% dev loss: 0.0162 F1: 40.00% 0.01 min
> Epoch: 47 Step: 3600, train loss: 0.0003 F1: 98.45% dev loss: 0.0165 F1: 36.25% 0.01 min
> Epoch: 49 Step: 3700, train loss: 0.0002 F1: 98.76% dev loss: 0.0166 F1: 38.38% 0.01 min
> Epoch: 50 Step: 3800, train loss: 0.0003 F1: 97.80% dev loss: 0.0164 F1: 37.68% 0.01 min
> Epoch: 51 Step: 3900, train loss: 0.0002 F1: 98.62% dev loss: 0.0170 F1: 35.27% 0.01 min
> Epoch: 53 Step: 4000, train loss: 0.0002 F1: 98.56% dev loss: 0.0173 F1: 36.22% 0.01 min
> Epoch: 54 Step: 4100, train loss: 0.0002 F1: 98.68% dev loss: 0.0169 F1: 37.90% 0.01 min
> Epoch: 55 Step: 4200, train loss: 0.0002 F1: 98.82% dev loss: 0.0172 F1: 35.29% 0.01 min
> Epoch: 57 Step: 4300, train loss: 0.0002 F1: 99.24% dev loss: 0.0174 F1: 35.34% 0.01 min
> Epoch: 58 Step: 4400, train loss: 0.0002 F1: 99.21% dev loss: 0.0172 F1: 36.60% 0.01 min
> Epoch: 59 Step: 4500, train loss: 0.0002 F1: 99.00% dev loss: 0.0170 F1: 35.32% 0.01 min
> Epoch: 61 Step: 4600, train loss: 0.0002 F1: 99.06% dev loss: 0.0174 F1: 35.41% 0.01 min
> Epoch: 62 Step: 4700, train loss: 0.0001 F1: 99.60% dev loss: 0.0174 F1: 35.72% 0.01 min
> Epoch: 63 Step: 4800, train loss: 0.0001 F1: 99.38% dev loss: 0.0174 F1: 35.63% 0.01 min
> Epoch: 65 Step: 4900, train loss: 0.0001 F1: 99.32% dev loss: 0.0173 F1: 35.66% 0.01 min
> Epoch: 66 Step: 5000, train loss: 0.0001 F1: 99.56% dev loss: 0.0172 F1: 35.61% 0.01 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 17:59:00.782208>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 0.008
>>> epochs: 400
>>> step: 100
>>> model_name: glove
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 0.1
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/glove-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 5000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 0.008, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
> Epoch: 1 Step: 100, train loss: 0.0575 F1: 11.31% dev loss: 0.0190 F1: 10.54% 0.03 min
> Epoch: 2 Step: 200, train loss: 0.0188 F1: 16.93% dev loss: 0.0148 F1: 20.30% 0.01 min
> Epoch: 3 Step: 300, train loss: 0.0133 F1: 27.64% dev loss: 0.0115 F1: 30.26% 0.01 min
> Epoch: 5 Step: 400, train loss: 0.0107 F1: 35.32% dev loss: 0.0102 F1: 33.56% 0.01 min
> Epoch: 6 Step: 500, train loss: 0.0091 F1: 43.58% dev loss: 0.0104 F1: 38.38% 0.01 min
> Epoch: 7 Step: 600, train loss: 0.0077 F1: 50.84% dev loss: 0.0105 F1: 37.36% 0.01 min
> Epoch: 9 Step: 700, train loss: 0.0063 F1: 61.25% dev loss: 0.0101 F1: 42.12% 0.01 min
> Epoch: 10 Step: 800, train loss: 0.0053 F1: 65.76% dev loss: 0.0108 F1: 35.55% 0.01 min
> Epoch: 11 Step: 900, train loss: 0.0045 F1: 69.66% dev loss: 0.0105 F1: 40.57% 0.01 min
> Epoch: 13 Step: 1000, train loss: 0.0038 F1: 75.81% dev loss: 0.0113 F1: 37.15% 0.01 min
> Epoch: 14 Step: 1100, train loss: 0.0034 F1: 78.68% dev loss: 0.0114 F1: 40.93% 0.01 min
> Epoch: 15 Step: 1200, train loss: 0.0029 F1: 81.96% dev loss: 0.0114 F1: 38.56% 0.01 min
> Epoch: 17 Step: 1300, train loss: 0.0024 F1: 85.17% dev loss: 0.0121 F1: 36.70% 0.01 min
> Epoch: 18 Step: 1400, train loss: 0.0020 F1: 86.98% dev loss: 0.0125 F1: 37.44% 0.01 min
> Epoch: 19 Step: 1500, train loss: 0.0017 F1: 88.49% dev loss: 0.0121 F1: 39.49% 0.01 min
> Epoch: 21 Step: 1600, train loss: 0.0015 F1: 90.64% dev loss: 0.0123 F1: 38.63% 0.01 min
> Epoch: 22 Step: 1700, train loss: 0.0014 F1: 91.02% dev loss: 0.0129 F1: 41.61% 0.01 min
> Epoch: 23 Step: 1800, train loss: 0.0013 F1: 91.80% dev loss: 0.0130 F1: 38.80% 0.01 min
> Epoch: 25 Step: 1900, train loss: 0.0010 F1: 93.84% dev loss: 0.0132 F1: 36.94% 0.01 min
> Epoch: 26 Step: 2000, train loss: 0.0010 F1: 92.26% dev loss: 0.0137 F1: 38.22% 0.01 min
> Epoch: 27 Step: 2100, train loss: 0.0009 F1: 94.17% dev loss: 0.0144 F1: 38.85% 0.01 min
> Epoch: 29 Step: 2200, train loss: 0.0008 F1: 94.13% dev loss: 0.0134 F1: 39.58% 0.01 min
> Epoch: 30 Step: 2300, train loss: 0.0007 F1: 95.55% dev loss: 0.0141 F1: 38.43% 0.01 min
> Epoch: 31 Step: 2400, train loss: 0.0006 F1: 95.34% dev loss: 0.0140 F1: 36.95% 0.01 min
> Epoch: 33 Step: 2500, train loss: 0.0005 F1: 95.70% dev loss: 0.0146 F1: 35.16% 0.01 min
> Epoch: 34 Step: 2600, train loss: 0.0005 F1: 96.64% dev loss: 0.0147 F1: 38.15% 0.01 min
> Epoch: 35 Step: 2700, train loss: 0.0004 F1: 96.62% dev loss: 0.0148 F1: 35.56% 0.01 min
> Epoch: 37 Step: 2800, train loss: 0.0004 F1: 97.98% dev loss: 0.0146 F1: 38.22% 0.01 min
> Epoch: 38 Step: 2900, train loss: 0.0004 F1: 97.48% dev loss: 0.0155 F1: 41.76% 0.01 min
> Epoch: 39 Step: 3000, train loss: 0.0004 F1: 97.19% dev loss: 0.0155 F1: 38.19% 0.01 min
> Epoch: 41 Step: 3100, train loss: 0.0003 F1: 97.37% dev loss: 0.0162 F1: 36.65% 0.01 min
> Epoch: 42 Step: 3200, train loss: 0.0003 F1: 97.41% dev loss: 0.0158 F1: 37.49% 0.01 min
> Epoch: 43 Step: 3300, train loss: 0.0003 F1: 98.48% dev loss: 0.0160 F1: 37.57% 0.01 min
> Epoch: 45 Step: 3400, train loss: 0.0002 F1: 97.47% dev loss: 0.0159 F1: 38.88% 0.01 min
> Epoch: 46 Step: 3500, train loss: 0.0003 F1: 98.05% dev loss: 0.0162 F1: 36.24% 0.01 min
> Epoch: 47 Step: 3600, train loss: 0.0002 F1: 98.59% dev loss: 0.0167 F1: 36.77% 0.01 min
> Epoch: 49 Step: 3700, train loss: 0.0002 F1: 98.68% dev loss: 0.0167 F1: 39.86% 0.01 min
> Epoch: 50 Step: 3800, train loss: 0.0002 F1: 98.56% dev loss: 0.0168 F1: 36.55% 0.01 min
> Epoch: 51 Step: 3900, train loss: 0.0002 F1: 98.85% dev loss: 0.0166 F1: 39.59% 0.01 min
> Epoch: 53 Step: 4000, train loss: 0.0002 F1: 99.33% dev loss: 0.0168 F1: 37.64% 0.01 min
> Epoch: 54 Step: 4100, train loss: 0.0002 F1: 99.19% dev loss: 0.0169 F1: 37.68% 0.01 min
> Epoch: 55 Step: 4200, train loss: 0.0001 F1: 99.41% dev loss: 0.0169 F1: 36.74% 0.01 min
> Epoch: 57 Step: 4300, train loss: 0.0001 F1: 99.16% dev loss: 0.0174 F1: 36.47% 0.01 min
> Epoch: 58 Step: 4400, train loss: 0.0001 F1: 99.30% dev loss: 0.0170 F1: 36.30% 0.01 min
> Epoch: 59 Step: 4500, train loss: 0.0001 F1: 99.54% dev loss: 0.0173 F1: 36.26% 0.01 min
> Epoch: 61 Step: 4600, train loss: 0.0001 F1: 99.50% dev loss: 0.0173 F1: 37.54% 0.01 min
> Epoch: 62 Step: 4700, train loss: 0.0001 F1: 99.38% dev loss: 0.0171 F1: 37.41% 0.01 min
> Epoch: 63 Step: 4800, train loss: 0.0001 F1: 99.54% dev loss: 0.0172 F1: 37.35% 0.01 min
> Epoch: 65 Step: 4900, train loss: 0.0001 F1: 99.65% dev loss: 0.0172 F1: 38.24% 0.01 min
> Epoch: 66 Step: 5000, train loss: 0.0001 F1: 99.18% dev loss: 0.0172 F1: 38.41% 0.01 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 18:00:50.650694>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 0.001
>>> epochs: 400
>>> step: 100
>>> model_name: glove
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 0.2
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/glove-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 5000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 0.001, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]
> Epoch: 1 Step: 100, train loss: 0.2219 F1: 9.43% dev loss: 0.0683 F1: 9.49% 0.03 min
> Epoch: 2 Step: 200, train loss: 0.0536 F1: 9.43% dev loss: 0.0413 F1: 9.49% 0.01 min
> Epoch: 3 Step: 300, train loss: 0.0413 F1: 11.22% dev loss: 0.0365 F1: 12.50% 0.01 min
> Epoch: 5 Step: 400, train loss: 0.0379 F1: 13.83% dev loss: 0.0335 F1: 13.60% 0.01 min
> Epoch: 6 Step: 500, train loss: 0.0348 F1: 18.63% dev loss: 0.0299 F1: 17.97% 0.01 min
> Epoch: 7 Step: 600, train loss: 0.0291 F1: 24.41% dev loss: 0.0270 F1: 22.87% 0.01 min
> Epoch: 9 Step: 700, train loss: 0.0266 F1: 26.67% dev loss: 0.0251 F1: 26.97% 0.01 min
> Epoch: 10 Step: 800, train loss: 0.0249 F1: 29.20% dev loss: 0.0233 F1: 28.18% 0.01 min
> Epoch: 11 Step: 900, train loss: 0.0219 F1: 32.18% dev loss: 0.0221 F1: 30.38% 0.01 min
> Epoch: 13 Step: 1000, train loss: 0.0207 F1: 34.20% dev loss: 0.0217 F1: 31.37% 0.01 min
> Epoch: 14 Step: 1100, train loss: 0.0199 F1: 39.75% dev loss: 0.0208 F1: 33.05% 0.01 min
> Epoch: 15 Step: 1200, train loss: 0.0181 F1: 37.35% dev loss: 0.0200 F1: 32.63% 0.01 min
> Epoch: 17 Step: 1300, train loss: 0.0172 F1: 45.00% dev loss: 0.0201 F1: 34.61% 0.01 min
> Epoch: 18 Step: 1400, train loss: 0.0165 F1: 46.84% dev loss: 0.0202 F1: 37.71% 0.01 min
> Epoch: 19 Step: 1500, train loss: 0.0153 F1: 46.83% dev loss: 0.0196 F1: 32.02% 0.01 min
> Epoch: 21 Step: 1600, train loss: 0.0150 F1: 49.27% dev loss: 0.0194 F1: 36.66% 0.01 min
> Epoch: 22 Step: 1700, train loss: 0.0143 F1: 53.19% dev loss: 0.0200 F1: 35.53% 0.01 min
> Epoch: 23 Step: 1800, train loss: 0.0136 F1: 54.19% dev loss: 0.0200 F1: 34.78% 0.01 min
> Epoch: 25 Step: 1900, train loss: 0.0128 F1: 56.82% dev loss: 0.0196 F1: 37.33% 0.01 min
> Epoch: 26 Step: 2000, train loss: 0.0125 F1: 58.48% dev loss: 0.0198 F1: 35.41% 0.01 min
> Epoch: 27 Step: 2100, train loss: 0.0118 F1: 59.56% dev loss: 0.0199 F1: 34.84% 0.01 min
> Epoch: 29 Step: 2200, train loss: 0.0113 F1: 62.02% dev loss: 0.0201 F1: 41.12% 0.01 min
> Epoch: 30 Step: 2300, train loss: 0.0112 F1: 63.49% dev loss: 0.0205 F1: 34.60% 0.01 min
> Epoch: 31 Step: 2400, train loss: 0.0106 F1: 63.42% dev loss: 0.0206 F1: 34.88% 0.01 min
> Epoch: 33 Step: 2500, train loss: 0.0103 F1: 64.68% dev loss: 0.0201 F1: 41.30% 0.01 min
> Epoch: 34 Step: 2600, train loss: 0.0100 F1: 65.82% dev loss: 0.0206 F1: 34.26% 0.01 min
> Epoch: 35 Step: 2700, train loss: 0.0093 F1: 69.69% dev loss: 0.0205 F1: 35.18% 0.01 min
> Epoch: 37 Step: 2800, train loss: 0.0091 F1: 69.06% dev loss: 0.0198 F1: 38.55% 0.01 min
> Epoch: 38 Step: 2900, train loss: 0.0089 F1: 71.75% dev loss: 0.0208 F1: 34.34% 0.01 min
> Epoch: 39 Step: 3000, train loss: 0.0085 F1: 69.37% dev loss: 0.0203 F1: 36.80% 0.01 min
> Epoch: 41 Step: 3100, train loss: 0.0082 F1: 72.75% dev loss: 0.0201 F1: 39.46% 0.01 min
> Epoch: 42 Step: 3200, train loss: 0.0079 F1: 72.55% dev loss: 0.0211 F1: 34.77% 0.01 min
> Epoch: 43 Step: 3300, train loss: 0.0077 F1: 73.89% dev loss: 0.0211 F1: 35.94% 0.01 min
> Epoch: 45 Step: 3400, train loss: 0.0076 F1: 76.03% dev loss: 0.0203 F1: 35.86% 0.01 min
> Epoch: 46 Step: 3500, train loss: 0.0074 F1: 75.01% dev loss: 0.0211 F1: 35.86% 0.01 min
> Epoch: 47 Step: 3600, train loss: 0.0070 F1: 76.56% dev loss: 0.0213 F1: 35.63% 0.01 min
> Epoch: 49 Step: 3700, train loss: 0.0069 F1: 74.65% dev loss: 0.0206 F1: 39.34% 0.01 min
> Epoch: 50 Step: 3800, train loss: 0.0069 F1: 78.09% dev loss: 0.0215 F1: 35.36% 0.01 min
> Epoch: 51 Step: 3900, train loss: 0.0066 F1: 78.43% dev loss: 0.0213 F1: 36.28% 0.01 min
> Epoch: 53 Step: 4000, train loss: 0.0063 F1: 80.62% dev loss: 0.0207 F1: 35.96% 0.01 min
> Epoch: 54 Step: 4100, train loss: 0.0065 F1: 78.73% dev loss: 0.0216 F1: 36.09% 0.01 min
> Epoch: 55 Step: 4200, train loss: 0.0064 F1: 79.17% dev loss: 0.0214 F1: 36.66% 0.01 min
> Epoch: 57 Step: 4300, train loss: 0.0061 F1: 76.19% dev loss: 0.0214 F1: 35.25% 0.01 min
> Epoch: 58 Step: 4400, train loss: 0.0061 F1: 80.28% dev loss: 0.0212 F1: 36.17% 0.01 min
> Epoch: 59 Step: 4500, train loss: 0.0059 F1: 80.52% dev loss: 0.0220 F1: 36.15% 0.01 min
> Epoch: 61 Step: 4600, train loss: 0.0060 F1: 78.21% dev loss: 0.0214 F1: 35.54% 0.01 min
> Epoch: 62 Step: 4700, train loss: 0.0060 F1: 81.56% dev loss: 0.0212 F1: 37.46% 0.01 min
> Epoch: 63 Step: 4800, train loss: 0.0059 F1: 82.01% dev loss: 0.0218 F1: 36.53% 0.01 min
> Epoch: 65 Step: 4900, train loss: 0.0058 F1: 79.75% dev loss: 0.0216 F1: 35.91% 0.01 min
> Epoch: 66 Step: 5000, train loss: 0.0059 F1: 82.29% dev loss: 0.0214 F1: 36.54% 0.01 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 18:01:48.981605>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 0.001
>>> epochs: 400
>>> step: 100
>>> model_name: glove
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 0.5
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/glove-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 5000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 0.001, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]
> Epoch: 1 Step: 100, train loss: 0.5548 F1: 9.43% dev loss: 0.1707 F1: 9.49% 0.03 min
> Epoch: 2 Step: 200, train loss: 0.1340 F1: 9.43% dev loss: 0.1033 F1: 9.49% 0.01 min
> Epoch: 3 Step: 300, train loss: 0.1033 F1: 11.22% dev loss: 0.0912 F1: 12.50% 0.01 min
> Epoch: 5 Step: 400, train loss: 0.0948 F1: 13.83% dev loss: 0.0839 F1: 13.60% 0.01 min
> Epoch: 6 Step: 500, train loss: 0.0869 F1: 18.63% dev loss: 0.0748 F1: 17.97% 0.01 min
> Epoch: 7 Step: 600, train loss: 0.0728 F1: 24.41% dev loss: 0.0675 F1: 22.87% 0.01 min
> Epoch: 9 Step: 700, train loss: 0.0666 F1: 26.67% dev loss: 0.0627 F1: 26.97% 0.01 min
> Epoch: 10 Step: 800, train loss: 0.0622 F1: 29.21% dev loss: 0.0584 F1: 28.18% 0.01 min
> Epoch: 11 Step: 900, train loss: 0.0548 F1: 32.18% dev loss: 0.0552 F1: 30.38% 0.01 min
> Epoch: 13 Step: 1000, train loss: 0.0517 F1: 34.20% dev loss: 0.0543 F1: 31.37% 0.01 min
> Epoch: 14 Step: 1100, train loss: 0.0497 F1: 39.75% dev loss: 0.0521 F1: 33.05% 0.01 min
> Epoch: 15 Step: 1200, train loss: 0.0451 F1: 37.31% dev loss: 0.0501 F1: 32.63% 0.01 min
> Epoch: 17 Step: 1300, train loss: 0.0430 F1: 45.00% dev loss: 0.0503 F1: 34.61% 0.01 min
> Epoch: 18 Step: 1400, train loss: 0.0413 F1: 46.87% dev loss: 0.0506 F1: 37.71% 0.01 min
> Epoch: 19 Step: 1500, train loss: 0.0382 F1: 46.87% dev loss: 0.0490 F1: 32.02% 0.01 min
> Epoch: 21 Step: 1600, train loss: 0.0375 F1: 49.27% dev loss: 0.0486 F1: 36.66% 0.01 min
> Epoch: 22 Step: 1700, train loss: 0.0358 F1: 53.08% dev loss: 0.0501 F1: 35.48% 0.01 min
> Epoch: 23 Step: 1800, train loss: 0.0340 F1: 54.19% dev loss: 0.0500 F1: 34.78% 0.01 min
> Epoch: 25 Step: 1900, train loss: 0.0321 F1: 56.82% dev loss: 0.0490 F1: 37.33% 0.01 min
> Epoch: 26 Step: 2000, train loss: 0.0314 F1: 58.47% dev loss: 0.0495 F1: 35.41% 0.01 min
> Epoch: 27 Step: 2100, train loss: 0.0294 F1: 59.60% dev loss: 0.0497 F1: 34.84% 0.01 min
> Epoch: 29 Step: 2200, train loss: 0.0283 F1: 62.05% dev loss: 0.0501 F1: 41.12% 0.01 min
> Epoch: 30 Step: 2300, train loss: 0.0279 F1: 63.55% dev loss: 0.0513 F1: 34.60% 0.01 min
> Epoch: 31 Step: 2400, train loss: 0.0265 F1: 63.42% dev loss: 0.0515 F1: 34.88% 0.01 min
> Epoch: 33 Step: 2500, train loss: 0.0257 F1: 64.68% dev loss: 0.0502 F1: 41.30% 0.01 min
> Epoch: 34 Step: 2600, train loss: 0.0249 F1: 65.83% dev loss: 0.0516 F1: 34.26% 0.01 min
> Epoch: 35 Step: 2700, train loss: 0.0234 F1: 69.70% dev loss: 0.0513 F1: 35.18% 0.01 min
> Epoch: 37 Step: 2800, train loss: 0.0227 F1: 69.10% dev loss: 0.0496 F1: 38.57% 0.01 min
> Epoch: 38 Step: 2900, train loss: 0.0222 F1: 71.76% dev loss: 0.0521 F1: 34.34% 0.01 min
> Epoch: 39 Step: 3000, train loss: 0.0212 F1: 69.34% dev loss: 0.0507 F1: 36.80% 0.01 min
> Epoch: 41 Step: 3100, train loss: 0.0204 F1: 72.70% dev loss: 0.0502 F1: 39.39% 0.01 min
> Epoch: 42 Step: 3200, train loss: 0.0198 F1: 72.56% dev loss: 0.0526 F1: 34.77% 0.01 min
> Epoch: 43 Step: 3300, train loss: 0.0194 F1: 73.89% dev loss: 0.0529 F1: 35.94% 0.01 min
> Epoch: 45 Step: 3400, train loss: 0.0189 F1: 76.08% dev loss: 0.0508 F1: 35.86% 0.01 min
> Epoch: 46 Step: 3500, train loss: 0.0184 F1: 75.01% dev loss: 0.0528 F1: 35.86% 0.01 min
> Epoch: 47 Step: 3600, train loss: 0.0176 F1: 76.55% dev loss: 0.0533 F1: 35.63% 0.01 min
> Epoch: 49 Step: 3700, train loss: 0.0172 F1: 75.19% dev loss: 0.0514 F1: 39.34% 0.01 min
> Epoch: 50 Step: 3800, train loss: 0.0173 F1: 78.09% dev loss: 0.0538 F1: 35.36% 0.01 min
> Epoch: 51 Step: 3900, train loss: 0.0164 F1: 78.41% dev loss: 0.0533 F1: 36.28% 0.01 min
> Epoch: 53 Step: 4000, train loss: 0.0158 F1: 80.62% dev loss: 0.0518 F1: 35.96% 0.01 min
> Epoch: 54 Step: 4100, train loss: 0.0161 F1: 78.74% dev loss: 0.0539 F1: 36.09% 0.01 min
> Epoch: 55 Step: 4200, train loss: 0.0159 F1: 79.19% dev loss: 0.0536 F1: 36.66% 0.01 min
> Epoch: 57 Step: 4300, train loss: 0.0153 F1: 76.19% dev loss: 0.0536 F1: 35.25% 0.01 min
> Epoch: 58 Step: 4400, train loss: 0.0153 F1: 80.41% dev loss: 0.0531 F1: 36.17% 0.01 min
> Epoch: 59 Step: 4500, train loss: 0.0148 F1: 80.52% dev loss: 0.0549 F1: 36.15% 0.01 min
> Epoch: 61 Step: 4600, train loss: 0.0150 F1: 78.21% dev loss: 0.0536 F1: 35.54% 0.01 min
> Epoch: 62 Step: 4700, train loss: 0.0150 F1: 81.56% dev loss: 0.0531 F1: 37.46% 0.01 min
> Epoch: 63 Step: 4800, train loss: 0.0147 F1: 82.01% dev loss: 0.0544 F1: 36.53% 0.01 min
> Epoch: 65 Step: 4900, train loss: 0.0144 F1: 79.75% dev loss: 0.0539 F1: 35.91% 0.01 min
> Epoch: 66 Step: 5000, train loss: 0.0147 F1: 82.29% dev loss: 0.0536 F1: 36.54% 0.01 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 18:05:36.684512>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 0.001
>>> epochs: 400
>>> step: 100
>>> model_name: glove
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 1.0
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/glove-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 5000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 0.001, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
> Epoch: 1 Step: 100, train loss: 1.1095 F1: 9.43% dev loss: 0.3413 F1: 9.49% 0.03 min
> Epoch: 2 Step: 200, train loss: 0.2680 F1: 9.43% dev loss: 0.2067 F1: 9.49% 0.01 min
> Epoch: 3 Step: 300, train loss: 0.2067 F1: 11.22% dev loss: 0.1824 F1: 12.50% 0.01 min
> Epoch: 5 Step: 400, train loss: 0.1896 F1: 13.83% dev loss: 0.1677 F1: 13.60% 0.01 min
> Epoch: 6 Step: 500, train loss: 0.1738 F1: 18.63% dev loss: 0.1497 F1: 17.97% 0.01 min
> Epoch: 7 Step: 600, train loss: 0.1456 F1: 24.41% dev loss: 0.1349 F1: 22.87% 0.01 min
> Epoch: 9 Step: 700, train loss: 0.1332 F1: 26.67% dev loss: 0.1254 F1: 26.97% 0.01 min
> Epoch: 10 Step: 800, train loss: 0.1245 F1: 29.21% dev loss: 0.1167 F1: 28.18% 0.01 min
> Epoch: 11 Step: 900, train loss: 0.1096 F1: 32.18% dev loss: 0.1103 F1: 30.38% 0.01 min
> Epoch: 13 Step: 1000, train loss: 0.1035 F1: 34.20% dev loss: 0.1086 F1: 31.37% 0.01 min
> Epoch: 14 Step: 1100, train loss: 0.0994 F1: 39.75% dev loss: 0.1041 F1: 33.05% 0.01 min
> Epoch: 15 Step: 1200, train loss: 0.0903 F1: 37.31% dev loss: 0.1002 F1: 32.63% 0.01 min
> Epoch: 17 Step: 1300, train loss: 0.0860 F1: 45.00% dev loss: 0.1007 F1: 34.61% 0.01 min
> Epoch: 18 Step: 1400, train loss: 0.0826 F1: 46.87% dev loss: 0.1011 F1: 37.71% 0.01 min
> Epoch: 19 Step: 1500, train loss: 0.0763 F1: 46.87% dev loss: 0.0980 F1: 32.02% 0.01 min
> Epoch: 21 Step: 1600, train loss: 0.0749 F1: 49.27% dev loss: 0.0972 F1: 36.66% 0.01 min
> Epoch: 22 Step: 1700, train loss: 0.0717 F1: 53.08% dev loss: 0.1001 F1: 35.48% 0.01 min
> Epoch: 23 Step: 1800, train loss: 0.0679 F1: 54.23% dev loss: 0.1001 F1: 34.78% 0.01 min
> Epoch: 25 Step: 1900, train loss: 0.0642 F1: 56.82% dev loss: 0.0981 F1: 37.33% 0.01 min
> Epoch: 26 Step: 2000, train loss: 0.0627 F1: 58.47% dev loss: 0.0990 F1: 35.41% 0.01 min
> Epoch: 27 Step: 2100, train loss: 0.0589 F1: 59.61% dev loss: 0.0994 F1: 34.84% 0.01 min
> Epoch: 29 Step: 2200, train loss: 0.0567 F1: 62.12% dev loss: 0.1003 F1: 41.12% 0.01 min
> Epoch: 30 Step: 2300, train loss: 0.0559 F1: 63.55% dev loss: 0.1027 F1: 34.60% 0.01 min
> Epoch: 31 Step: 2400, train loss: 0.0530 F1: 63.41% dev loss: 0.1029 F1: 34.88% 0.01 min
> Epoch: 33 Step: 2500, train loss: 0.0513 F1: 64.68% dev loss: 0.1005 F1: 41.30% 0.01 min
> Epoch: 34 Step: 2600, train loss: 0.0498 F1: 65.83% dev loss: 0.1031 F1: 34.26% 0.01 min
> Epoch: 35 Step: 2700, train loss: 0.0467 F1: 69.70% dev loss: 0.1027 F1: 35.18% 0.01 min
> Epoch: 37 Step: 2800, train loss: 0.0454 F1: 69.10% dev loss: 0.0992 F1: 38.57% 0.01 min
> Epoch: 38 Step: 2900, train loss: 0.0445 F1: 71.76% dev loss: 0.1041 F1: 34.34% 0.01 min
> Epoch: 39 Step: 3000, train loss: 0.0424 F1: 69.33% dev loss: 0.1015 F1: 36.80% 0.01 min
> Epoch: 41 Step: 3100, train loss: 0.0408 F1: 72.70% dev loss: 0.1003 F1: 39.39% 0.01 min
> Epoch: 42 Step: 3200, train loss: 0.0397 F1: 72.73% dev loss: 0.1053 F1: 34.77% 0.01 min
> Epoch: 43 Step: 3300, train loss: 0.0387 F1: 73.89% dev loss: 0.1057 F1: 35.94% 0.01 min
> Epoch: 45 Step: 3400, train loss: 0.0378 F1: 76.08% dev loss: 0.1016 F1: 35.86% 0.01 min
> Epoch: 46 Step: 3500, train loss: 0.0368 F1: 75.01% dev loss: 0.1056 F1: 35.86% 0.01 min
> Epoch: 47 Step: 3600, train loss: 0.0352 F1: 76.55% dev loss: 0.1067 F1: 35.63% 0.01 min
> Epoch: 49 Step: 3700, train loss: 0.0343 F1: 75.19% dev loss: 0.1029 F1: 39.34% 0.01 min
> Epoch: 50 Step: 3800, train loss: 0.0346 F1: 78.09% dev loss: 0.1077 F1: 35.36% 0.01 min
> Epoch: 51 Step: 3900, train loss: 0.0328 F1: 78.43% dev loss: 0.1067 F1: 36.28% 0.01 min
> Epoch: 53 Step: 4000, train loss: 0.0315 F1: 80.62% dev loss: 0.1037 F1: 35.96% 0.01 min
> Epoch: 54 Step: 4100, train loss: 0.0323 F1: 78.74% dev loss: 0.1079 F1: 36.09% 0.01 min
> Epoch: 55 Step: 4200, train loss: 0.0319 F1: 79.23% dev loss: 0.1073 F1: 36.66% 0.01 min
> Epoch: 57 Step: 4300, train loss: 0.0306 F1: 76.19% dev loss: 0.1071 F1: 35.25% 0.01 min
> Epoch: 58 Step: 4400, train loss: 0.0306 F1: 80.41% dev loss: 0.1062 F1: 36.17% 0.01 min
> Epoch: 59 Step: 4500, train loss: 0.0296 F1: 80.52% dev loss: 0.1098 F1: 36.15% 0.01 min
> Epoch: 61 Step: 4600, train loss: 0.0299 F1: 78.21% dev loss: 0.1072 F1: 35.54% 0.01 min
> Epoch: 62 Step: 4700, train loss: 0.0300 F1: 81.56% dev loss: 0.1062 F1: 37.46% 0.01 min
> Epoch: 63 Step: 4800, train loss: 0.0295 F1: 82.01% dev loss: 0.1088 F1: 36.53% 0.01 min
> Epoch: 65 Step: 4900, train loss: 0.0288 F1: 79.79% dev loss: 0.1078 F1: 35.88% 0.01 min
> Epoch: 66 Step: 5000, train loss: 0.0293 F1: 82.29% dev loss: 0.1072 F1: 36.54% 0.01 min
>>>>>>>>>>>>>>>>>>>>>2021-07-21 18:18:18.590879>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 0.001
>>> epochs: 400
>>> step: 100
>>> model_name: glove
>>> downstream: crf
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 1.0
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/glove-crf_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 5000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 0.001, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
> Epoch: 1 Step: 100, train loss: 31.7922 F1: 11.53% dev loss: 28.1500 F1: 10.22% 0.23 min
> Epoch: 2 Step: 200, train loss: 20.5265 F1: 14.83% dev loss: 14.4398 F1: 14.25% 0.22 min
> Epoch: 3 Step: 300, train loss: 10.7455 F1: 13.76% dev loss: 8.5532 F1: 10.90% 0.22 min
> Epoch: 5 Step: 400, train loss: 7.6995 F1: 11.52% dev loss: 6.8795 F1: 11.51% 0.22 min
> Epoch: 6 Step: 500, train loss: 6.6583 F1: 12.55% dev loss: 6.1533 F1: 15.64% 0.22 min
> Epoch: 7 Step: 600, train loss: 5.9672 F1: 14.22% dev loss: 5.7514 F1: 13.88% 0.22 min
> Epoch: 9 Step: 700, train loss: 5.7195 F1: 16.83% dev loss: 5.4916 F1: 16.55% 0.22 min
> Epoch: 10 Step: 800, train loss: 5.5359 F1: 17.16% dev loss: 5.3011 F1: 17.71% 0.22 min
> Epoch: 11 Step: 900, train loss: 5.2055 F1: 20.06% dev loss: 5.1458 F1: 19.22% 0.22 min
> Epoch: 13 Step: 1000, train loss: 5.1256 F1: 18.35% dev loss: 5.0160 F1: 16.57% 0.22 min
> Epoch: 14 Step: 1100, train loss: 5.0311 F1: 20.19% dev loss: 4.9052 F1: 17.23% 0.22 min
> Epoch: 15 Step: 1200, train loss: 4.8009 F1: 20.20% dev loss: 4.8100 F1: 17.65% 0.22 min
> Epoch: 17 Step: 1300, train loss: 4.7457 F1: 21.95% dev loss: 4.7209 F1: 22.44% 0.22 min
> Epoch: 18 Step: 1400, train loss: 4.7137 F1: 21.37% dev loss: 4.6443 F1: 22.75% 0.22 min
> Epoch: 19 Step: 1500, train loss: 4.5098 F1: 22.58% dev loss: 4.5746 F1: 21.66% 0.22 min
> Epoch: 21 Step: 1600, train loss: 4.4877 F1: 22.72% dev loss: 4.5110 F1: 22.04% 0.22 min
> Epoch: 22 Step: 1700, train loss: 4.4820 F1: 24.56% dev loss: 4.4538 F1: 22.54% 0.22 min
> Epoch: 23 Step: 1800, train loss: 4.3072 F1: 23.94% dev loss: 4.4003 F1: 25.05% 0.22 min
> Epoch: 25 Step: 1900, train loss: 4.2806 F1: 25.17% dev loss: 4.3495 F1: 25.33% 0.22 min
> Epoch: 26 Step: 2000, train loss: 4.2935 F1: 25.38% dev loss: 4.3053 F1: 26.67% 0.22 min
> Epoch: 27 Step: 2100, train loss: 4.1278 F1: 25.93% dev loss: 4.2636 F1: 26.69% 0.22 min
> Epoch: 29 Step: 2200, train loss: 4.1122 F1: 25.23% dev loss: 4.2225 F1: 26.94% 0.22 min
> Epoch: 30 Step: 2300, train loss: 4.1421 F1: 24.91% dev loss: 4.1869 F1: 27.40% 0.22 min
> Epoch: 31 Step: 2400, train loss: 4.0073 F1: 26.85% dev loss: 4.1523 F1: 28.84% 0.22 min
> Epoch: 33 Step: 2500, train loss: 4.0014 F1: 26.39% dev loss: 4.1199 F1: 27.90% 0.22 min
> Epoch: 34 Step: 2600, train loss: 4.0117 F1: 26.31% dev loss: 4.0913 F1: 27.38% 0.22 min
> Epoch: 35 Step: 2700, train loss: 3.8952 F1: 25.00% dev loss: 4.0624 F1: 28.59% 0.22 min
> Epoch: 37 Step: 2800, train loss: 3.8830 F1: 25.67% dev loss: 4.0355 F1: 27.23% 0.22 min
> Epoch: 38 Step: 2900, train loss: 3.9254 F1: 26.01% dev loss: 4.0123 F1: 27.23% 0.22 min
> Epoch: 39 Step: 3000, train loss: 3.8043 F1: 28.50% dev loss: 3.9897 F1: 28.19% 0.22 min
> Epoch: 41 Step: 3100, train loss: 3.7994 F1: 27.14% dev loss: 3.9675 F1: 27.23% 0.22 min
> Epoch: 42 Step: 3200, train loss: 3.8478 F1: 26.33% dev loss: 3.9485 F1: 26.90% 0.22 min
> Epoch: 43 Step: 3300, train loss: 3.7460 F1: 29.74% dev loss: 3.9288 F1: 28.60% 0.22 min
> Epoch: 45 Step: 3400, train loss: 3.7325 F1: 27.07% dev loss: 3.9117 F1: 27.17% 0.22 min
> Epoch: 46 Step: 3500, train loss: 3.7839 F1: 27.90% dev loss: 3.8965 F1: 28.21% 0.22 min
> Epoch: 47 Step: 3600, train loss: 3.7142 F1: 26.27% dev loss: 3.8803 F1: 28.17% 0.22 min
> Epoch: 49 Step: 3700, train loss: 3.6805 F1: 27.96% dev loss: 3.8657 F1: 28.43% 0.23 min
> Epoch: 50 Step: 3800, train loss: 3.7276 F1: 26.07% dev loss: 3.8546 F1: 28.02% 0.22 min
> Epoch: 51 Step: 3900, train loss: 3.6488 F1: 27.50% dev loss: 3.8419 F1: 28.02% 0.22 min
> Epoch: 53 Step: 4000, train loss: 3.6412 F1: 30.31% dev loss: 3.8311 F1: 28.10% 0.22 min
> Epoch: 54 Step: 4100, train loss: 3.6722 F1: 26.75% dev loss: 3.8229 F1: 28.10% 0.22 min
> Epoch: 55 Step: 4200, train loss: 3.6017 F1: 27.41% dev loss: 3.8139 F1: 28.08% 0.22 min
> Epoch: 57 Step: 4300, train loss: 3.5914 F1: 28.93% dev loss: 3.8057 F1: 28.08% 0.22 min
> Epoch: 58 Step: 4400, train loss: 3.6708 F1: 27.81% dev loss: 3.7995 F1: 28.08% 0.22 min
> Epoch: 59 Step: 4500, train loss: 3.5666 F1: 28.89% dev loss: 3.7930 F1: 28.08% 0.22 min
> Epoch: 61 Step: 4600, train loss: 3.5905 F1: 27.37% dev loss: 3.7883 F1: 28.08% 0.22 min
> Epoch: 62 Step: 4700, train loss: 3.6437 F1: 27.44% dev loss: 3.7851 F1: 28.08% 0.22 min
> Epoch: 63 Step: 4800, train loss: 3.5647 F1: 28.59% dev loss: 3.7823 F1: 28.08% 0.22 min
> Epoch: 65 Step: 4900, train loss: 3.5599 F1: 28.51% dev loss: 3.7805 F1: 28.08% 0.22 min
> Epoch: 66 Step: 5000, train loss: 3.6431 F1: 26.90% dev loss: 3.7799 F1: 28.08% 0.22 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 18:29:51.074352>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 400
>>> step: 100
>>> model_name: glove
>>> downstream: crf
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 1.0
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/glove-crf_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 5000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
> Epoch: 1 Step: 100, train loss: 33.9462 F1: 10.23% dev loss: 34.8443 F1: 0.84% 0.23 min
> Epoch: 2 Step: 200, train loss: 33.0828 F1: 10.22% dev loss: 33.7364 F1: 0.84% 0.22 min
> Epoch: 3 Step: 300, train loss: 31.8267 F1: 10.22% dev loss: 31.9243 F1: 10.21% 0.22 min
> Epoch: 5 Step: 400, train loss: 29.8107 F1: 10.24% dev loss: 29.4898 F1: 10.25% 0.22 min
> Epoch: 6 Step: 500, train loss: 27.1277 F1: 12.32% dev loss: 26.5749 F1: 10.64% 0.22 min
> Epoch: 7 Step: 600, train loss: 24.3294 F1: 13.28% dev loss: 23.6486 F1: 15.66% 0.22 min
> Epoch: 9 Step: 700, train loss: 21.7640 F1: 13.14% dev loss: 21.1128 F1: 13.29% 0.22 min
> Epoch: 10 Step: 800, train loss: 19.4574 F1: 13.07% dev loss: 18.9597 F1: 13.20% 0.22 min
> Epoch: 11 Step: 900, train loss: 17.5654 F1: 13.77% dev loss: 17.1386 F1: 14.13% 0.22 min
> Epoch: 13 Step: 1000, train loss: 16.0523 F1: 12.59% dev loss: 15.6223 F1: 13.55% 0.22 min
> Epoch: 14 Step: 1100, train loss: 14.6938 F1: 14.38% dev loss: 14.3685 F1: 14.27% 0.22 min
> Epoch: 15 Step: 1200, train loss: 13.5554 F1: 13.60% dev loss: 13.3253 F1: 12.96% 0.22 min
> Epoch: 17 Step: 1300, train loss: 12.7390 F1: 12.66% dev loss: 12.4586 F1: 12.20% 0.22 min
> Epoch: 18 Step: 1400, train loss: 11.9686 F1: 13.62% dev loss: 11.7382 F1: 12.34% 0.22 min
> Epoch: 19 Step: 1500, train loss: 11.2575 F1: 13.62% dev loss: 11.1304 F1: 10.86% 0.22 min
> Epoch: 21 Step: 1600, train loss: 10.8188 F1: 14.39% dev loss: 10.6162 F1: 10.95% 0.22 min
> Epoch: 22 Step: 1700, train loss: 10.3755 F1: 12.13% dev loss: 10.1797 F1: 10.87% 0.22 min
> Epoch: 23 Step: 1800, train loss: 9.8955 F1: 11.73% dev loss: 9.8039 F1: 10.84% 0.22 min
> Epoch: 25 Step: 1900, train loss: 9.6591 F1: 11.19% dev loss: 9.4791 F1: 11.02% 0.22 min
> Epoch: 26 Step: 2000, train loss: 9.3818 F1: 11.99% dev loss: 9.1975 F1: 11.01% 0.22 min
> Epoch: 27 Step: 2100, train loss: 9.0166 F1: 14.25% dev loss: 8.9499 F1: 11.06% 0.22 min
> Epoch: 29 Step: 2200, train loss: 8.8967 F1: 14.32% dev loss: 8.7315 F1: 11.09% 0.22 min
> Epoch: 30 Step: 2300, train loss: 8.7100 F1: 11.85% dev loss: 8.5393 F1: 11.00% 0.22 min
> Epoch: 31 Step: 2400, train loss: 8.4219 F1: 12.68% dev loss: 8.3673 F1: 11.04% 0.22 min
> Epoch: 33 Step: 2500, train loss: 8.3698 F1: 14.32% dev loss: 8.2136 F1: 11.04% 0.22 min
> Epoch: 34 Step: 2600, train loss: 8.2474 F1: 11.36% dev loss: 8.0763 F1: 11.04% 0.22 min
> Epoch: 35 Step: 2700, train loss: 7.9926 F1: 13.98% dev loss: 7.9523 F1: 11.04% 0.22 min
> Epoch: 37 Step: 2800, train loss: 8.0032 F1: 12.44% dev loss: 7.8399 F1: 11.04% 0.22 min
> Epoch: 38 Step: 2900, train loss: 7.9136 F1: 11.65% dev loss: 7.7386 F1: 10.82% 0.22 min
> Epoch: 39 Step: 3000, train loss: 7.6981 F1: 12.92% dev loss: 7.6465 F1: 10.82% 0.22 min
> Epoch: 41 Step: 3100, train loss: 7.7100 F1: 11.64% dev loss: 7.5623 F1: 10.91% 0.22 min
> Epoch: 42 Step: 3200, train loss: 7.6666 F1: 14.90% dev loss: 7.4863 F1: 11.07% 0.22 min
> Epoch: 43 Step: 3300, train loss: 7.4690 F1: 11.56% dev loss: 7.4166 F1: 11.07% 0.22 min
> Epoch: 45 Step: 3400, train loss: 7.5133 F1: 11.46% dev loss: 7.3529 F1: 11.04% 0.22 min
> Epoch: 46 Step: 3500, train loss: 7.4728 F1: 11.73% dev loss: 7.2952 F1: 11.04% 0.22 min
> Epoch: 47 Step: 3600, train loss: 7.3022 F1: 11.69% dev loss: 7.2423 F1: 11.07% 0.22 min
> Epoch: 49 Step: 3700, train loss: 7.3467 F1: 11.72% dev loss: 7.1940 F1: 11.07% 0.22 min
> Epoch: 50 Step: 3800, train loss: 7.3285 F1: 11.66% dev loss: 7.1507 F1: 11.07% 0.22 min
> Epoch: 51 Step: 3900, train loss: 7.1584 F1: 11.66% dev loss: 7.1112 F1: 11.04% 0.22 min
> Epoch: 53 Step: 4000, train loss: 7.2263 F1: 11.66% dev loss: 7.0754 F1: 11.05% 0.22 min
> Epoch: 54 Step: 4100, train loss: 7.2074 F1: 11.54% dev loss: 7.0437 F1: 11.05% 0.22 min
> Epoch: 55 Step: 4200, train loss: 7.0540 F1: 11.65% dev loss: 7.0153 F1: 11.05% 0.22 min
> Epoch: 57 Step: 4300, train loss: 7.1399 F1: 11.82% dev loss: 6.9903 F1: 11.05% 0.22 min
> Epoch: 58 Step: 4400, train loss: 7.1421 F1: 11.59% dev loss: 6.9688 F1: 11.04% 0.22 min
> Epoch: 59 Step: 4500, train loss: 6.9855 F1: 11.51% dev loss: 6.9504 F1: 11.04% 0.22 min
> Epoch: 61 Step: 4600, train loss: 7.0831 F1: 11.81% dev loss: 6.9353 F1: 11.04% 0.22 min
> Epoch: 62 Step: 4700, train loss: 7.0920 F1: 11.59% dev loss: 6.9235 F1: 11.04% 0.22 min
> Epoch: 63 Step: 4800, train loss: 6.9501 F1: 11.75% dev loss: 6.9149 F1: 11.04% 0.22 min
> Epoch: 65 Step: 4900, train loss: 7.0405 F1: 12.24% dev loss: 6.9096 F1: 11.04% 0.22 min
> Epoch: 66 Step: 5000, train loss: 7.0762 F1: 11.76% dev loss: 6.9079 F1: 11.04% 0.22 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 18:48:50.778337>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 0.0005
>>> epochs: 400
>>> step: 100
>>> model_name: glove
>>> downstream: crf
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 1.0
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/glove-crf_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 5000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 0.0005, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
> Epoch: 1 Step: 100, train loss: 32.9085 F1: 10.02% dev loss: 31.5725 F1: 10.22% 0.23 min
> Epoch: 2 Step: 200, train loss: 26.3337 F1: 12.59% dev loss: 22.2521 F1: 13.36% 0.22 min
> Epoch: 3 Step: 300, train loss: 17.0400 F1: 12.88% dev loss: 13.3212 F1: 14.37% 0.22 min
> Epoch: 5 Step: 400, train loss: 10.8821 F1: 11.85% dev loss: 9.1395 F1: 11.01% 0.22 min
> Epoch: 6 Step: 500, train loss: 8.3187 F1: 11.41% dev loss: 7.4848 F1: 10.99% 0.22 min
> Epoch: 7 Step: 600, train loss: 7.0933 F1: 12.99% dev loss: 6.7277 F1: 11.64% 0.22 min
> Epoch: 9 Step: 700, train loss: 6.6393 F1: 14.31% dev loss: 6.3130 F1: 12.11% 0.22 min
> Epoch: 10 Step: 800, train loss: 6.3441 F1: 13.29% dev loss: 6.0437 F1: 15.74% 0.22 min
> Epoch: 11 Step: 900, train loss: 5.9689 F1: 14.83% dev loss: 5.8434 F1: 13.75% 0.22 min
> Epoch: 13 Step: 1000, train loss: 5.8761 F1: 15.80% dev loss: 5.6859 F1: 14.06% 0.22 min
> Epoch: 14 Step: 1100, train loss: 5.7597 F1: 14.64% dev loss: 5.5569 F1: 14.66% 0.22 min
> Epoch: 15 Step: 1200, train loss: 5.5083 F1: 16.00% dev loss: 5.4471 F1: 16.30% 0.22 min
> Epoch: 17 Step: 1300, train loss: 5.4732 F1: 17.15% dev loss: 5.3494 F1: 16.78% 0.22 min
> Epoch: 18 Step: 1400, train loss: 5.4307 F1: 17.33% dev loss: 5.2640 F1: 17.77% 0.22 min
> Epoch: 19 Step: 1500, train loss: 5.2046 F1: 17.07% dev loss: 5.1871 F1: 17.78% 0.22 min
> Epoch: 21 Step: 1600, train loss: 5.2002 F1: 19.00% dev loss: 5.1175 F1: 19.31% 0.22 min
> Epoch: 22 Step: 1700, train loss: 5.1928 F1: 18.70% dev loss: 5.0545 F1: 20.00% 0.22 min
> Epoch: 23 Step: 1800, train loss: 4.9938 F1: 17.54% dev loss: 4.9962 F1: 20.02% 0.22 min
> Epoch: 25 Step: 1900, train loss: 4.9922 F1: 20.04% dev loss: 4.9418 F1: 16.55% 0.22 min
> Epoch: 26 Step: 2000, train loss: 4.9977 F1: 22.43% dev loss: 4.8925 F1: 17.02% 0.22 min
> Epoch: 27 Step: 2100, train loss: 4.8030 F1: 19.65% dev loss: 4.8464 F1: 17.43% 0.22 min
> Epoch: 29 Step: 2200, train loss: 4.8118 F1: 23.37% dev loss: 4.8021 F1: 21.54% 0.22 min
> Epoch: 30 Step: 2300, train loss: 4.8346 F1: 19.66% dev loss: 4.7621 F1: 21.92% 0.22 min
> Epoch: 31 Step: 2400, train loss: 4.6678 F1: 20.37% dev loss: 4.7246 F1: 22.17% 0.22 min
> Epoch: 33 Step: 2500, train loss: 4.6837 F1: 23.54% dev loss: 4.6889 F1: 22.36% 0.22 min
> Epoch: 34 Step: 2600, train loss: 4.6954 F1: 22.42% dev loss: 4.6561 F1: 22.48% 0.22 min
> Epoch: 35 Step: 2700, train loss: 4.5402 F1: 21.29% dev loss: 4.6250 F1: 22.60% 0.22 min
> Epoch: 37 Step: 2800, train loss: 4.5632 F1: 22.20% dev loss: 4.5953 F1: 22.56% 0.23 min
> Epoch: 38 Step: 2900, train loss: 4.5946 F1: 22.77% dev loss: 4.5683 F1: 21.44% 0.22 min
> Epoch: 39 Step: 3000, train loss: 4.4424 F1: 23.07% dev loss: 4.5427 F1: 21.91% 0.22 min
> Epoch: 41 Step: 3100, train loss: 4.4648 F1: 24.01% dev loss: 4.5179 F1: 22.03% 0.22 min
> Epoch: 42 Step: 3200, train loss: 4.5096 F1: 22.18% dev loss: 4.4955 F1: 22.00% 0.22 min
> Epoch: 43 Step: 3300, train loss: 4.3700 F1: 23.12% dev loss: 4.4740 F1: 22.09% 0.22 min
> Epoch: 45 Step: 3400, train loss: 4.3880 F1: 24.87% dev loss: 4.4541 F1: 22.09% 0.22 min
> Epoch: 46 Step: 3500, train loss: 4.4340 F1: 24.30% dev loss: 4.4359 F1: 25.12% 0.23 min
> Epoch: 47 Step: 3600, train loss: 4.3263 F1: 22.78% dev loss: 4.4182 F1: 25.12% 0.23 min
> Epoch: 49 Step: 3700, train loss: 4.3237 F1: 24.32% dev loss: 4.4019 F1: 25.53% 0.23 min
> Epoch: 50 Step: 3800, train loss: 4.3717 F1: 24.30% dev loss: 4.3877 F1: 26.69% 0.24 min
> Epoch: 51 Step: 3900, train loss: 4.2538 F1: 24.52% dev loss: 4.3740 F1: 26.69% 0.23 min
> Epoch: 53 Step: 4000, train loss: 4.2782 F1: 24.64% dev loss: 4.3613 F1: 26.69% 0.23 min
> Epoch: 54 Step: 4100, train loss: 4.3087 F1: 24.22% dev loss: 4.3506 F1: 26.46% 0.23 min
> Epoch: 55 Step: 4200, train loss: 4.2017 F1: 23.65% dev loss: 4.3405 F1: 26.67% 0.22 min
> Epoch: 57 Step: 4300, train loss: 4.2252 F1: 23.98% dev loss: 4.3313 F1: 26.67% 0.23 min
> Epoch: 58 Step: 4400, train loss: 4.3012 F1: 24.81% dev loss: 4.3236 F1: 26.86% 0.23 min
> Epoch: 59 Step: 4500, train loss: 4.1628 F1: 23.38% dev loss: 4.3166 F1: 26.86% 0.23 min
> Epoch: 61 Step: 4600, train loss: 4.2135 F1: 23.77% dev loss: 4.3110 F1: 26.86% 0.23 min
> Epoch: 62 Step: 4700, train loss: 4.2715 F1: 23.82% dev loss: 4.3068 F1: 26.86% 0.23 min
> Epoch: 63 Step: 4800, train loss: 4.1592 F1: 27.04% dev loss: 4.3037 F1: 26.86% 0.23 min
> Epoch: 65 Step: 4900, train loss: 4.1846 F1: 24.76% dev loss: 4.3016 F1: 26.86% 0.23 min
> Epoch: 66 Step: 5000, train loss: 4.2676 F1: 24.31% dev loss: 4.3010 F1: 26.86% 0.23 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 19:10:42.607760>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 0.0005
>>> epochs: 400
>>> step: 100
>>> model_name: glove
>>> downstream: crf
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 0.1
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/glove-crf_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 5000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 0.0005, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
> Epoch: 1 Step: 100, train loss: 32.9085 F1: 10.02% dev loss: 31.5725 F1: 10.22% 0.23 min
> Epoch: 2 Step: 200, train loss: 26.3337 F1: 12.59% dev loss: 22.2521 F1: 13.36% 0.24 min
> Epoch: 3 Step: 300, train loss: 17.0400 F1: 12.88% dev loss: 13.3212 F1: 14.37% 0.23 min
> Epoch: 5 Step: 400, train loss: 10.8821 F1: 11.85% dev loss: 9.1395 F1: 11.01% 0.24 min
> Epoch: 6 Step: 500, train loss: 8.3187 F1: 11.41% dev loss: 7.4848 F1: 10.99% 0.23 min
> Epoch: 7 Step: 600, train loss: 7.0933 F1: 12.99% dev loss: 6.7277 F1: 11.64% 0.23 min
> Epoch: 9 Step: 700, train loss: 6.6393 F1: 14.31% dev loss: 6.3130 F1: 12.11% 0.23 min
> Epoch: 10 Step: 800, train loss: 6.3441 F1: 13.29% dev loss: 6.0437 F1: 15.74% 0.23 min
> Epoch: 11 Step: 900, train loss: 5.9689 F1: 14.83% dev loss: 5.8434 F1: 13.75% 0.24 min
> Epoch: 13 Step: 1000, train loss: 5.8761 F1: 15.80% dev loss: 5.6859 F1: 14.06% 0.23 min
> Epoch: 14 Step: 1100, train loss: 5.7597 F1: 14.64% dev loss: 5.5569 F1: 14.66% 0.23 min
> Epoch: 15 Step: 1200, train loss: 5.5083 F1: 16.00% dev loss: 5.4471 F1: 16.30% 0.23 min
> Epoch: 17 Step: 1300, train loss: 5.4732 F1: 17.15% dev loss: 5.3494 F1: 16.78% 0.23 min
> Epoch: 18 Step: 1400, train loss: 5.4307 F1: 17.33% dev loss: 5.2640 F1: 17.77% 0.23 min
> Epoch: 19 Step: 1500, train loss: 5.2046 F1: 17.07% dev loss: 5.1871 F1: 17.78% 0.23 min
> Epoch: 21 Step: 1600, train loss: 5.2002 F1: 19.00% dev loss: 5.1175 F1: 19.31% 0.23 min
> Epoch: 22 Step: 1700, train loss: 5.1928 F1: 18.70% dev loss: 5.0545 F1: 20.00% 0.22 min
> Epoch: 23 Step: 1800, train loss: 4.9938 F1: 17.54% dev loss: 4.9962 F1: 20.02% 0.23 min
> Epoch: 25 Step: 1900, train loss: 4.9922 F1: 20.04% dev loss: 4.9418 F1: 16.55% 0.23 min
> Epoch: 26 Step: 2000, train loss: 4.9977 F1: 22.43% dev loss: 4.8925 F1: 17.02% 0.23 min
> Epoch: 27 Step: 2100, train loss: 4.8030 F1: 19.65% dev loss: 4.8464 F1: 17.43% 0.23 min
> Epoch: 29 Step: 2200, train loss: 4.8118 F1: 23.37% dev loss: 4.8021 F1: 21.54% 0.23 min
> Epoch: 30 Step: 2300, train loss: 4.8346 F1: 19.66% dev loss: 4.7621 F1: 21.92% 0.24 min
> Epoch: 31 Step: 2400, train loss: 4.6678 F1: 20.37% dev loss: 4.7246 F1: 22.17% 0.23 min
> Epoch: 33 Step: 2500, train loss: 4.6837 F1: 23.54% dev loss: 4.6889 F1: 22.36% 0.23 min
> Epoch: 34 Step: 2600, train loss: 4.6954 F1: 22.42% dev loss: 4.6561 F1: 22.48% 0.23 min
> Epoch: 35 Step: 2700, train loss: 4.5402 F1: 21.29% dev loss: 4.6250 F1: 22.60% 0.23 min
> Epoch: 37 Step: 2800, train loss: 4.5632 F1: 22.20% dev loss: 4.5953 F1: 22.56% 0.23 min
> Epoch: 38 Step: 2900, train loss: 4.5946 F1: 22.77% dev loss: 4.5683 F1: 21.44% 0.23 min
> Epoch: 39 Step: 3000, train loss: 4.4424 F1: 23.07% dev loss: 4.5427 F1: 21.91% 0.23 min
> Epoch: 41 Step: 3100, train loss: 4.4648 F1: 24.01% dev loss: 4.5179 F1: 22.03% 0.23 min
> Epoch: 42 Step: 3200, train loss: 4.5096 F1: 22.18% dev loss: 4.4955 F1: 22.00% 0.23 min
> Epoch: 43 Step: 3300, train loss: 4.3700 F1: 23.12% dev loss: 4.4740 F1: 22.09% 0.23 min
> Epoch: 45 Step: 3400, train loss: 4.3880 F1: 24.87% dev loss: 4.4541 F1: 22.09% 0.23 min
> Epoch: 46 Step: 3500, train loss: 4.4340 F1: 24.30% dev loss: 4.4359 F1: 25.12% 0.22 min
> Epoch: 47 Step: 3600, train loss: 4.3263 F1: 22.78% dev loss: 4.4182 F1: 25.12% 0.23 min
> Epoch: 49 Step: 3700, train loss: 4.3237 F1: 24.32% dev loss: 4.4019 F1: 25.53% 0.22 min
> Epoch: 50 Step: 3800, train loss: 4.3717 F1: 24.30% dev loss: 4.3877 F1: 26.69% 0.22 min
> Epoch: 51 Step: 3900, train loss: 4.2538 F1: 24.52% dev loss: 4.3740 F1: 26.69% 0.22 min
> Epoch: 53 Step: 4000, train loss: 4.2782 F1: 24.64% dev loss: 4.3613 F1: 26.69% 0.22 min
> Epoch: 54 Step: 4100, train loss: 4.3087 F1: 24.22% dev loss: 4.3506 F1: 26.46% 0.22 min
> Epoch: 55 Step: 4200, train loss: 4.2017 F1: 23.65% dev loss: 4.3405 F1: 26.67% 0.22 min
> Epoch: 57 Step: 4300, train loss: 4.2252 F1: 23.98% dev loss: 4.3313 F1: 26.67% 0.22 min
> Epoch: 58 Step: 4400, train loss: 4.3012 F1: 24.81% dev loss: 4.3236 F1: 26.86% 0.22 min
> Epoch: 59 Step: 4500, train loss: 4.1628 F1: 23.38% dev loss: 4.3166 F1: 26.86% 0.22 min
> Epoch: 61 Step: 4600, train loss: 4.2135 F1: 23.77% dev loss: 4.3110 F1: 26.86% 0.23 min
> Epoch: 62 Step: 4700, train loss: 4.2715 F1: 23.82% dev loss: 4.3068 F1: 26.86% 0.23 min
> Epoch: 63 Step: 4800, train loss: 4.1592 F1: 27.04% dev loss: 4.3037 F1: 26.86% 0.23 min
> Epoch: 65 Step: 4900, train loss: 4.1846 F1: 24.76% dev loss: 4.3016 F1: 26.86% 0.22 min
> Epoch: 66 Step: 5000, train loss: 4.2676 F1: 24.31% dev loss: 4.3010 F1: 26.86% 0.24 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 20:07:48.218479>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 0.0005
>>> epochs: 400
>>> step: 100
>>> model_name: glove
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 0.1
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/glove-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 5000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 0.0005, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
> Epoch: 1 Step: 100, train loss: 0.1410 F1: 12.14% dev loss: 0.1234 F1: 13.52% 0.01 min
> Epoch: 2 Step: 200, train loss: 0.0981 F1: 13.72% dev loss: 0.0702 F1: 12.37% 0.00 min
> Epoch: 3 Step: 300, train loss: 0.0540 F1: 13.01% dev loss: 0.0402 F1: 11.88% 0.00 min
> Epoch: 5 Step: 400, train loss: 0.0363 F1: 14.67% dev loss: 0.0300 F1: 12.61% 0.00 min
> Epoch: 6 Step: 500, train loss: 0.0300 F1: 13.36% dev loss: 0.0257 F1: 13.26% 0.00 min
> Epoch: 7 Step: 600, train loss: 0.0260 F1: 14.55% dev loss: 0.0236 F1: 13.76% 0.00 min
> Epoch: 9 Step: 700, train loss: 0.0248 F1: 16.74% dev loss: 0.0224 F1: 14.31% 0.00 min
> Epoch: 10 Step: 800, train loss: 0.0241 F1: 15.55% dev loss: 0.0215 F1: 14.56% 0.00 min
> Epoch: 11 Step: 900, train loss: 0.0224 F1: 16.42% dev loss: 0.0209 F1: 13.98% 0.00 min
> Epoch: 13 Step: 1000, train loss: 0.0223 F1: 17.58% dev loss: 0.0205 F1: 13.99% 0.00 min
> Epoch: 14 Step: 1100, train loss: 0.0221 F1: 16.89% dev loss: 0.0201 F1: 13.96% 0.00 min
> Epoch: 15 Step: 1200, train loss: 0.0209 F1: 16.59% dev loss: 0.0198 F1: 13.99% 0.00 min
> Epoch: 17 Step: 1300, train loss: 0.0210 F1: 17.78% dev loss: 0.0195 F1: 14.57% 0.00 min
> Epoch: 18 Step: 1400, train loss: 0.0211 F1: 17.30% dev loss: 0.0193 F1: 14.60% 0.00 min
> Epoch: 19 Step: 1500, train loss: 0.0200 F1: 17.85% dev loss: 0.0191 F1: 16.82% 0.00 min
> Epoch: 21 Step: 1600, train loss: 0.0202 F1: 17.70% dev loss: 0.0189 F1: 18.03% 0.00 min
> Epoch: 22 Step: 1700, train loss: 0.0204 F1: 18.32% dev loss: 0.0188 F1: 17.99% 0.00 min
> Epoch: 23 Step: 1800, train loss: 0.0194 F1: 17.96% dev loss: 0.0187 F1: 17.91% 0.00 min
> Epoch: 25 Step: 1900, train loss: 0.0196 F1: 18.80% dev loss: 0.0185 F1: 17.94% 0.00 min
> Epoch: 26 Step: 2000, train loss: 0.0199 F1: 19.40% dev loss: 0.0184 F1: 18.50% 0.00 min
> Epoch: 27 Step: 2100, train loss: 0.0189 F1: 19.15% dev loss: 0.0183 F1: 18.84% 0.00 min
> Epoch: 29 Step: 2200, train loss: 0.0192 F1: 19.67% dev loss: 0.0183 F1: 18.71% 0.00 min
> Epoch: 30 Step: 2300, train loss: 0.0195 F1: 19.08% dev loss: 0.0182 F1: 18.71% 0.00 min
> Epoch: 31 Step: 2400, train loss: 0.0186 F1: 19.23% dev loss: 0.0181 F1: 20.05% 0.00 min
> Epoch: 33 Step: 2500, train loss: 0.0189 F1: 20.30% dev loss: 0.0180 F1: 20.70% 0.00 min
> Epoch: 34 Step: 2600, train loss: 0.0192 F1: 19.16% dev loss: 0.0180 F1: 21.39% 0.00 min
> Epoch: 35 Step: 2700, train loss: 0.0183 F1: 19.66% dev loss: 0.0179 F1: 22.02% 0.00 min
> Epoch: 37 Step: 2800, train loss: 0.0187 F1: 19.25% dev loss: 0.0179 F1: 22.02% 0.00 min
> Epoch: 38 Step: 2900, train loss: 0.0190 F1: 19.86% dev loss: 0.0178 F1: 21.99% 0.00 min
> Epoch: 39 Step: 3000, train loss: 0.0181 F1: 19.59% dev loss: 0.0178 F1: 21.99% 0.00 min
> Epoch: 41 Step: 3100, train loss: 0.0184 F1: 19.66% dev loss: 0.0177 F1: 22.00% 0.00 min
> Epoch: 42 Step: 3200, train loss: 0.0188 F1: 20.30% dev loss: 0.0177 F1: 22.00% 0.00 min
> Epoch: 43 Step: 3300, train loss: 0.0180 F1: 20.93% dev loss: 0.0176 F1: 22.24% 0.00 min
> Epoch: 45 Step: 3400, train loss: 0.0183 F1: 21.08% dev loss: 0.0176 F1: 22.22% 0.00 min
> Epoch: 46 Step: 3500, train loss: 0.0187 F1: 19.53% dev loss: 0.0176 F1: 22.24% 0.00 min
> Epoch: 47 Step: 3600, train loss: 0.0180 F1: 19.28% dev loss: 0.0175 F1: 22.35% 0.00 min
> Epoch: 49 Step: 3700, train loss: 0.0181 F1: 19.93% dev loss: 0.0175 F1: 22.35% 0.00 min
> Epoch: 50 Step: 3800, train loss: 0.0185 F1: 20.07% dev loss: 0.0175 F1: 22.35% 0.00 min
> Epoch: 51 Step: 3900, train loss: 0.0177 F1: 20.59% dev loss: 0.0174 F1: 22.23% 0.00 min
> Epoch: 53 Step: 4000, train loss: 0.0181 F1: 20.33% dev loss: 0.0174 F1: 22.31% 0.00 min
> Epoch: 54 Step: 4100, train loss: 0.0184 F1: 20.19% dev loss: 0.0174 F1: 22.31% 0.00 min
> Epoch: 55 Step: 4200, train loss: 0.0176 F1: 20.64% dev loss: 0.0174 F1: 22.27% 0.00 min
> Epoch: 57 Step: 4300, train loss: 0.0179 F1: 19.95% dev loss: 0.0174 F1: 22.27% 0.00 min
> Epoch: 58 Step: 4400, train loss: 0.0184 F1: 20.16% dev loss: 0.0174 F1: 22.27% 0.00 min
> Epoch: 59 Step: 4500, train loss: 0.0175 F1: 20.58% dev loss: 0.0173 F1: 22.27% 0.00 min
> Epoch: 61 Step: 4600, train loss: 0.0180 F1: 19.91% dev loss: 0.0173 F1: 22.27% 0.00 min
> Epoch: 62 Step: 4700, train loss: 0.0184 F1: 20.06% dev loss: 0.0173 F1: 22.27% 0.00 min
> Epoch: 63 Step: 4800, train loss: 0.0175 F1: 20.69% dev loss: 0.0173 F1: 22.27% 0.00 min
> Epoch: 65 Step: 4900, train loss: 0.0179 F1: 21.13% dev loss: 0.0173 F1: 22.19% 0.00 min
> Epoch: 66 Step: 5000, train loss: 0.0184 F1: 20.29% dev loss: 0.0173 F1: 22.19% 0.00 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 20:08:23.795363>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 0.005
>>> epochs: 400
>>> step: 100
>>> model_name: glove
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 0.1
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/glove-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 5000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 0.005, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
> Epoch: 1 Step: 100, train loss: 0.0943 F1: 13.26% dev loss: 0.0355 F1: 11.78% 0.01 min
> Epoch: 2 Step: 200, train loss: 0.0293 F1: 14.56% dev loss: 0.0227 F1: 13.60% 0.00 min
> Epoch: 3 Step: 300, train loss: 0.0224 F1: 16.25% dev loss: 0.0199 F1: 14.13% 0.00 min
> Epoch: 5 Step: 400, train loss: 0.0205 F1: 18.11% dev loss: 0.0187 F1: 17.80% 0.00 min
> Epoch: 6 Step: 500, train loss: 0.0198 F1: 18.54% dev loss: 0.0180 F1: 21.19% 0.00 min
> Epoch: 7 Step: 600, train loss: 0.0183 F1: 19.98% dev loss: 0.0175 F1: 19.46% 0.00 min
> Epoch: 9 Step: 700, train loss: 0.0182 F1: 20.93% dev loss: 0.0173 F1: 22.81% 0.00 min
> Epoch: 10 Step: 800, train loss: 0.0183 F1: 20.01% dev loss: 0.0171 F1: 22.15% 0.00 min
> Epoch: 11 Step: 900, train loss: 0.0172 F1: 21.01% dev loss: 0.0169 F1: 19.23% 0.00 min
> Epoch: 13 Step: 1000, train loss: 0.0174 F1: 20.42% dev loss: 0.0168 F1: 22.60% 0.00 min
> Epoch: 14 Step: 1100, train loss: 0.0176 F1: 20.59% dev loss: 0.0167 F1: 21.26% 0.00 min
> Epoch: 15 Step: 1200, train loss: 0.0167 F1: 20.31% dev loss: 0.0166 F1: 19.38% 0.00 min
> Epoch: 17 Step: 1300, train loss: 0.0170 F1: 19.83% dev loss: 0.0164 F1: 23.87% 0.00 min
> Epoch: 18 Step: 1400, train loss: 0.0172 F1: 22.32% dev loss: 0.0164 F1: 22.10% 0.00 min
> Epoch: 19 Step: 1500, train loss: 0.0163 F1: 21.16% dev loss: 0.0162 F1: 18.89% 0.00 min
> Epoch: 21 Step: 1600, train loss: 0.0166 F1: 21.22% dev loss: 0.0162 F1: 21.23% 0.00 min
> Epoch: 22 Step: 1700, train loss: 0.0169 F1: 22.11% dev loss: 0.0162 F1: 22.00% 0.00 min
> Epoch: 23 Step: 1800, train loss: 0.0161 F1: 21.79% dev loss: 0.0160 F1: 17.44% 0.00 min
> Epoch: 25 Step: 1900, train loss: 0.0164 F1: 21.97% dev loss: 0.0160 F1: 21.83% 0.00 min
> Epoch: 26 Step: 2000, train loss: 0.0167 F1: 23.32% dev loss: 0.0160 F1: 20.44% 0.00 min
> Epoch: 27 Step: 2100, train loss: 0.0159 F1: 22.49% dev loss: 0.0158 F1: 17.41% 0.00 min
> Epoch: 29 Step: 2200, train loss: 0.0162 F1: 23.36% dev loss: 0.0158 F1: 21.21% 0.00 min
> Epoch: 30 Step: 2300, train loss: 0.0165 F1: 22.82% dev loss: 0.0158 F1: 20.39% 0.00 min
> Epoch: 31 Step: 2400, train loss: 0.0158 F1: 22.98% dev loss: 0.0157 F1: 20.71% 0.00 min
> Epoch: 33 Step: 2500, train loss: 0.0161 F1: 21.26% dev loss: 0.0157 F1: 21.29% 0.00 min
> Epoch: 34 Step: 2600, train loss: 0.0163 F1: 24.15% dev loss: 0.0157 F1: 21.04% 0.00 min
> Epoch: 35 Step: 2700, train loss: 0.0156 F1: 23.04% dev loss: 0.0156 F1: 20.85% 0.00 min
> Epoch: 37 Step: 2800, train loss: 0.0158 F1: 22.54% dev loss: 0.0156 F1: 21.23% 0.00 min
> Epoch: 38 Step: 2900, train loss: 0.0162 F1: 23.67% dev loss: 0.0156 F1: 20.54% 0.00 min
> Epoch: 39 Step: 3000, train loss: 0.0154 F1: 22.31% dev loss: 0.0155 F1: 20.82% 0.00 min
> Epoch: 41 Step: 3100, train loss: 0.0158 F1: 26.84% dev loss: 0.0156 F1: 21.73% 0.00 min
> Epoch: 42 Step: 3200, train loss: 0.0161 F1: 24.35% dev loss: 0.0156 F1: 20.77% 0.00 min
> Epoch: 43 Step: 3300, train loss: 0.0154 F1: 25.64% dev loss: 0.0154 F1: 20.91% 0.00 min
> Epoch: 45 Step: 3400, train loss: 0.0157 F1: 23.36% dev loss: 0.0155 F1: 21.75% 0.00 min
> Epoch: 46 Step: 3500, train loss: 0.0161 F1: 22.28% dev loss: 0.0155 F1: 22.70% 0.00 min
> Epoch: 47 Step: 3600, train loss: 0.0154 F1: 23.75% dev loss: 0.0154 F1: 20.86% 0.00 min
> Epoch: 49 Step: 3700, train loss: 0.0156 F1: 23.28% dev loss: 0.0154 F1: 21.69% 0.00 min
> Epoch: 50 Step: 3800, train loss: 0.0160 F1: 23.01% dev loss: 0.0155 F1: 20.60% 0.00 min
> Epoch: 51 Step: 3900, train loss: 0.0153 F1: 24.11% dev loss: 0.0154 F1: 20.74% 0.00 min
> Epoch: 53 Step: 4000, train loss: 0.0156 F1: 23.37% dev loss: 0.0154 F1: 20.98% 0.00 min
> Epoch: 54 Step: 4100, train loss: 0.0159 F1: 25.47% dev loss: 0.0154 F1: 20.94% 0.00 min
> Epoch: 55 Step: 4200, train loss: 0.0152 F1: 23.31% dev loss: 0.0154 F1: 20.87% 0.00 min
> Epoch: 57 Step: 4300, train loss: 0.0155 F1: 21.15% dev loss: 0.0154 F1: 21.04% 0.00 min
> Epoch: 58 Step: 4400, train loss: 0.0159 F1: 23.69% dev loss: 0.0154 F1: 20.80% 0.00 min
> Epoch: 59 Step: 4500, train loss: 0.0150 F1: 26.45% dev loss: 0.0153 F1: 20.77% 0.00 min
> Epoch: 61 Step: 4600, train loss: 0.0156 F1: 22.49% dev loss: 0.0153 F1: 21.14% 0.00 min
> Epoch: 62 Step: 4700, train loss: 0.0159 F1: 22.68% dev loss: 0.0153 F1: 21.11% 0.00 min
> Epoch: 63 Step: 4800, train loss: 0.0151 F1: 23.38% dev loss: 0.0153 F1: 21.00% 0.00 min
> Epoch: 65 Step: 4900, train loss: 0.0154 F1: 23.99% dev loss: 0.0153 F1: 21.12% 0.00 min
> Epoch: 66 Step: 5000, train loss: 0.0159 F1: 22.51% dev loss: 0.0153 F1: 21.12% 0.00 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 20:08:55.631021>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 0.001
>>> epochs: 400
>>> step: 100
>>> model_name: glove
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 0.1
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/glove-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 5000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 0.001, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
> Epoch: 1 Step: 100, train loss: 0.1331 F1: 12.71% dev loss: 0.1012 F1: 13.39% 0.01 min
> Epoch: 2 Step: 200, train loss: 0.0690 F1: 13.48% dev loss: 0.0429 F1: 11.39% 0.00 min
> Epoch: 3 Step: 300, train loss: 0.0355 F1: 13.62% dev loss: 0.0285 F1: 12.81% 0.00 min
> Epoch: 5 Step: 400, train loss: 0.0280 F1: 14.78% dev loss: 0.0241 F1: 13.78% 0.00 min
> Epoch: 6 Step: 500, train loss: 0.0252 F1: 15.59% dev loss: 0.0219 F1: 14.00% 0.00 min
> Epoch: 7 Step: 600, train loss: 0.0225 F1: 16.09% dev loss: 0.0207 F1: 13.91% 0.00 min
> Epoch: 9 Step: 700, train loss: 0.0218 F1: 18.06% dev loss: 0.0199 F1: 14.07% 0.00 min
> Epoch: 10 Step: 800, train loss: 0.0215 F1: 16.54% dev loss: 0.0194 F1: 14.21% 0.00 min
> Epoch: 11 Step: 900, train loss: 0.0201 F1: 17.60% dev loss: 0.0191 F1: 16.42% 0.00 min
> Epoch: 13 Step: 1000, train loss: 0.0201 F1: 18.43% dev loss: 0.0188 F1: 17.99% 0.00 min
> Epoch: 14 Step: 1100, train loss: 0.0201 F1: 17.85% dev loss: 0.0185 F1: 18.94% 0.00 min
> Epoch: 15 Step: 1200, train loss: 0.0191 F1: 18.35% dev loss: 0.0184 F1: 18.76% 0.00 min
> Epoch: 17 Step: 1300, train loss: 0.0192 F1: 19.67% dev loss: 0.0182 F1: 18.85% 0.00 min
> Epoch: 18 Step: 1400, train loss: 0.0194 F1: 19.05% dev loss: 0.0180 F1: 22.49% 0.00 min
> Epoch: 19 Step: 1500, train loss: 0.0184 F1: 20.16% dev loss: 0.0179 F1: 21.93% 0.00 min
> Epoch: 21 Step: 1600, train loss: 0.0186 F1: 21.38% dev loss: 0.0178 F1: 22.05% 0.00 min
> Epoch: 22 Step: 1700, train loss: 0.0190 F1: 20.91% dev loss: 0.0177 F1: 22.22% 0.00 min
> Epoch: 23 Step: 1800, train loss: 0.0180 F1: 20.54% dev loss: 0.0176 F1: 21.88% 0.00 min
> Epoch: 25 Step: 1900, train loss: 0.0182 F1: 20.02% dev loss: 0.0175 F1: 22.22% 0.00 min
> Epoch: 26 Step: 2000, train loss: 0.0186 F1: 21.29% dev loss: 0.0175 F1: 21.65% 0.00 min
> Epoch: 27 Step: 2100, train loss: 0.0177 F1: 20.33% dev loss: 0.0174 F1: 21.34% 0.00 min
> Epoch: 29 Step: 2200, train loss: 0.0179 F1: 21.43% dev loss: 0.0173 F1: 21.60% 0.00 min
> Epoch: 30 Step: 2300, train loss: 0.0183 F1: 20.69% dev loss: 0.0173 F1: 21.40% 0.00 min
> Epoch: 31 Step: 2400, train loss: 0.0175 F1: 21.35% dev loss: 0.0172 F1: 21.18% 0.00 min
> Epoch: 33 Step: 2500, train loss: 0.0178 F1: 21.51% dev loss: 0.0172 F1: 21.45% 0.00 min
> Epoch: 34 Step: 2600, train loss: 0.0181 F1: 21.02% dev loss: 0.0171 F1: 22.21% 0.00 min
> Epoch: 35 Step: 2700, train loss: 0.0172 F1: 20.95% dev loss: 0.0171 F1: 21.37% 0.00 min
> Epoch: 37 Step: 2800, train loss: 0.0175 F1: 20.93% dev loss: 0.0170 F1: 20.84% 0.00 min
> Epoch: 38 Step: 2900, train loss: 0.0179 F1: 21.69% dev loss: 0.0170 F1: 20.43% 0.00 min
> Epoch: 39 Step: 3000, train loss: 0.0171 F1: 20.88% dev loss: 0.0170 F1: 21.39% 0.00 min
> Epoch: 41 Step: 3100, train loss: 0.0173 F1: 20.86% dev loss: 0.0169 F1: 20.78% 0.00 min
> Epoch: 42 Step: 3200, train loss: 0.0177 F1: 20.88% dev loss: 0.0169 F1: 22.06% 0.00 min
> Epoch: 43 Step: 3300, train loss: 0.0170 F1: 21.46% dev loss: 0.0168 F1: 21.17% 0.00 min
> Epoch: 45 Step: 3400, train loss: 0.0173 F1: 21.14% dev loss: 0.0168 F1: 21.45% 0.00 min
> Epoch: 46 Step: 3500, train loss: 0.0177 F1: 21.56% dev loss: 0.0168 F1: 21.84% 0.00 min
> Epoch: 47 Step: 3600, train loss: 0.0170 F1: 20.10% dev loss: 0.0168 F1: 21.51% 0.00 min
> Epoch: 49 Step: 3700, train loss: 0.0171 F1: 20.81% dev loss: 0.0167 F1: 21.51% 0.00 min
> Epoch: 50 Step: 3800, train loss: 0.0175 F1: 20.69% dev loss: 0.0167 F1: 21.72% 0.00 min
> Epoch: 51 Step: 3900, train loss: 0.0168 F1: 21.30% dev loss: 0.0167 F1: 21.48% 0.00 min
> Epoch: 53 Step: 4000, train loss: 0.0171 F1: 20.45% dev loss: 0.0167 F1: 21.95% 0.00 min
> Epoch: 54 Step: 4100, train loss: 0.0174 F1: 21.62% dev loss: 0.0167 F1: 22.01% 0.00 min
> Epoch: 55 Step: 4200, train loss: 0.0167 F1: 21.04% dev loss: 0.0166 F1: 21.53% 0.00 min
> Epoch: 57 Step: 4300, train loss: 0.0169 F1: 20.69% dev loss: 0.0166 F1: 21.73% 0.00 min
> Epoch: 58 Step: 4400, train loss: 0.0174 F1: 20.91% dev loss: 0.0166 F1: 21.79% 0.00 min
> Epoch: 59 Step: 4500, train loss: 0.0165 F1: 21.61% dev loss: 0.0166 F1: 21.99% 0.00 min
> Epoch: 61 Step: 4600, train loss: 0.0170 F1: 20.59% dev loss: 0.0166 F1: 22.33% 0.00 min
> Epoch: 62 Step: 4700, train loss: 0.0174 F1: 21.01% dev loss: 0.0166 F1: 22.11% 0.00 min
> Epoch: 63 Step: 4800, train loss: 0.0166 F1: 21.08% dev loss: 0.0166 F1: 21.98% 0.00 min
> Epoch: 65 Step: 4900, train loss: 0.0169 F1: 21.31% dev loss: 0.0166 F1: 22.11% 0.00 min
> Epoch: 66 Step: 5000, train loss: 0.0174 F1: 20.85% dev loss: 0.0166 F1: 22.11% 0.00 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 20:09:44.886586>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 0.01
>>> epochs: 400
>>> step: 100
>>> model_name: glove
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 0.1
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/glove-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 5000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 0.01, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
> Epoch: 1 Step: 100, train loss: 0.0759 F1: 13.82% dev loss: 0.0262 F1: 13.09% 0.01 min
> Epoch: 2 Step: 200, train loss: 0.0247 F1: 16.32% dev loss: 0.0202 F1: 13.71% 0.01 min
> Epoch: 3 Step: 300, train loss: 0.0202 F1: 17.51% dev loss: 0.0185 F1: 18.01% 0.01 min
> Epoch: 5 Step: 400, train loss: 0.0191 F1: 19.78% dev loss: 0.0178 F1: 20.71% 0.01 min
> Epoch: 6 Step: 500, train loss: 0.0188 F1: 19.95% dev loss: 0.0173 F1: 19.35% 0.01 min
> Epoch: 7 Step: 600, train loss: 0.0176 F1: 19.99% dev loss: 0.0169 F1: 19.67% 0.01 min
> Epoch: 9 Step: 700, train loss: 0.0176 F1: 21.09% dev loss: 0.0168 F1: 19.69% 0.01 min
> Epoch: 10 Step: 800, train loss: 0.0178 F1: 20.00% dev loss: 0.0166 F1: 19.96% 0.01 min
> Epoch: 11 Step: 900, train loss: 0.0168 F1: 20.66% dev loss: 0.0163 F1: 19.65% 0.01 min
> Epoch: 13 Step: 1000, train loss: 0.0169 F1: 21.43% dev loss: 0.0164 F1: 19.61% 0.01 min
> Epoch: 14 Step: 1100, train loss: 0.0171 F1: 22.10% dev loss: 0.0163 F1: 20.85% 0.01 min
> Epoch: 15 Step: 1200, train loss: 0.0163 F1: 21.32% dev loss: 0.0160 F1: 18.42% 0.01 min
> Epoch: 17 Step: 1300, train loss: 0.0166 F1: 19.94% dev loss: 0.0161 F1: 19.24% 0.01 min
> Epoch: 18 Step: 1400, train loss: 0.0169 F1: 20.83% dev loss: 0.0159 F1: 19.41% 0.01 min
> Epoch: 19 Step: 1500, train loss: 0.0159 F1: 22.07% dev loss: 0.0157 F1: 17.53% 0.01 min
> Epoch: 21 Step: 1600, train loss: 0.0163 F1: 21.01% dev loss: 0.0159 F1: 20.29% 0.01 min
> Epoch: 22 Step: 1700, train loss: 0.0167 F1: 23.55% dev loss: 0.0157 F1: 17.79% 0.01 min
> Epoch: 23 Step: 1800, train loss: 0.0158 F1: 22.50% dev loss: 0.0155 F1: 21.54% 0.01 min
> Epoch: 25 Step: 1900, train loss: 0.0161 F1: 21.09% dev loss: 0.0157 F1: 20.85% 0.01 min
> Epoch: 26 Step: 2000, train loss: 0.0165 F1: 24.60% dev loss: 0.0156 F1: 18.21% 0.01 min
> Epoch: 27 Step: 2100, train loss: 0.0156 F1: 22.24% dev loss: 0.0155 F1: 18.30% 0.01 min
> Epoch: 29 Step: 2200, train loss: 0.0159 F1: 24.38% dev loss: 0.0156 F1: 21.04% 0.01 min
> Epoch: 30 Step: 2300, train loss: 0.0164 F1: 26.08% dev loss: 0.0155 F1: 20.71% 0.01 min
> Epoch: 31 Step: 2400, train loss: 0.0156 F1: 21.42% dev loss: 0.0154 F1: 20.07% 0.01 min
> Epoch: 33 Step: 2500, train loss: 0.0160 F1: 22.04% dev loss: 0.0156 F1: 24.65% 0.01 min
> Epoch: 34 Step: 2600, train loss: 0.0162 F1: 23.42% dev loss: 0.0154 F1: 20.12% 0.01 min
> Epoch: 35 Step: 2700, train loss: 0.0155 F1: 24.93% dev loss: 0.0154 F1: 19.22% 0.01 min
> Epoch: 37 Step: 2800, train loss: 0.0157 F1: 22.22% dev loss: 0.0155 F1: 23.60% 0.01 min
> Epoch: 38 Step: 2900, train loss: 0.0161 F1: 24.48% dev loss: 0.0154 F1: 22.35% 0.01 min
> Epoch: 39 Step: 3000, train loss: 0.0153 F1: 21.28% dev loss: 0.0153 F1: 20.91% 0.01 min
> Epoch: 41 Step: 3100, train loss: 0.0157 F1: 26.29% dev loss: 0.0155 F1: 23.82% 0.01 min
> Epoch: 42 Step: 3200, train loss: 0.0161 F1: 26.63% dev loss: 0.0153 F1: 23.57% 0.01 min
> Epoch: 43 Step: 3300, train loss: 0.0153 F1: 23.79% dev loss: 0.0153 F1: 21.88% 0.01 min
> Epoch: 45 Step: 3400, train loss: 0.0156 F1: 22.54% dev loss: 0.0154 F1: 23.70% 0.01 min
> Epoch: 46 Step: 3500, train loss: 0.0160 F1: 21.74% dev loss: 0.0153 F1: 23.00% 0.01 min
> Epoch: 47 Step: 3600, train loss: 0.0153 F1: 24.81% dev loss: 0.0152 F1: 21.40% 0.01 min
> Epoch: 49 Step: 3700, train loss: 0.0155 F1: 23.17% dev loss: 0.0153 F1: 22.14% 0.00 min
> Epoch: 50 Step: 3800, train loss: 0.0159 F1: 25.19% dev loss: 0.0153 F1: 21.08% 0.00 min
> Epoch: 51 Step: 3900, train loss: 0.0152 F1: 24.85% dev loss: 0.0152 F1: 21.22% 0.00 min
> Epoch: 53 Step: 4000, train loss: 0.0155 F1: 23.63% dev loss: 0.0153 F1: 20.74% 0.00 min
> Epoch: 54 Step: 4100, train loss: 0.0158 F1: 26.27% dev loss: 0.0153 F1: 20.90% 0.00 min
> Epoch: 55 Step: 4200, train loss: 0.0151 F1: 24.31% dev loss: 0.0152 F1: 20.89% 0.00 min
> Epoch: 57 Step: 4300, train loss: 0.0154 F1: 21.10% dev loss: 0.0152 F1: 21.39% 0.01 min
> Epoch: 58 Step: 4400, train loss: 0.0158 F1: 23.63% dev loss: 0.0152 F1: 20.99% 0.01 min
> Epoch: 59 Step: 4500, train loss: 0.0149 F1: 25.67% dev loss: 0.0152 F1: 21.50% 0.01 min
> Epoch: 61 Step: 4600, train loss: 0.0154 F1: 21.66% dev loss: 0.0152 F1: 21.51% 0.01 min
> Epoch: 62 Step: 4700, train loss: 0.0157 F1: 22.38% dev loss: 0.0152 F1: 21.10% 0.01 min
> Epoch: 63 Step: 4800, train loss: 0.0149 F1: 23.93% dev loss: 0.0152 F1: 21.03% 0.01 min
> Epoch: 65 Step: 4900, train loss: 0.0152 F1: 24.68% dev loss: 0.0152 F1: 21.17% 0.01 min
> Epoch: 66 Step: 5000, train loss: 0.0157 F1: 23.19% dev loss: 0.0152 F1: 21.06% 0.01 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 05:26:50.014449>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: crf
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-crf_res14_seed7.pth
>>> seed: 7
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 2000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: True
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 15.9654 F1: 9.43% dev loss: 8.3642 F1: 9.41% 1.24 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 05:29:13.202350>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: crf
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-crf_res14_seed7.pth
>>> seed: 7
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 2000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: True
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 15.9654 F1: 9.43% dev loss: 8.3642 F1: 9.41% 1.35 min
> Epoch: 2 Step: 200, train loss: 6.9905 F1: 13.87% dev loss: 5.0074 F1: 14.64% 1.35 min
> Epoch: 3 Step: 300, train loss: 4.1309 F1: 22.91% dev loss: 4.0407 F1: 28.40% 1.34 min
> Epoch: 5 Step: 400, train loss: 2.8788 F1: 46.47% dev loss: 4.0390 F1: 44.09% 1.34 min
> Epoch: 6 Step: 500, train loss: 2.0996 F1: 59.32% dev loss: 3.6439 F1: 41.18% 1.35 min
> Epoch: 7 Step: 600, train loss: 1.4415 F1: 74.71% dev loss: 3.3512 F1: 49.87% 1.35 min
> Epoch: 9 Step: 700, train loss: 0.9326 F1: 82.67% dev loss: 3.7740 F1: 52.30% 1.35 min
> Epoch: 10 Step: 800, train loss: 0.7494 F1: 86.04% dev loss: 4.0879 F1: 53.60% 1.35 min
> Epoch: 11 Step: 900, train loss: 0.5103 F1: 92.97% dev loss: 3.9327 F1: 55.24% 1.35 min
> Epoch: 13 Step: 1000, train loss: 0.3108 F1: 94.50% dev loss: 4.2605 F1: 54.72% 1.35 min
> Epoch: 14 Step: 1100, train loss: 0.1821 F1: 97.89% dev loss: 4.4942 F1: 53.75% 1.35 min
> Epoch: 15 Step: 1200, train loss: 0.1304 F1: 97.90% dev loss: 4.8261 F1: 60.26% 1.35 min
> Epoch: 17 Step: 1300, train loss: 0.0977 F1: 97.95% dev loss: 4.8733 F1: 57.03% 1.35 min
> Epoch: 18 Step: 1400, train loss: 0.0807 F1: 99.44% dev loss: 4.9582 F1: 56.20% 1.35 min
> Epoch: 19 Step: 1500, train loss: 0.0823 F1: 99.33% dev loss: 5.0509 F1: 59.35% 1.35 min
> Epoch: 21 Step: 1600, train loss: 0.0602 F1: 89.94% dev loss: 5.1906 F1: 58.37% 1.35 min
> Epoch: 22 Step: 1700, train loss: 0.0463 F1: 99.84% dev loss: 5.1882 F1: 58.31% 1.34 min
> Epoch: 23 Step: 1800, train loss: 0.0400 F1: 99.88% dev loss: 5.2215 F1: 61.63% 1.35 min
> Epoch: 24 Step: 1900, train loss: 0.0354 F1: 99.79% dev loss: 5.3185 F1: 59.17% 1.35 min
> Epoch: 26 Step: 2000, train loss: 0.0292 F1: 99.18% dev loss: 5.2785 F1: 59.64% 1.34 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 05:58:34.010645>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: crf
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-crf_res14_seed7.pth
>>> seed: 7
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3500
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: True
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 15.9654 F1: 9.43% dev loss: 8.3642 F1: 9.41% 1.33 min
> Epoch: 2 Step: 200, train loss: 6.9905 F1: 13.87% dev loss: 5.0074 F1: 14.64% 1.34 min
> Epoch: 3 Step: 300, train loss: 4.1309 F1: 22.91% dev loss: 4.0407 F1: 28.40% 1.35 min
> Epoch: 5 Step: 400, train loss: 2.8788 F1: 46.47% dev loss: 4.0390 F1: 44.09% 1.35 min
> Epoch: 6 Step: 500, train loss: 2.0996 F1: 59.32% dev loss: 3.6439 F1: 41.18% 1.35 min
> Epoch: 7 Step: 600, train loss: 1.4482 F1: 74.08% dev loss: 3.3374 F1: 50.68% 1.35 min
> Epoch: 9 Step: 700, train loss: 0.9426 F1: 82.16% dev loss: 3.8394 F1: 52.39% 1.35 min
> Epoch: 10 Step: 800, train loss: 0.7535 F1: 86.91% dev loss: 4.0044 F1: 53.66% 1.35 min
> Epoch: 11 Step: 900, train loss: 0.4958 F1: 92.99% dev loss: 3.9729 F1: 56.00% 1.35 min
> Epoch: 13 Step: 1000, train loss: 0.3856 F1: 93.52% dev loss: 4.1716 F1: 57.88% 1.35 min
> Epoch: 14 Step: 1100, train loss: 0.2491 F1: 97.35% dev loss: 4.6969 F1: 57.16% 1.35 min
Initializing ELMo

>>>>>>>>>>>>>>>>>>>>>2021-07-21 06:16:15.202640>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 50
>>> step: 100
>>> model_name: elmo
>>> downstream: crf
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/elmo-crf_res14_seed7.pth
>>> seed: 7
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3500
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: True
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 34.9179 F1: 10.44% dev loss: 23.4622 F1: 9.47% 1.68 min
> Epoch: 2 Step: 200, train loss: 11.0293 F1: 10.44% dev loss: 6.7574 F1: 11.93% 1.69 min
> Epoch: 3 Step: 300, train loss: 5.3638 F1: 18.05% dev loss: 4.9100 F1: 15.01% 1.70 min
> Epoch: 5 Step: 400, train loss: 4.2071 F1: 26.98% dev loss: 4.3755 F1: 22.35% 1.68 min
> Epoch: 6 Step: 500, train loss: 3.5237 F1: 31.31% dev loss: 4.1492 F1: 26.01% 1.70 min
> Epoch: 7 Step: 600, train loss: 2.7943 F1: 36.35% dev loss: 3.9507 F1: 29.04% 1.71 min
> Epoch: 9 Step: 700, train loss: 2.2509 F1: 43.04% dev loss: 3.8537 F1: 32.04% 1.68 min
> Epoch: 10 Step: 800, train loss: 1.9267 F1: 57.23% dev loss: 3.7842 F1: 32.23% 1.71 min
> Epoch: 11 Step: 900, train loss: 1.4856 F1: 66.09% dev loss: 3.8551 F1: 32.90% 1.72 min
> Epoch: 13 Step: 1000, train loss: 1.1766 F1: 76.21% dev loss: 3.9121 F1: 37.14% 1.69 min
> Epoch: 14 Step: 1100, train loss: 0.9943 F1: 79.95% dev loss: 3.8753 F1: 37.17% 1.70 min
> Epoch: 15 Step: 1200, train loss: 0.7666 F1: 85.51% dev loss: 4.3082 F1: 35.11% 1.70 min
> Epoch: 17 Step: 1300, train loss: 0.6447 F1: 90.03% dev loss: 3.9891 F1: 37.48% 1.69 min
> Epoch: 18 Step: 1400, train loss: 0.5949 F1: 89.38% dev loss: 3.9632 F1: 37.18% 1.69 min
> Epoch: 19 Step: 1500, train loss: 0.4614 F1: 94.42% dev loss: 4.1541 F1: 39.01% 1.70 min
> Epoch: 21 Step: 1600, train loss: 0.3509 F1: 85.36% dev loss: 4.3084 F1: 38.80% 1.69 min
> Epoch: 22 Step: 1700, train loss: 0.3008 F1: 97.54% dev loss: 4.3765 F1: 40.07% 1.70 min
> Epoch: 23 Step: 1800, train loss: 0.2590 F1: 97.53% dev loss: 4.3917 F1: 36.26% 1.68 min
> Epoch: 24 Step: 1900, train loss: 0.2118 F1: 98.34% dev loss: 4.4971 F1: 36.60% 1.70 min
> Epoch: 26 Step: 2000, train loss: 0.1864 F1: 98.62% dev loss: 4.6615 F1: 41.38% 1.69 min
> Epoch: 27 Step: 2100, train loss: 0.1686 F1: 99.11% dev loss: 4.5916 F1: 38.25% 1.69 min
> Epoch: 28 Step: 2200, train loss: 0.1395 F1: 99.16% dev loss: 4.6220 F1: 35.35% 1.71 min
> Epoch: 30 Step: 2300, train loss: 0.1255 F1: 99.42% dev loss: 4.7506 F1: 41.28% 1.68 min
> Epoch: 31 Step: 2400, train loss: 0.1219 F1: 99.56% dev loss: 4.8558 F1: 35.74% 1.70 min
> Epoch: 32 Step: 2500, train loss: 0.1001 F1: 99.50% dev loss: 4.8166 F1: 38.31% 1.70 min
> Epoch: 34 Step: 2600, train loss: 0.0898 F1: 99.59% dev loss: 4.8812 F1: 40.28% 1.68 min
> Epoch: 35 Step: 2700, train loss: 0.0859 F1: 99.71% dev loss: 4.9265 F1: 41.03% 1.70 min
> Epoch: 36 Step: 2800, train loss: 0.0764 F1: 99.83% dev loss: 5.0582 F1: 40.18% 1.70 min
> Epoch: 38 Step: 2900, train loss: 0.0679 F1: 100.00% dev loss: 4.9663 F1: 37.81% 1.69 min
> Epoch: 39 Step: 3000, train loss: 0.0714 F1: 99.91% dev loss: 4.9832 F1: 38.97% 1.70 min
> Epoch: 40 Step: 3100, train loss: 0.0650 F1: 100.00% dev loss: 5.0732 F1: 36.52% 1.70 min
> Epoch: 42 Step: 3200, train loss: 0.0607 F1: 100.00% dev loss: 5.1116 F1: 39.94% 1.69 min
> Epoch: 43 Step: 3300, train loss: 0.0594 F1: 100.00% dev loss: 5.0578 F1: 39.79% 1.69 min
> Epoch: 44 Step: 3400, train loss: 0.0577 F1: 100.00% dev loss: 5.0445 F1: 39.55% 1.70 min
> Epoch: 46 Step: 3500, train loss: 0.0564 F1: 90.00% dev loss: 5.0739 F1: 40.10% 1.69 min
Initializing ELMo

>>>>>>>>>>>>>>>>>>>>>2021-07-21 07:29:19.816667>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 50
>>> step: 100
>>> model_name: elmo
>>> downstream: crf
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/elmo-crf_res14_seed7.pth
>>> seed: 7
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: True
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 34.9179 F1: 10.44% dev loss: 23.4622 F1: 9.47% 1.62 min
> Epoch: 2 Step: 200, train loss: 11.1586 F1: 10.40% dev loss: 6.9267 F1: 13.60% 1.69 min
> Epoch: 3 Step: 300, train loss: 5.5662 F1: 18.64% dev loss: 5.0197 F1: 15.38% 1.70 min
> Epoch: 5 Step: 400, train loss: 4.3995 F1: 24.76% dev loss: 4.4794 F1: 21.23% 1.68 min
> Epoch: 6 Step: 500, train loss: 3.6272 F1: 30.89% dev loss: 4.1224 F1: 26.93% 1.70 min
> Epoch: 7 Step: 600, train loss: 2.7414 F1: 37.88% dev loss: 4.0578 F1: 26.90% 1.70 min
> Epoch: 9 Step: 700, train loss: 2.2776 F1: 43.40% dev loss: 4.0334 F1: 29.69% 1.68 min
> Epoch: 10 Step: 800, train loss: 2.0224 F1: 51.78% dev loss: 3.7291 F1: 32.22% 1.70 min
> Epoch: 11 Step: 900, train loss: 1.5873 F1: 60.17% dev loss: 3.7834 F1: 32.87% 1.70 min
> Epoch: 13 Step: 1000, train loss: 1.2652 F1: 67.41% dev loss: 3.7938 F1: 33.37% 1.68 min
> Epoch: 14 Step: 1100, train loss: 1.0994 F1: 76.89% dev loss: 3.8197 F1: 37.72% 1.69 min
> Epoch: 15 Step: 1200, train loss: 0.8731 F1: 82.31% dev loss: 4.0794 F1: 37.35% 1.70 min
> Epoch: 17 Step: 1300, train loss: 0.7181 F1: 86.05% dev loss: 4.2376 F1: 35.24% 1.69 min
> Epoch: 18 Step: 1400, train loss: 0.6663 F1: 87.32% dev loss: 4.1398 F1: 34.38% 1.69 min
> Epoch: 19 Step: 1500, train loss: 0.5784 F1: 90.88% dev loss: 4.0359 F1: 34.44% 1.70 min
> Epoch: 21 Step: 1600, train loss: 0.4512 F1: 84.56% dev loss: 4.3033 F1: 35.17% 1.69 min
> Epoch: 22 Step: 1700, train loss: 0.3853 F1: 96.06% dev loss: 4.2534 F1: 38.44% 1.70 min
> Epoch: 23 Step: 1800, train loss: 0.3328 F1: 96.53% dev loss: 4.3251 F1: 37.66% 1.68 min
> Epoch: 24 Step: 1900, train loss: 0.2771 F1: 97.72% dev loss: 4.5268 F1: 35.44% 1.69 min
> Epoch: 26 Step: 2000, train loss: 0.2471 F1: 98.06% dev loss: 4.4919 F1: 36.87% 1.69 min
> Epoch: 27 Step: 2100, train loss: 0.2269 F1: 97.99% dev loss: 4.5129 F1: 36.36% 1.69 min
> Epoch: 28 Step: 2200, train loss: 0.1882 F1: 98.48% dev loss: 4.6238 F1: 34.91% 1.70 min
> Epoch: 30 Step: 2300, train loss: 0.1751 F1: 99.18% dev loss: 4.6702 F1: 35.31% 1.68 min
> Epoch: 31 Step: 2400, train loss: 0.1722 F1: 99.01% dev loss: 4.6937 F1: 35.25% 1.70 min
> Epoch: 32 Step: 2500, train loss: 0.1472 F1: 99.16% dev loss: 4.7311 F1: 35.06% 1.70 min
> Epoch: 34 Step: 2600, train loss: 0.1366 F1: 99.22% dev loss: 4.7490 F1: 35.85% 1.67 min
> Epoch: 35 Step: 2700, train loss: 0.1375 F1: 99.27% dev loss: 4.7239 F1: 36.43% 1.69 min
> Epoch: 36 Step: 2800, train loss: 0.1244 F1: 99.49% dev loss: 4.8561 F1: 34.90% 1.70 min
> Epoch: 38 Step: 2900, train loss: 0.1184 F1: 99.03% dev loss: 4.8400 F1: 34.55% 1.68 min
> Epoch: 39 Step: 3000, train loss: 0.1260 F1: 99.25% dev loss: 4.8398 F1: 34.76% 1.69 min
Initializing ELMo

>>>>>>>>>>>>>>>>>>>>>2021-07-21 08:33:57.756112>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 50
>>> step: 100
>>> model_name: elmo
>>> downstream: crf
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.5
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/elmo-crf_res14_seed7.pth
>>> seed: 7
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: True
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]
> Epoch: 1 Step: 100, train loss: 34.9179 F1: 10.44% dev loss: 23.4622 F1: 9.47% 1.62 min
> Epoch: 2 Step: 200, train loss: 11.9816 F1: 10.00% dev loss: 9.2264 F1: 10.77% 1.69 min
> Epoch: 3 Step: 300, train loss: 8.3506 F1: 10.48% dev loss: 7.9834 F1: 10.88% 1.70 min
> Epoch: 5 Step: 400, train loss: 7.4076 F1: 11.24% dev loss: 6.7295 F1: 12.16% 1.68 min
> Epoch: 6 Step: 500, train loss: 6.3483 F1: 13.37% dev loss: 5.7766 F1: 13.48% 1.70 min
> Epoch: 7 Step: 600, train loss: 5.3502 F1: 17.68% dev loss: 5.3233 F1: 13.93% 1.70 min
> Epoch: 9 Step: 700, train loss: 4.8615 F1: 17.00% dev loss: 4.9425 F1: 19.99% 1.67 min
> Epoch: 10 Step: 800, train loss: 4.7924 F1: 20.48% dev loss: 4.7430 F1: 20.60% 1.69 min
> Epoch: 11 Step: 900, train loss: 4.3032 F1: 22.87% dev loss: 4.5692 F1: 22.13% 1.70 min
Initializing ELMo

>>>>>>>>>>>>>>>>>>>>>2021-07-21 08:50:01.585581>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 0.0005
>>> epochs: 50
>>> step: 100
>>> model_name: elmo
>>> downstream: crf
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.5
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/elmo-crf_res14_seed7.pth
>>> seed: 7
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: True
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 0.0005, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]
> Epoch: 1 Step: 100, train loss: 17.3041 F1: 19.35% dev loss: 4.5598 F1: 24.68% 1.68 min
> Epoch: 2 Step: 200, train loss: 3.6235 F1: 37.25% dev loss: 3.6797 F1: 38.01% 1.69 min
> Epoch: 3 Step: 300, train loss: 1.9890 F1: 60.82% dev loss: 3.5683 F1: 40.18% 1.70 min
> Epoch: 5 Step: 400, train loss: 1.2204 F1: 80.49% dev loss: 3.7126 F1: 42.67% 1.67 min
> Epoch: 6 Step: 500, train loss: 1.0453 F1: 85.97% dev loss: 4.7986 F1: 35.82% 1.69 min
> Epoch: 7 Step: 600, train loss: 0.7110 F1: 89.76% dev loss: 4.7889 F1: 35.33% 1.70 min
> Epoch: 9 Step: 700, train loss: 0.3985 F1: 96.50% dev loss: 4.4688 F1: 37.88% 1.68 min
> Epoch: 10 Step: 800, train loss: 0.3015 F1: 97.12% dev loss: 5.2084 F1: 36.48% 1.69 min
> Epoch: 11 Step: 900, train loss: 0.1561 F1: 98.39% dev loss: 5.5147 F1: 36.72% 1.70 min
> Epoch: 13 Step: 1000, train loss: 0.0988 F1: 97.78% dev loss: 5.7563 F1: 34.88% 1.68 min
> Epoch: 14 Step: 1100, train loss: 0.0911 F1: 98.00% dev loss: 5.4086 F1: 42.49% 1.68 min
> Epoch: 15 Step: 1200, train loss: 0.0676 F1: 99.44% dev loss: 5.8369 F1: 45.25% 1.70 min
> Epoch: 17 Step: 1300, train loss: 0.0414 F1: 100.00% dev loss: 6.0344 F1: 38.80% 1.69 min
> Epoch: 18 Step: 1400, train loss: 0.0326 F1: 99.90% dev loss: 6.2004 F1: 37.00% 1.69 min
> Epoch: 19 Step: 1500, train loss: 0.0262 F1: 99.82% dev loss: 6.2131 F1: 37.86% 1.69 min
> Epoch: 21 Step: 1600, train loss: 0.0195 F1: 90.00% dev loss: 6.3485 F1: 39.96% 1.68 min
> Epoch: 22 Step: 1700, train loss: 0.0146 F1: 100.00% dev loss: 6.4985 F1: 39.43% 1.70 min
> Epoch: 23 Step: 1800, train loss: 0.0119 F1: 100.00% dev loss: 6.5408 F1: 39.48% 1.68 min
> Epoch: 24 Step: 1900, train loss: 0.0101 F1: 100.00% dev loss: 6.5986 F1: 39.97% 1.69 min
> Epoch: 26 Step: 2000, train loss: 0.0074 F1: 100.00% dev loss: 6.7136 F1: 39.63% 1.69 min
> Epoch: 27 Step: 2100, train loss: 0.0061 F1: 100.00% dev loss: 6.7843 F1: 39.57% 1.68 min
> Epoch: 28 Step: 2200, train loss: 0.0051 F1: 100.00% dev loss: 6.7811 F1: 40.04% 1.70 min
> Epoch: 30 Step: 2300, train loss: 0.0042 F1: 100.00% dev loss: 6.8486 F1: 39.45% 1.68 min
> Epoch: 31 Step: 2400, train loss: 0.0037 F1: 100.00% dev loss: 6.8946 F1: 38.57% 1.69 min
> Epoch: 32 Step: 2500, train loss: 0.0035 F1: 100.00% dev loss: 6.9128 F1: 43.65% 1.70 min
> Epoch: 34 Step: 2600, train loss: 0.0032 F1: 100.00% dev loss: 6.9085 F1: 44.14% 1.67 min
> Epoch: 35 Step: 2700, train loss: 0.0028 F1: 100.00% dev loss: 6.9595 F1: 38.47% 1.69 min
> Epoch: 36 Step: 2800, train loss: 0.0027 F1: 100.00% dev loss: 6.9722 F1: 38.46% 1.70 min
> Epoch: 38 Step: 2900, train loss: 0.0036 F1: 100.00% dev loss: 6.9749 F1: 38.55% 1.68 min
> Epoch: 39 Step: 3000, train loss: 0.0025 F1: 100.00% dev loss: 6.9736 F1: 38.43% 1.69 min
Initializing ELMo

>>>>>>>>>>>>>>>>>>>>>2021-07-21 09:55:40.709820>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 0.0005
>>> epochs: 50
>>> step: 100
>>> model_name: elmo
>>> downstream: crf
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/elmo-crf_res14_seed7.pth
>>> seed: 7
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: True
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 0.0005, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 17.0482 F1: 21.15% dev loss: 4.2881 F1: 27.72% 1.63 min
> Epoch: 2 Step: 200, train loss: 3.1714 F1: 43.65% dev loss: 3.3496 F1: 42.53% 1.69 min
> Epoch: 3 Step: 300, train loss: 1.5730 F1: 71.11% dev loss: 3.0979 F1: 46.65% 1.71 min
> Epoch: 5 Step: 400, train loss: 0.8918 F1: 87.74% dev loss: 3.5901 F1: 45.95% 1.68 min
> Epoch: 6 Step: 500, train loss: 0.7159 F1: 89.66% dev loss: 3.9810 F1: 43.94% 1.70 min
Initializing ELMo

>>>>>>>>>>>>>>>>>>>>>2021-07-21 10:04:59.098299>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 0.0005
>>> epochs: 50
>>> step: 100
>>> model_name: elmo
>>> downstream: crf
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/elmo-crf_res14_seed7.pth
>>> seed: 7
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: True
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 0.0005, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 17.3218 F1: 19.31% dev loss: 4.5595 F1: 23.62% 1.68 min
> Epoch: 2 Step: 200, train loss: 3.6249 F1: 37.86% dev loss: 3.5937 F1: 33.86% 1.69 min
> Epoch: 3 Step: 300, train loss: 2.0319 F1: 61.63% dev loss: 3.6622 F1: 42.99% 1.71 min
> Epoch: 5 Step: 400, train loss: 1.2278 F1: 79.87% dev loss: 4.3901 F1: 42.77% 1.68 min
> Epoch: 6 Step: 500, train loss: 1.0877 F1: 85.59% dev loss: 5.0585 F1: 36.79% 1.69 min
> Epoch: 7 Step: 600, train loss: 0.8077 F1: 89.02% dev loss: 5.0103 F1: 38.69% 1.70 min
> Epoch: 9 Step: 700, train loss: 0.4433 F1: 96.23% dev loss: 5.0907 F1: 42.90% 1.68 min
> Epoch: 10 Step: 800, train loss: 0.2530 F1: 96.89% dev loss: 5.2100 F1: 40.82% 1.69 min
> Epoch: 11 Step: 900, train loss: 0.1749 F1: 97.91% dev loss: 5.2052 F1: 43.27% 1.71 min
> Epoch: 13 Step: 1000, train loss: 0.1397 F1: 97.84% dev loss: 5.3982 F1: 40.42% 1.68 min
> Epoch: 14 Step: 1100, train loss: 0.1141 F1: 98.64% dev loss: 5.6002 F1: 41.86% 1.69 min
> Epoch: 15 Step: 1200, train loss: 0.1068 F1: 98.82% dev loss: 5.7641 F1: 41.94% 1.70 min
> Epoch: 17 Step: 1300, train loss: 0.0873 F1: 99.62% dev loss: 5.7862 F1: 41.57% 1.69 min
> Epoch: 18 Step: 1400, train loss: 0.0797 F1: 99.07% dev loss: 5.9482 F1: 42.65% 1.69 min
Initializing ELMo

>>>>>>>>>>>>>>>>>>>>>2021-07-21 10:30:20.184988>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 1e-05
>>> epochs: 50
>>> step: 100
>>> model_name: elmo
>>> downstream: crf
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/elmo-crf_res14_seed7.pth
>>> seed: 7
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: True
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 1e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 38.8192 F1: 10.06% dev loss: 35.6394 F1: 10.89% 1.68 min
> Epoch: 2 Step: 200, train loss: 32.8385 F1: 9.77% dev loss: 26.5463 F1: 11.10% 1.69 min
> Epoch: 3 Step: 300, train loss: 16.7125 F1: 9.78% dev loss: 9.9754 F1: 10.13% 1.70 min
> Epoch: 5 Step: 400, train loss: 8.8718 F1: 10.58% dev loss: 7.7570 F1: 10.87% 1.68 min
> Epoch: 6 Step: 500, train loss: 7.2143 F1: 11.63% dev loss: 6.3841 F1: 12.63% 1.70 min
> Epoch: 7 Step: 600, train loss: 5.8492 F1: 13.37% dev loss: 5.5817 F1: 13.82% 1.70 min
Initializing ELMo

>>>>>>>>>>>>>>>>>>>>>2021-07-21 10:42:25.250357>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 0.0001
>>> epochs: 50
>>> step: 100
>>> model_name: elmo
>>> downstream: crf
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/elmo-crf_res14_seed7.pth
>>> seed: 7
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: True
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 0.0001, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 29.4972 F1: 9.96% dev loss: 10.1716 F1: 11.10% 1.68 min
> Epoch: 2 Step: 200, train loss: 8.3974 F1: 10.72% dev loss: 7.8423 F1: 10.97% 1.69 min
> Epoch: 3 Step: 300, train loss: 6.6059 F1: 12.43% dev loss: 5.9428 F1: 13.48% 1.71 min
> Epoch: 5 Step: 400, train loss: 5.4533 F1: 16.93% dev loss: 5.2279 F1: 16.74% 1.68 min
> Epoch: 6 Step: 500, train loss: 4.9501 F1: 20.79% dev loss: 4.7815 F1: 20.80% 1.70 min
> Epoch: 7 Step: 600, train loss: 4.2704 F1: 23.75% dev loss: 4.4377 F1: 21.73% 1.69 min
> Epoch: 9 Step: 700, train loss: 3.8570 F1: 27.09% dev loss: 4.2188 F1: 24.68% 1.68 min
> Epoch: 10 Step: 800, train loss: 3.7457 F1: 30.45% dev loss: 4.0358 F1: 28.36% 1.70 min
> Epoch: 11 Step: 900, train loss: 3.2603 F1: 33.03% dev loss: 3.8926 F1: 29.29% 1.70 min
> Epoch: 13 Step: 1000, train loss: 2.9346 F1: 37.05% dev loss: 3.7965 F1: 29.55% 1.68 min
> Epoch: 14 Step: 1100, train loss: 2.8259 F1: 41.21% dev loss: 3.6717 F1: 31.18% 1.69 min
> Epoch: 15 Step: 1200, train loss: 2.5422 F1: 43.83% dev loss: 3.6357 F1: 32.73% 1.70 min
> Epoch: 17 Step: 1300, train loss: 2.3581 F1: 41.88% dev loss: 3.6015 F1: 33.20% 1.69 min
> Epoch: 18 Step: 1400, train loss: 2.2498 F1: 48.85% dev loss: 3.5155 F1: 34.30% 1.69 min
> Epoch: 19 Step: 1500, train loss: 1.9721 F1: 57.05% dev loss: 3.4397 F1: 35.28% 1.69 min
> Epoch: 21 Step: 1600, train loss: 1.6861 F1: 63.12% dev loss: 3.4643 F1: 33.42% 1.69 min
> Epoch: 22 Step: 1700, train loss: 1.5646 F1: 66.29% dev loss: 3.3838 F1: 34.91% 1.70 min
> Epoch: 23 Step: 1800, train loss: 1.3835 F1: 71.43% dev loss: 3.3723 F1: 37.85% 1.69 min
> Epoch: 24 Step: 1900, train loss: 1.2288 F1: 73.81% dev loss: 3.3825 F1: 37.91% 1.70 min
> Epoch: 26 Step: 2000, train loss: 1.1151 F1: 80.21% dev loss: 3.3566 F1: 37.84% 1.69 min
> Epoch: 27 Step: 2100, train loss: 1.0355 F1: 80.75% dev loss: 3.3636 F1: 46.80% 1.69 min
> Epoch: 28 Step: 2200, train loss: 0.8942 F1: 83.95% dev loss: 3.3705 F1: 45.88% 1.70 min
> Epoch: 30 Step: 2300, train loss: 0.8165 F1: 86.91% dev loss: 3.4034 F1: 39.44% 1.68 min
> Epoch: 31 Step: 2400, train loss: 0.7970 F1: 87.66% dev loss: 3.4186 F1: 47.94% 1.70 min
> Epoch: 32 Step: 2500, train loss: 0.7181 F1: 88.19% dev loss: 3.4062 F1: 44.78% 1.70 min
> Epoch: 34 Step: 2600, train loss: 0.6920 F1: 88.86% dev loss: 3.4152 F1: 44.29% 1.68 min
> Epoch: 35 Step: 2700, train loss: 0.6815 F1: 89.12% dev loss: 3.4110 F1: 42.60% 1.70 min
> Epoch: 36 Step: 2800, train loss: 0.6157 F1: 90.53% dev loss: 3.4217 F1: 39.91% 1.70 min
> Epoch: 38 Step: 2900, train loss: 0.5804 F1: 91.07% dev loss: 3.4316 F1: 45.18% 1.68 min
> Epoch: 39 Step: 3000, train loss: 0.5986 F1: 91.19% dev loss: 3.4207 F1: 44.44% 1.69 min

>>>>>>>>>>>>>>>>>>>>>2021-07-17 11:26:10.652208>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 2e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: CE
>>> gamma: 2.0
>>> alpha: 0.25
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 4000
>>> working_path: ./
>>> mode: res14
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> device: cuda
>>> class weight(or alpha) [0.07, 1.0, 1.0, 1.0, 1.0, 1.0, 1.1, 1.0, 1.0, 1.0]
> Epoch: 1 Step: 100, train loss: 2.1006 F1: 10.97% dev loss: 1.7612 F1: 16.79% 0.43 min
> Epoch: 2 Step: 200, train loss: 1.6157 F1: 20.76% dev loss: 1.2442 F1: 22.36% 0.43 min
> Epoch: 3 Step: 300, train loss: 1.0661 F1: 41.10% dev loss: 0.8429 F1: 40.28% 0.43 min
> Epoch: 5 Step: 400, train loss: 0.7041 F1: 56.83% dev loss: 0.7689 F1: 49.08% 0.43 min
> Epoch: 6 Step: 500, train loss: 0.4659 F1: 70.01% dev loss: 0.7787 F1: 48.82% 0.43 min
> Epoch: 7 Step: 600, train loss: 0.3372 F1: 76.63% dev loss: 0.7936 F1: 51.30% 0.43 min
> Epoch: 9 Step: 700, train loss: 0.2282 F1: 86.62% dev loss: 0.8387 F1: 51.40% 0.44 min
> Epoch: 10 Step: 800, train loss: 0.1674 F1: 89.35% dev loss: 0.9173 F1: 50.07% 0.44 min
> Epoch: 11 Step: 900, train loss: 0.1116 F1: 91.38% dev loss: 0.9297 F1: 54.75% 0.44 min
> Epoch: 13 Step: 1000, train loss: 0.0910 F1: 95.13% dev loss: 0.9389 F1: 53.64% 0.44 min
> Epoch: 14 Step: 1100, train loss: 0.0765 F1: 94.96% dev loss: 1.0164 F1: 53.80% 0.44 min
> Epoch: 15 Step: 1200, train loss: 0.0607 F1: 95.43% dev loss: 1.0235 F1: 55.01% 0.44 min
> Epoch: 17 Step: 1300, train loss: 0.0455 F1: 96.78% dev loss: 1.0380 F1: 54.63% 0.44 min
> Epoch: 18 Step: 1400, train loss: 0.0372 F1: 96.70% dev loss: 1.1065 F1: 54.09% 0.44 min
> Epoch: 19 Step: 1500, train loss: 0.0341 F1: 96.80% dev loss: 1.0999 F1: 56.29% 0.44 min
> Epoch: 21 Step: 1600, train loss: 0.0260 F1: 97.99% dev loss: 1.1071 F1: 54.67% 0.44 min
> Epoch: 22 Step: 1700, train loss: 0.0272 F1: 97.38% dev loss: 1.1427 F1: 54.87% 0.44 min
> Epoch: 23 Step: 1800, train loss: 0.0203 F1: 97.88% dev loss: 1.1467 F1: 55.28% 0.44 min
> Epoch: 25 Step: 1900, train loss: 0.0187 F1: 98.39% dev loss: 1.1673 F1: 56.10% 0.44 min
> Epoch: 26 Step: 2000, train loss: 0.0139 F1: 98.39% dev loss: 1.1647 F1: 55.68% 0.44 min
> Epoch: 27 Step: 2100, train loss: 0.0127 F1: 98.46% dev loss: 1.2231 F1: 54.87% 0.44 min
> Epoch: 29 Step: 2200, train loss: 0.0111 F1: 99.08% dev loss: 1.2237 F1: 56.14% 0.44 min
> Epoch: 30 Step: 2300, train loss: 0.0131 F1: 98.98% dev loss: 1.2115 F1: 55.53% 0.44 min
> Epoch: 31 Step: 2400, train loss: 0.0121 F1: 98.94% dev loss: 1.2698 F1: 55.44% 0.44 min
> Epoch: 33 Step: 2500, train loss: 0.0087 F1: 99.44% dev loss: 1.2840 F1: 56.26% 0.44 min
> Epoch: 34 Step: 2600, train loss: 0.0092 F1: 99.27% dev loss: 1.2578 F1: 54.96% 0.44 min
> Epoch: 35 Step: 2700, train loss: 0.0079 F1: 99.39% dev loss: 1.3127 F1: 56.42% 0.44 min
> Epoch: 37 Step: 2800, train loss: 0.0062 F1: 99.59% dev loss: 1.3147 F1: 56.73% 0.43 min
> Epoch: 38 Step: 2900, train loss: 0.0072 F1: 99.34% dev loss: 1.2671 F1: 54.89% 0.43 min
> Epoch: 39 Step: 3000, train loss: 0.0060 F1: 99.42% dev loss: 1.3305 F1: 55.48% 0.44 min
> Epoch: 41 Step: 3100, train loss: 0.0087 F1: 99.62% dev loss: 1.3615 F1: 56.82% 0.44 min
> Epoch: 42 Step: 3200, train loss: 0.0086 F1: 99.27% dev loss: 1.3120 F1: 55.93% 0.44 min
> Epoch: 43 Step: 3300, train loss: 0.0060 F1: 99.59% dev loss: 1.3365 F1: 55.30% 0.44 min
> Epoch: 45 Step: 3400, train loss: 0.0049 F1: 99.52% dev loss: 1.3769 F1: 57.68% 0.44 min
> Epoch: 46 Step: 3500, train loss: 0.0058 F1: 99.51% dev loss: 1.3176 F1: 55.55% 0.44 min

>>>>>>>>>>>>>>>>>>>>>2021-07-17 11:42:44.973012>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 2e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: CE
>>> gamma: 2.0
>>> alpha: 0.25
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> working_path: ./
>>> mode: res14
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> device: cuda
>>> class weight(or alpha) [0.05, 0.8, 1.0, 1.0, 1.2, 1.2, 1.2, 1.0, 1.0, 1.0]
> Epoch: 1 Step: 100, train loss: 2.1718 F1: 11.39% dev loss: 1.8936 F1: 13.56% 0.44 min
> Epoch: 2 Step: 200, train loss: 1.7413 F1: 21.57% dev loss: 1.3643 F1: 24.32% 0.43 min
> Epoch: 3 Step: 300, train loss: 1.1458 F1: 40.10% dev loss: 0.9369 F1: 39.33% 0.44 min
> Epoch: 5 Step: 400, train loss: 0.7433 F1: 57.45% dev loss: 0.8633 F1: 43.98% 0.44 min
> Epoch: 6 Step: 500, train loss: 0.4900 F1: 70.46% dev loss: 0.9208 F1: 45.79% 0.44 min
> Epoch: 7 Step: 600, train loss: 0.3446 F1: 76.63% dev loss: 0.9587 F1: 48.75% 0.44 min
> Epoch: 9 Step: 700, train loss: 0.2385 F1: 85.98% dev loss: 0.9460 F1: 51.59% 0.44 min
> Epoch: 10 Step: 800, train loss: 0.1844 F1: 88.01% dev loss: 1.1009 F1: 50.92% 0.44 min
> Epoch: 11 Step: 900, train loss: 0.1162 F1: 90.63% dev loss: 1.1018 F1: 52.52% 0.44 min
> Epoch: 13 Step: 1000, train loss: 0.0872 F1: 94.78% dev loss: 1.0639 F1: 52.08% 0.44 min
> Epoch: 14 Step: 1100, train loss: 0.0777 F1: 94.08% dev loss: 1.1816 F1: 54.40% 0.44 min
> Epoch: 15 Step: 1200, train loss: 0.0618 F1: 94.75% dev loss: 1.1277 F1: 54.62% 0.44 min
> Epoch: 17 Step: 1300, train loss: 0.0505 F1: 96.57% dev loss: 1.2033 F1: 55.18% 0.44 min
> Epoch: 18 Step: 1400, train loss: 0.0442 F1: 95.91% dev loss: 1.1758 F1: 55.43% 0.44 min
> Epoch: 19 Step: 1500, train loss: 0.0337 F1: 96.14% dev loss: 1.2813 F1: 55.00% 0.44 min
> Epoch: 21 Step: 1600, train loss: 0.0293 F1: 97.39% dev loss: 1.2764 F1: 54.49% 0.44 min
> Epoch: 22 Step: 1700, train loss: 0.0243 F1: 97.03% dev loss: 1.2792 F1: 53.98% 0.44 min
> Epoch: 23 Step: 1800, train loss: 0.0234 F1: 97.25% dev loss: 1.2688 F1: 53.84% 0.44 min
> Epoch: 25 Step: 1900, train loss: 0.0210 F1: 97.93% dev loss: 1.3164 F1: 56.05% 0.44 min
> Epoch: 26 Step: 2000, train loss: 0.0204 F1: 97.57% dev loss: 1.2506 F1: 55.81% 0.44 min
> Epoch: 27 Step: 2100, train loss: 0.0153 F1: 97.96% dev loss: 1.3320 F1: 55.87% 0.43 min
> Epoch: 29 Step: 2200, train loss: 0.0151 F1: 98.56% dev loss: 1.3754 F1: 56.16% 0.44 min
> Epoch: 30 Step: 2300, train loss: 0.0162 F1: 98.38% dev loss: 1.3284 F1: 54.37% 0.44 min
> Epoch: 31 Step: 2400, train loss: 0.0141 F1: 98.26% dev loss: 1.3487 F1: 54.37% 0.44 min
> Epoch: 33 Step: 2500, train loss: 0.0122 F1: 98.65% dev loss: 1.3806 F1: 55.76% 0.44 min
> Epoch: 34 Step: 2600, train loss: 0.0123 F1: 98.58% dev loss: 1.3509 F1: 55.40% 0.44 min
> Epoch: 35 Step: 2700, train loss: 0.0112 F1: 98.58% dev loss: 1.3723 F1: 55.02% 0.44 min
> Epoch: 37 Step: 2800, train loss: 0.0099 F1: 98.95% dev loss: 1.4090 F1: 55.32% 0.44 min
> Epoch: 38 Step: 2900, train loss: 0.0103 F1: 98.80% dev loss: 1.4106 F1: 55.06% 0.44 min
> Epoch: 39 Step: 3000, train loss: 0.0103 F1: 98.60% dev loss: 1.3990 F1: 55.20% 0.44 min
>> saved: checkout/state_dict/bert-linear_res14_final.pth

>>>>>>>>>>>>>>>>>>>>>2021-07-17 12:02:00.779758>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 2e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: CE
>>> gamma: 2.0
>>> alpha: 0.25
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3500
>>> working_path: ./
>>> mode: res14
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> device: cuda
>>> class weight(or alpha) [0.1, 0.8, 1.0, 1.0, 1.2, 1.2, 1.2, 1.0, 1.0, 1.0]
> Epoch: 1 Step: 100, train loss: 2.0444 F1: 10.54% dev loss: 1.6737 F1: 14.28% 0.44 min
> Epoch: 2 Step: 200, train loss: 1.5355 F1: 20.64% dev loss: 1.1846 F1: 22.69% 0.44 min
> Epoch: 3 Step: 300, train loss: 1.0168 F1: 36.19% dev loss: 0.7971 F1: 37.09% 0.44 min
> Epoch: 5 Step: 400, train loss: 0.6687 F1: 58.92% dev loss: 0.7044 F1: 46.60% 0.44 min
> Epoch: 6 Step: 500, train loss: 0.4388 F1: 71.73% dev loss: 0.7124 F1: 48.48% 0.44 min
> Epoch: 7 Step: 600, train loss: 0.3057 F1: 79.04% dev loss: 0.7438 F1: 53.31% 0.44 min
> Epoch: 9 Step: 700, train loss: 0.2116 F1: 88.33% dev loss: 0.7854 F1: 51.85% 0.44 min
> Epoch: 10 Step: 800, train loss: 0.1571 F1: 90.51% dev loss: 0.8260 F1: 53.97% 0.44 min
> Epoch: 11 Step: 900, train loss: 0.1055 F1: 92.81% dev loss: 0.9602 F1: 50.53% 0.44 min
> Epoch: 13 Step: 1000, train loss: 0.0889 F1: 94.19% dev loss: 0.9100 F1: 50.94% 0.44 min
> Epoch: 14 Step: 1100, train loss: 0.0878 F1: 94.60% dev loss: 0.9634 F1: 56.82% 0.44 min
> Epoch: 15 Step: 1200, train loss: 0.0598 F1: 96.07% dev loss: 0.9779 F1: 53.81% 0.44 min
> Epoch: 17 Step: 1300, train loss: 0.0452 F1: 97.18% dev loss: 1.0060 F1: 55.41% 0.44 min
> Epoch: 18 Step: 1400, train loss: 0.0368 F1: 97.69% dev loss: 0.9920 F1: 52.96% 0.44 min
> Epoch: 19 Step: 1500, train loss: 0.0283 F1: 97.72% dev loss: 1.0096 F1: 55.75% 0.44 min
> Epoch: 21 Step: 1600, train loss: 0.0243 F1: 98.68% dev loss: 1.0447 F1: 54.98% 0.44 min
> Epoch: 22 Step: 1700, train loss: 0.0225 F1: 98.02% dev loss: 1.0307 F1: 53.73% 0.44 min
> Epoch: 23 Step: 1800, train loss: 0.0192 F1: 98.49% dev loss: 1.0334 F1: 55.20% 0.44 min
> Epoch: 25 Step: 1900, train loss: 0.0172 F1: 98.87% dev loss: 1.0877 F1: 56.37% 0.44 min
> Epoch: 26 Step: 2000, train loss: 0.0126 F1: 99.01% dev loss: 1.0892 F1: 55.83% 0.44 min
> Epoch: 27 Step: 2100, train loss: 0.0119 F1: 99.06% dev loss: 1.1352 F1: 56.76% 0.44 min
> Epoch: 29 Step: 2200, train loss: 0.0120 F1: 99.42% dev loss: 1.1145 F1: 56.17% 0.44 min
> Epoch: 30 Step: 2300, train loss: 0.0123 F1: 99.16% dev loss: 1.1051 F1: 55.15% 0.44 min
> Epoch: 31 Step: 2400, train loss: 0.0104 F1: 99.33% dev loss: 1.1232 F1: 55.78% 0.44 min
> Epoch: 33 Step: 2500, train loss: 0.0083 F1: 99.51% dev loss: 1.1379 F1: 55.51% 0.44 min
> Epoch: 34 Step: 2600, train loss: 0.0080 F1: 99.44% dev loss: 1.1436 F1: 55.39% 0.44 min
> Epoch: 35 Step: 2700, train loss: 0.0084 F1: 99.55% dev loss: 1.1534 F1: 54.62% 0.44 min
> Epoch: 37 Step: 2800, train loss: 0.0075 F1: 99.70% dev loss: 1.1824 F1: 56.17% 0.44 min
> Epoch: 38 Step: 2900, train loss: 0.0067 F1: 99.55% dev loss: 1.1577 F1: 55.12% 0.44 min
> Epoch: 39 Step: 3000, train loss: 0.0061 F1: 99.51% dev loss: 1.1611 F1: 55.02% 0.44 min
> Epoch: 41 Step: 3100, train loss: 0.0060 F1: 99.62% dev loss: 1.1852 F1: 55.45% 0.44 min
> Epoch: 42 Step: 3200, train loss: 0.0056 F1: 99.65% dev loss: 1.1770 F1: 55.21% 0.44 min
> Epoch: 43 Step: 3300, train loss: 0.0053 F1: 99.58% dev loss: 1.1829 F1: 55.92% 0.44 min
> Epoch: 45 Step: 3400, train loss: 0.0047 F1: 99.71% dev loss: 1.1892 F1: 55.10% 0.44 min
> Epoch: 46 Step: 3500, train loss: 0.0056 F1: 99.61% dev loss: 1.1838 F1: 55.31% 0.44 min
>> saved: checkout/state_dict/bert-linear_res14_final.pth

>>>>>>>>>>>>>>>>>>>>>2021-07-17 12:30:07.686437>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 2e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'torch.optim.adam.Adam'>
>>> loss: CE
>>> gamma: 2.0
>>> alpha: 0.25
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3500
>>> working_path: ./
>>> mode: res14
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> device: cuda
>>> class weight(or alpha) [0.1, 0.8, 1.0, 1.0, 1.2, 1.2, 1.2, 1.0, 1.0, 1.0]
> Epoch: 1 Step: 100, train loss: 2.0425 F1: 10.14% dev loss: 1.6799 F1: 14.33% 0.45 min
> Epoch: 2 Step: 200, train loss: 1.5676 F1: 18.73% dev loss: 1.2385 F1: 23.44% 0.44 min
> Epoch: 3 Step: 300, train loss: 1.0845 F1: 31.09% dev loss: 0.8402 F1: 33.87% 0.44 min
> Epoch: 5 Step: 400, train loss: 0.7265 F1: 54.17% dev loss: 0.7273 F1: 45.73% 0.44 min
> Epoch: 6 Step: 500, train loss: 0.4914 F1: 68.76% dev loss: 0.6904 F1: 49.91% 0.44 min
> Epoch: 7 Step: 600, train loss: 0.3303 F1: 77.32% dev loss: 0.7337 F1: 53.12% 0.45 min
> Epoch: 9 Step: 700, train loss: 0.2429 F1: 85.70% dev loss: 0.7937 F1: 49.80% 0.45 min
> Epoch: 10 Step: 800, train loss: 0.1815 F1: 89.89% dev loss: 0.8354 F1: 53.36% 0.45 min
> Epoch: 11 Step: 900, train loss: 0.1197 F1: 91.92% dev loss: 0.9874 F1: 49.06% 0.45 min
> Epoch: 13 Step: 1000, train loss: 0.0891 F1: 94.32% dev loss: 0.8667 F1: 52.29% 0.45 min
> Epoch: 14 Step: 1100, train loss: 0.0820 F1: 94.78% dev loss: 0.9906 F1: 52.85% 0.45 min
> Epoch: 15 Step: 1200, train loss: 0.0622 F1: 95.08% dev loss: 0.9812 F1: 55.54% 0.45 min
> Epoch: 17 Step: 1300, train loss: 0.0530 F1: 97.05% dev loss: 0.9992 F1: 53.65% 0.45 min
> Epoch: 18 Step: 1400, train loss: 0.0412 F1: 97.22% dev loss: 0.9945 F1: 54.86% 0.45 min
> Epoch: 19 Step: 1500, train loss: 0.0361 F1: 97.26% dev loss: 1.0199 F1: 54.77% 0.45 min
> Epoch: 21 Step: 1600, train loss: 0.0266 F1: 98.25% dev loss: 1.0729 F1: 55.28% 0.45 min
> Epoch: 22 Step: 1700, train loss: 0.0260 F1: 98.05% dev loss: 1.0373 F1: 53.81% 0.45 min
> Epoch: 23 Step: 1800, train loss: 0.0235 F1: 98.03% dev loss: 1.1005 F1: 54.92% 0.45 min
> Epoch: 25 Step: 1900, train loss: 0.0174 F1: 98.54% dev loss: 1.1444 F1: 56.00% 0.45 min
> Epoch: 26 Step: 2000, train loss: 0.0163 F1: 99.09% dev loss: 1.1034 F1: 55.10% 0.45 min
> Epoch: 27 Step: 2100, train loss: 0.0128 F1: 98.82% dev loss: 1.1733 F1: 55.42% 0.45 min
> Epoch: 29 Step: 2200, train loss: 0.0147 F1: 99.27% dev loss: 1.1603 F1: 55.22% 0.45 min
> Epoch: 30 Step: 2300, train loss: 0.0139 F1: 99.18% dev loss: 1.1274 F1: 55.02% 0.45 min
> Epoch: 31 Step: 2400, train loss: 0.0123 F1: 99.32% dev loss: 1.1662 F1: 56.08% 0.45 min
> Epoch: 33 Step: 2500, train loss: 0.0095 F1: 99.35% dev loss: 1.2153 F1: 57.22% 0.44 min
> Epoch: 34 Step: 2600, train loss: 0.0103 F1: 99.29% dev loss: 1.1582 F1: 54.21% 0.44 min
> Epoch: 35 Step: 2700, train loss: 0.0088 F1: 99.40% dev loss: 1.1792 F1: 56.78% 0.44 min
> Epoch: 37 Step: 2800, train loss: 0.0077 F1: 99.62% dev loss: 1.2030 F1: 57.91% 0.44 min
> Epoch: 38 Step: 2900, train loss: 0.0069 F1: 99.50% dev loss: 1.1878 F1: 56.72% 0.45 min
> Epoch: 39 Step: 3000, train loss: 0.0075 F1: 99.46% dev loss: 1.2059 F1: 55.48% 0.45 min
> Epoch: 41 Step: 3100, train loss: 0.0061 F1: 99.48% dev loss: 1.2288 F1: 55.39% 0.45 min
> Epoch: 42 Step: 3200, train loss: 0.0077 F1: 99.43% dev loss: 1.2116 F1: 57.50% 0.45 min
> Epoch: 43 Step: 3300, train loss: 0.0070 F1: 99.58% dev loss: 1.2133 F1: 56.40% 0.45 min
> Epoch: 45 Step: 3400, train loss: 0.0055 F1: 99.57% dev loss: 1.2159 F1: 56.11% 0.44 min
> Epoch: 46 Step: 3500, train loss: 0.0059 F1: 99.65% dev loss: 1.2155 F1: 56.14% 0.44 min
>> saved: checkout/state_dict/bert-linear_res14_final.pth

>>>>>>>>>>>>>>>>>>>>>2021-07-17 12:55:30.809801>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 2e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'torch.optim.adam.Adam'>
>>> loss: CE
>>> gamma: 2.0
>>> alpha: 0.25
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3500
>>> working_path: ./
>>> mode: res14
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> device: cuda
>>> class weight(or alpha) [0.1, 0.8, 1.0, 1.0, 1.2, 1.2, 1.2, 1.0, 1.0, 1.0]
> Epoch: 1 Step: 100, train loss: 2.0427 F1: 10.68% dev loss: 1.6707 F1: 11.19% 0.45 min
> Epoch: 2 Step: 200, train loss: 1.5379 F1: 20.49% dev loss: 1.1918 F1: 22.85% 0.45 min
> Epoch: 3 Step: 300, train loss: 1.0237 F1: 35.47% dev loss: 0.8011 F1: 36.95% 0.45 min
> Epoch: 5 Step: 400, train loss: 0.6752 F1: 58.13% dev loss: 0.7082 F1: 47.34% 0.45 min
> Epoch: 6 Step: 500, train loss: 0.4440 F1: 71.37% dev loss: 0.7091 F1: 48.82% 0.45 min
> Epoch: 7 Step: 600, train loss: 0.3065 F1: 79.62% dev loss: 0.7372 F1: 53.66% 0.45 min
> Epoch: 9 Step: 700, train loss: 0.2102 F1: 88.25% dev loss: 0.8051 F1: 51.70% 0.45 min
> Epoch: 10 Step: 800, train loss: 0.1568 F1: 90.16% dev loss: 0.8411 F1: 53.51% 0.45 min
> Epoch: 11 Step: 900, train loss: 0.1038 F1: 92.85% dev loss: 0.9606 F1: 48.92% 0.45 min
> Epoch: 13 Step: 1000, train loss: 0.0904 F1: 94.24% dev loss: 0.9118 F1: 52.13% 0.45 min
> Epoch: 14 Step: 1100, train loss: 0.0908 F1: 94.61% dev loss: 0.9785 F1: 53.83% 0.45 min
> Epoch: 15 Step: 1200, train loss: 0.0627 F1: 96.03% dev loss: 0.9955 F1: 54.43% 0.45 min
> Epoch: 17 Step: 1300, train loss: 0.0438 F1: 97.59% dev loss: 1.0063 F1: 54.02% 0.45 min
> Epoch: 18 Step: 1400, train loss: 0.0317 F1: 97.83% dev loss: 0.9958 F1: 54.78% 0.45 min
> Epoch: 19 Step: 1500, train loss: 0.0291 F1: 97.77% dev loss: 1.0001 F1: 57.02% 0.45 min
> Epoch: 21 Step: 1600, train loss: 0.0222 F1: 98.51% dev loss: 1.0343 F1: 54.30% 0.45 min
> Epoch: 22 Step: 1700, train loss: 0.0227 F1: 98.29% dev loss: 1.0124 F1: 54.94% 0.45 min
> Epoch: 23 Step: 1800, train loss: 0.0183 F1: 98.54% dev loss: 1.0441 F1: 55.51% 0.45 min
> Epoch: 25 Step: 1900, train loss: 0.0157 F1: 98.95% dev loss: 1.0996 F1: 55.37% 0.45 min
> Epoch: 26 Step: 2000, train loss: 0.0137 F1: 98.99% dev loss: 1.0776 F1: 55.87% 0.45 min
> Epoch: 27 Step: 2100, train loss: 0.0126 F1: 99.11% dev loss: 1.1056 F1: 56.08% 0.45 min
> Epoch: 29 Step: 2200, train loss: 0.0115 F1: 99.57% dev loss: 1.1136 F1: 56.86% 0.45 min
> Epoch: 30 Step: 2300, train loss: 0.0123 F1: 99.26% dev loss: 1.0928 F1: 55.35% 0.45 min
> Epoch: 31 Step: 2400, train loss: 0.0108 F1: 99.36% dev loss: 1.1329 F1: 56.89% 0.45 min
> Epoch: 33 Step: 2500, train loss: 0.0094 F1: 99.56% dev loss: 1.1500 F1: 56.26% 0.45 min
> Epoch: 34 Step: 2600, train loss: 0.0108 F1: 99.27% dev loss: 1.1093 F1: 54.38% 0.45 min
> Epoch: 35 Step: 2700, train loss: 0.0080 F1: 99.56% dev loss: 1.1396 F1: 55.50% 0.45 min
> Epoch: 37 Step: 2800, train loss: 0.0077 F1: 99.55% dev loss: 1.1585 F1: 56.71% 0.45 min
> Epoch: 38 Step: 2900, train loss: 0.0074 F1: 99.53% dev loss: 1.1429 F1: 57.21% 0.45 min
> Epoch: 39 Step: 3000, train loss: 0.0074 F1: 99.41% dev loss: 1.1470 F1: 55.83% 0.45 min
> Epoch: 41 Step: 3100, train loss: 0.0078 F1: 99.53% dev loss: 1.1608 F1: 56.08% 0.45 min
> Epoch: 42 Step: 3200, train loss: 0.0073 F1: 99.56% dev loss: 1.1648 F1: 56.78% 0.45 min
> Epoch: 43 Step: 3300, train loss: 0.0068 F1: 99.56% dev loss: 1.1667 F1: 56.56% 0.45 min
> Epoch: 45 Step: 3400, train loss: 0.0065 F1: 99.44% dev loss: 1.1733 F1: 57.00% 0.45 min
> Epoch: 46 Step: 3500, train loss: 0.0072 F1: 99.59% dev loss: 1.1661 F1: 56.91% 0.45 min

>>>>>>>>>>>>>>>>>>>>>2021-07-18 05:16:17.865685>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 2e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.25
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> working_path: ./
>>> mode: res14
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> device: cuda
>>> class weight(or alpha) [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]
> Epoch: 1 Step: 100, train loss: 0.2479 F1: 9.32% dev loss: 0.0966 F1: 9.40% 0.43 min
> Epoch: 2 Step: 200, train loss: 0.0927 F1: 9.32% dev loss: 0.0725 F1: 9.40% 0.43 min
> Epoch: 3 Step: 300, train loss: 0.0691 F1: 17.29% dev loss: 0.0527 F1: 22.32% 0.43 min
> Epoch: 5 Step: 400, train loss: 0.0504 F1: 30.58% dev loss: 0.0422 F1: 33.51% 0.43 min
> Epoch: 6 Step: 500, train loss: 0.0348 F1: 50.38% dev loss: 0.0337 F1: 48.93% 0.43 min
> Epoch: 7 Step: 600, train loss: 0.0221 F1: 60.67% dev loss: 0.0317 F1: 50.66% 0.44 min
> Epoch: 9 Step: 700, train loss: 0.0154 F1: 74.41% dev loss: 0.0388 F1: 49.35% 0.44 min
> Epoch: 10 Step: 800, train loss: 0.0115 F1: 81.51% dev loss: 0.0353 F1: 53.17% 0.44 min
> Epoch: 11 Step: 900, train loss: 0.0084 F1: 84.19% dev loss: 0.0323 F1: 53.90% 0.44 min
> Epoch: 13 Step: 1000, train loss: 0.0066 F1: 89.74% dev loss: 0.0366 F1: 54.68% 0.44 min
> Epoch: 14 Step: 1100, train loss: 0.0053 F1: 91.00% dev loss: 0.0379 F1: 54.73% 0.44 min
> Epoch: 15 Step: 1200, train loss: 0.0046 F1: 91.10% dev loss: 0.0377 F1: 55.13% 0.44 min
> Epoch: 17 Step: 1300, train loss: 0.0038 F1: 93.98% dev loss: 0.0388 F1: 55.99% 0.44 min
> Epoch: 18 Step: 1400, train loss: 0.0032 F1: 94.57% dev loss: 0.0380 F1: 55.21% 0.44 min
> Epoch: 19 Step: 1500, train loss: 0.0028 F1: 94.07% dev loss: 0.0399 F1: 54.63% 0.44 min
> Epoch: 21 Step: 1600, train loss: 0.0022 F1: 95.26% dev loss: 0.0403 F1: 55.87% 0.44 min
> Epoch: 22 Step: 1700, train loss: 0.0018 F1: 96.25% dev loss: 0.0428 F1: 56.44% 0.44 min
> Epoch: 23 Step: 1800, train loss: 0.0015 F1: 96.68% dev loss: 0.0428 F1: 55.32% 0.44 min
> Epoch: 25 Step: 1900, train loss: 0.0013 F1: 97.79% dev loss: 0.0433 F1: 54.32% 0.44 min
> Epoch: 26 Step: 2000, train loss: 0.0011 F1: 97.81% dev loss: 0.0459 F1: 55.02% 0.44 min
> Epoch: 27 Step: 2100, train loss: 0.0009 F1: 97.88% dev loss: 0.0459 F1: 56.78% 0.44 min
> Epoch: 29 Step: 2200, train loss: 0.0009 F1: 98.38% dev loss: 0.0436 F1: 55.80% 0.44 min
> Epoch: 30 Step: 2300, train loss: 0.0008 F1: 98.70% dev loss: 0.0469 F1: 55.57% 0.44 min
> Epoch: 31 Step: 2400, train loss: 0.0008 F1: 98.41% dev loss: 0.0444 F1: 55.60% 0.44 min
> Epoch: 33 Step: 2500, train loss: 0.0006 F1: 98.82% dev loss: 0.0444 F1: 55.70% 0.44 min
> Epoch: 34 Step: 2600, train loss: 0.0007 F1: 98.64% dev loss: 0.0491 F1: 53.95% 0.44 min
> Epoch: 35 Step: 2700, train loss: 0.0006 F1: 98.64% dev loss: 0.0467 F1: 55.38% 0.44 min
> Epoch: 37 Step: 2800, train loss: 0.0005 F1: 99.08% dev loss: 0.0462 F1: 55.22% 0.44 min
> Epoch: 38 Step: 2900, train loss: 0.0006 F1: 98.76% dev loss: 0.0470 F1: 56.19% 0.44 min
> Epoch: 39 Step: 3000, train loss: 0.0006 F1: 98.77% dev loss: 0.0471 F1: 56.12% 0.44 min

>>>>>>>>>>>>>>>>>>>>>2021-07-18 05:31:25.477813>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 2e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 1.0
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> working_path: ./
>>> mode: res14
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> device: cuda
>>> class weight(or alpha) [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
> Epoch: 1 Step: 100, train loss: 0.9851 F1: 9.32% dev loss: 0.3846 F1: 9.40% 0.43 min
> Epoch: 2 Step: 200, train loss: 0.3693 F1: 13.05% dev loss: 0.2873 F1: 9.40% 0.43 min
> Epoch: 3 Step: 300, train loss: 0.2733 F1: 18.02% dev loss: 0.2086 F1: 22.30% 0.43 min
> Epoch: 5 Step: 400, train loss: 0.1987 F1: 31.37% dev loss: 0.1667 F1: 33.24% 0.43 min
> Epoch: 6 Step: 500, train loss: 0.1359 F1: 50.42% dev loss: 0.1338 F1: 48.92% 0.44 min
> Epoch: 7 Step: 600, train loss: 0.0862 F1: 61.57% dev loss: 0.1274 F1: 51.00% 0.44 min
> Epoch: 9 Step: 700, train loss: 0.0596 F1: 75.34% dev loss: 0.1553 F1: 49.65% 0.44 min
> Epoch: 10 Step: 800, train loss: 0.0445 F1: 82.13% dev loss: 0.1404 F1: 53.71% 0.44 min
> Epoch: 11 Step: 900, train loss: 0.0326 F1: 85.00% dev loss: 0.1296 F1: 54.83% 0.44 min
> Epoch: 13 Step: 1000, train loss: 0.0255 F1: 90.35% dev loss: 0.1460 F1: 54.73% 0.44 min
> Epoch: 14 Step: 1100, train loss: 0.0203 F1: 91.22% dev loss: 0.1536 F1: 54.20% 0.44 min
> Epoch: 15 Step: 1200, train loss: 0.0177 F1: 91.23% dev loss: 0.1520 F1: 56.27% 0.44 min
> Epoch: 17 Step: 1300, train loss: 0.0146 F1: 93.73% dev loss: 0.1573 F1: 55.73% 0.44 min
> Epoch: 18 Step: 1400, train loss: 0.0123 F1: 94.74% dev loss: 0.1535 F1: 56.05% 0.44 min
> Epoch: 19 Step: 1500, train loss: 0.0109 F1: 94.28% dev loss: 0.1614 F1: 53.95% 0.44 min
> Epoch: 21 Step: 1600, train loss: 0.0082 F1: 96.16% dev loss: 0.1626 F1: 55.60% 0.44 min
> Epoch: 22 Step: 1700, train loss: 0.0067 F1: 96.17% dev loss: 0.1744 F1: 56.20% 0.44 min
> Epoch: 23 Step: 1800, train loss: 0.0058 F1: 96.75% dev loss: 0.1710 F1: 55.76% 0.44 min
> Epoch: 25 Step: 1900, train loss: 0.0050 F1: 97.78% dev loss: 0.1769 F1: 54.38% 0.44 min
> Epoch: 26 Step: 2000, train loss: 0.0041 F1: 97.95% dev loss: 0.1857 F1: 55.42% 0.44 min
> Epoch: 27 Step: 2100, train loss: 0.0033 F1: 97.92% dev loss: 0.1861 F1: 56.72% 0.44 min
> Epoch: 29 Step: 2200, train loss: 0.0034 F1: 98.49% dev loss: 0.1763 F1: 56.20% 0.44 min
> Epoch: 30 Step: 2300, train loss: 0.0031 F1: 98.89% dev loss: 0.1886 F1: 56.29% 0.44 min
> Epoch: 31 Step: 2400, train loss: 0.0029 F1: 98.51% dev loss: 0.1779 F1: 55.96% 0.44 min
> Epoch: 33 Step: 2500, train loss: 0.0024 F1: 98.81% dev loss: 0.1786 F1: 56.04% 0.44 min
> Epoch: 34 Step: 2600, train loss: 0.0026 F1: 98.71% dev loss: 0.1982 F1: 53.62% 0.44 min
> Epoch: 35 Step: 2700, train loss: 0.0023 F1: 98.68% dev loss: 0.1884 F1: 56.07% 0.44 min
> Epoch: 37 Step: 2800, train loss: 0.0018 F1: 99.43% dev loss: 0.1854 F1: 55.57% 0.44 min
> Epoch: 38 Step: 2900, train loss: 0.0020 F1: 98.76% dev loss: 0.1888 F1: 56.10% 0.44 min
> Epoch: 39 Step: 3000, train loss: 0.0021 F1: 98.91% dev loss: 0.1892 F1: 56.03% 0.44 min


>>>>>>>>>>>>>>>>>>>>>2021-07-18 05:57:05.630013>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 2e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3500
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-06
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-06, 'lr': 2e-05, 'weight_decay': 0.0001}
>>> device: cuda
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.7390 F1: 9.32% dev loss: 0.2880 F1: 9.40% 0.44 min
> Epoch: 2 Step: 200, train loss: 0.2769 F1: 13.05% dev loss: 0.2156 F1: 9.40% 0.43 min
> Epoch: 3 Step: 300, train loss: 0.2049 F1: 17.78% dev loss: 0.1564 F1: 22.48% 0.43 min
> Epoch: 5 Step: 400, train loss: 0.1486 F1: 30.59% dev loss: 0.1246 F1: 33.43% 0.44 min
> Epoch: 6 Step: 500, train loss: 0.1017 F1: 51.05% dev loss: 0.1003 F1: 48.71% 0.44 min
> Epoch: 7 Step: 600, train loss: 0.0643 F1: 61.78% dev loss: 0.0949 F1: 50.89% 0.44 min
> Epoch: 9 Step: 700, train loss: 0.0447 F1: 75.48% dev loss: 0.1173 F1: 48.86% 0.44 min
> Epoch: 10 Step: 800, train loss: 0.0331 F1: 82.56% dev loss: 0.1063 F1: 53.47% 0.44 min
> Epoch: 11 Step: 900, train loss: 0.0242 F1: 85.12% dev loss: 0.0973 F1: 54.04% 0.44 min
> Epoch: 13 Step: 1000, train loss: 0.0188 F1: 90.13% dev loss: 0.1104 F1: 54.12% 0.44 min
> Epoch: 14 Step: 1100, train loss: 0.0149 F1: 91.38% dev loss: 0.1157 F1: 54.11% 0.44 min
> Epoch: 15 Step: 1200, train loss: 0.0130 F1: 91.55% dev loss: 0.1160 F1: 53.49% 0.44 min
> Epoch: 17 Step: 1300, train loss: 0.0106 F1: 94.11% dev loss: 0.1189 F1: 55.88% 0.44 min
> Epoch: 18 Step: 1400, train loss: 0.0090 F1: 94.66% dev loss: 0.1163 F1: 54.80% 0.44 min
> Epoch: 19 Step: 1500, train loss: 0.0082 F1: 93.96% dev loss: 0.1224 F1: 54.21% 0.44 min
> Epoch: 21 Step: 1600, train loss: 0.0061 F1: 96.49% dev loss: 0.1228 F1: 54.99% 0.44 min
> Epoch: 22 Step: 1700, train loss: 0.0047 F1: 96.55% dev loss: 0.1306 F1: 55.62% 0.44 min
> Epoch: 23 Step: 1800, train loss: 0.0041 F1: 97.12% dev loss: 0.1296 F1: 56.53% 0.44 min
> Epoch: 25 Step: 1900, train loss: 0.0034 F1: 98.04% dev loss: 0.1335 F1: 54.17% 0.44 min
> Epoch: 26 Step: 2000, train loss: 0.0028 F1: 98.37% dev loss: 0.1391 F1: 55.75% 0.44 min
> Epoch: 27 Step: 2100, train loss: 0.0022 F1: 98.11% dev loss: 0.1392 F1: 57.38% 0.44 min
> Epoch: 29 Step: 2200, train loss: 0.0022 F1: 98.85% dev loss: 0.1345 F1: 56.38% 0.44 min
> Epoch: 30 Step: 2300, train loss: 0.0020 F1: 99.02% dev loss: 0.1443 F1: 55.63% 0.44 min
> Epoch: 31 Step: 2400, train loss: 0.0019 F1: 98.57% dev loss: 0.1355 F1: 56.38% 0.44 min
> Epoch: 33 Step: 2500, train loss: 0.0015 F1: 98.86% dev loss: 0.1374 F1: 55.80% 0.44 min
> Epoch: 34 Step: 2600, train loss: 0.0016 F1: 99.09% dev loss: 0.1515 F1: 53.26% 0.44 min
> Epoch: 35 Step: 2700, train loss: 0.0013 F1: 99.13% dev loss: 0.1425 F1: 54.48% 0.44 min
> Epoch: 37 Step: 2800, train loss: 0.0010 F1: 99.23% dev loss: 0.1408 F1: 55.13% 0.44 min
> Epoch: 38 Step: 2900, train loss: 0.0012 F1: 99.35% dev loss: 0.1516 F1: 55.45% 0.44 min
> Epoch: 39 Step: 3000, train loss: 0.0012 F1: 99.15% dev loss: 0.1415 F1: 55.75% 0.44 min
> Epoch: 41 Step: 3100, train loss: 0.0011 F1: 99.61% dev loss: 0.1398 F1: 55.30% 0.44 min
> Epoch: 42 Step: 3200, train loss: 0.0010 F1: 99.27% dev loss: 0.1461 F1: 55.95% 0.44 min
> Epoch: 43 Step: 3300, train loss: 0.0008 F1: 99.53% dev loss: 0.1462 F1: 55.90% 0.44 min
> Epoch: 45 Step: 3400, train loss: 0.0008 F1: 99.36% dev loss: 0.1451 F1: 55.88% 0.44 min
> Epoch: 46 Step: 3500, train loss: 0.0010 F1: 99.31% dev loss: 0.1462 F1: 55.21% 0.44 min

>>>>>>>>>>>>>>>>>>>>>2021-07-18 06:14:50.384279>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-06
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.25
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3500
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-06, 'weight_decay': 0.0001}
>>> device: cuda
>>> class weight(or alpha) [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]
> Epoch: 1 Step: 100, train loss: 0.3738 F1: 9.29% dev loss: 0.1697 F1: 9.36% 0.44 min
> Epoch: 2 Step: 200, train loss: 0.1218 F1: 9.32% dev loss: 0.0949 F1: 9.40% 0.43 min
> Epoch: 3 Step: 300, train loss: 0.0947 F1: 9.34% dev loss: 0.0818 F1: 9.40% 0.44 min
> Epoch: 5 Step: 400, train loss: 0.0853 F1: 9.87% dev loss: 0.0706 F1: 13.16% 0.44 min
> Epoch: 6 Step: 500, train loss: 0.0749 F1: 14.72% dev loss: 0.0601 F1: 20.24% 0.44 min
> Epoch: 7 Step: 600, train loss: 0.0611 F1: 22.52% dev loss: 0.0526 F1: 22.18% 0.44 min
> Epoch: 9 Step: 700, train loss: 0.0540 F1: 24.74% dev loss: 0.0493 F1: 26.44% 0.44 min
> Epoch: 10 Step: 800, train loss: 0.0474 F1: 33.97% dev loss: 0.0422 F1: 32.17% 0.44 min
> Epoch: 11 Step: 900, train loss: 0.0396 F1: 34.61% dev loss: 0.0388 F1: 33.95% 0.44 min
> Epoch: 13 Step: 1000, train loss: 0.0351 F1: 41.13% dev loss: 0.0379 F1: 41.42% 0.44 min
> Epoch: 14 Step: 1100, train loss: 0.0316 F1: 49.01% dev loss: 0.0343 F1: 42.51% 0.44 min
> Epoch: 15 Step: 1200, train loss: 0.0275 F1: 50.56% dev loss: 0.0329 F1: 46.81% 0.44 min
> Epoch: 17 Step: 1300, train loss: 0.0251 F1: 55.62% dev loss: 0.0358 F1: 48.31% 0.44 min
> Epoch: 18 Step: 1400, train loss: 0.0225 F1: 62.14% dev loss: 0.0320 F1: 50.05% 0.44 min
> Epoch: 19 Step: 1500, train loss: 0.0199 F1: 63.50% dev loss: 0.0315 F1: 51.84% 0.44 min
> Epoch: 21 Step: 1600, train loss: 0.0181 F1: 68.32% dev loss: 0.0340 F1: 50.85% 0.44 min
> Epoch: 22 Step: 1700, train loss: 0.0171 F1: 69.41% dev loss: 0.0316 F1: 51.25% 0.44 min
> Epoch: 23 Step: 1800, train loss: 0.0154 F1: 73.50% dev loss: 0.0318 F1: 52.06% 0.44 min
> Epoch: 25 Step: 1900, train loss: 0.0142 F1: 74.52% dev loss: 0.0331 F1: 51.60% 0.44 min
> Epoch: 26 Step: 2000, train loss: 0.0135 F1: 76.16% dev loss: 0.0329 F1: 51.75% 0.44 min
> Epoch: 27 Step: 2100, train loss: 0.0127 F1: 77.40% dev loss: 0.0320 F1: 51.49% 0.44 min
> Epoch: 29 Step: 2200, train loss: 0.0118 F1: 78.64% dev loss: 0.0328 F1: 51.74% 0.44 min
> Epoch: 30 Step: 2300, train loss: 0.0115 F1: 80.35% dev loss: 0.0340 F1: 51.50% 0.44 min
> Epoch: 31 Step: 2400, train loss: 0.0109 F1: 80.21% dev loss: 0.0330 F1: 52.61% 0.44 min
> Epoch: 33 Step: 2500, train loss: 0.0101 F1: 81.56% dev loss: 0.0339 F1: 51.86% 0.44 min
> Epoch: 34 Step: 2600, train loss: 0.0097 F1: 81.85% dev loss: 0.0350 F1: 50.98% 0.44 min
> Epoch: 35 Step: 2700, train loss: 0.0094 F1: 83.07% dev loss: 0.0330 F1: 51.61% 0.44 min
> Epoch: 37 Step: 2800, train loss: 0.0089 F1: 84.38% dev loss: 0.0344 F1: 52.43% 0.44 min
> Epoch: 38 Step: 2900, train loss: 0.0088 F1: 84.59% dev loss: 0.0363 F1: 51.41% 0.44 min
> Epoch: 39 Step: 3000, train loss: 0.0086 F1: 84.37% dev loss: 0.0340 F1: 51.94% 0.44 min
> Epoch: 41 Step: 3100, train loss: 0.0083 F1: 85.37% dev loss: 0.0339 F1: 51.97% 0.44 min
> Epoch: 42 Step: 3200, train loss: 0.0083 F1: 84.85% dev loss: 0.0362 F1: 51.11% 0.44 min
> Epoch: 43 Step: 3300, train loss: 0.0079 F1: 85.45% dev loss: 0.0352 F1: 51.12% 0.44 min
> Epoch: 45 Step: 3400, train loss: 0.0078 F1: 87.16% dev loss: 0.0344 F1: 51.61% 0.44 min
> Epoch: 46 Step: 3500, train loss: 0.0079 F1: 84.78% dev loss: 0.0349 F1: 51.63% 0.44 min

>>>>>>>>>>>>>>>>>>>>>2021-07-18 06:34:23.081238>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 2.0
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3500
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda
>>> class weight(or alpha) [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0]
> Epoch: 1 Step: 100, train loss: 1.3376 F1: 9.32% dev loss: 0.5431 F1: 9.40% 0.44 min
> Epoch: 2 Step: 200, train loss: 0.5076 F1: 19.29% dev loss: 0.3430 F1: 23.53% 0.43 min
> Epoch: 3 Step: 300, train loss: 0.2868 F1: 39.82% dev loss: 0.2286 F1: 46.30% 0.43 min
> Epoch: 5 Step: 400, train loss: 0.1600 F1: 61.97% dev loss: 0.2320 F1: 51.91% 0.44 min
> Epoch: 6 Step: 500, train loss: 0.0933 F1: 74.40% dev loss: 0.2133 F1: 50.55% 0.44 min
> Epoch: 7 Step: 600, train loss: 0.0552 F1: 82.64% dev loss: 0.2330 F1: 54.75% 0.44 min
> Epoch: 9 Step: 700, train loss: 0.0314 F1: 91.25% dev loss: 0.3066 F1: 52.77% 0.44 min
> Epoch: 10 Step: 800, train loss: 0.0238 F1: 92.85% dev loss: 0.2747 F1: 54.55% 0.44 min
> Epoch: 11 Step: 900, train loss: 0.0151 F1: 94.50% dev loss: 0.2492 F1: 55.80% 0.44 min
> Epoch: 13 Step: 1000, train loss: 0.0097 F1: 95.43% dev loss: 0.2516 F1: 57.64% 0.44 min
> Epoch: 14 Step: 1100, train loss: 0.0078 F1: 97.30% dev loss: 0.2849 F1: 57.05% 0.44 min
> Epoch: 15 Step: 1200, train loss: 0.0055 F1: 97.82% dev loss: 0.2629 F1: 58.52% 0.44 min
> Epoch: 17 Step: 1300, train loss: 0.0038 F1: 98.53% dev loss: 0.2669 F1: 58.12% 0.44 min
> Epoch: 18 Step: 1400, train loss: 0.0035 F1: 98.02% dev loss: 0.2968 F1: 56.07% 0.44 min
> Epoch: 19 Step: 1500, train loss: 0.0026 F1: 98.70% dev loss: 0.2756 F1: 58.45% 0.44 min
> Epoch: 21 Step: 1600, train loss: 0.0020 F1: 99.33% dev loss: 0.2868 F1: 58.04% 0.44 min
> Epoch: 22 Step: 1700, train loss: 0.0014 F1: 99.29% dev loss: 0.3039 F1: 57.74% 0.44 min
> Epoch: 23 Step: 1800, train loss: 0.0019 F1: 99.38% dev loss: 0.2892 F1: 58.54% 0.44 min
> Epoch: 25 Step: 1900, train loss: 0.0009 F1: 99.53% dev loss: 0.3062 F1: 56.92% 0.44 min
> Epoch: 26 Step: 2000, train loss: 0.0010 F1: 99.70% dev loss: 0.3149 F1: 56.69% 0.44 min
> Epoch: 27 Step: 2100, train loss: 0.0008 F1: 99.66% dev loss: 0.3285 F1: 56.46% 0.44 min
> Epoch: 29 Step: 2200, train loss: 0.0010 F1: 99.86% dev loss: 0.3145 F1: 58.34% 0.44 min
> Epoch: 30 Step: 2300, train loss: 0.0009 F1: 99.75% dev loss: 0.3253 F1: 56.30% 0.44 min
> Epoch: 31 Step: 2400, train loss: 0.0006 F1: 99.72% dev loss: 0.3349 F1: 58.67% 0.44 min
> Epoch: 33 Step: 2500, train loss: 0.0006 F1: 99.95% dev loss: 0.3175 F1: 59.41% 0.44 min
> Epoch: 34 Step: 2600, train loss: 0.0006 F1: 99.79% dev loss: 0.3397 F1: 56.59% 0.44 min
> Epoch: 35 Step: 2700, train loss: 0.0007 F1: 99.76% dev loss: 0.3289 F1: 56.70% 0.44 min
> Epoch: 37 Step: 2800, train loss: 0.0004 F1: 99.87% dev loss: 0.3245 F1: 58.88% 0.44 min
> Epoch: 38 Step: 2900, train loss: 0.0004 F1: 99.81% dev loss: 0.3235 F1: 58.25% 0.44 min
> Epoch: 39 Step: 3000, train loss: 0.0006 F1: 99.78% dev loss: 0.3305 F1: 58.82% 0.44 min
> Epoch: 41 Step: 3100, train loss: 0.0004 F1: 99.99% dev loss: 0.3246 F1: 58.64% 0.44 min
> Epoch: 42 Step: 3200, train loss: 0.0003 F1: 99.97% dev loss: 0.3278 F1: 58.71% 0.44 min
> Epoch: 43 Step: 3300, train loss: 0.0002 F1: 99.96% dev loss: 0.3303 F1: 58.49% 0.44 min
> Epoch: 45 Step: 3400, train loss: 0.0002 F1: 99.92% dev loss: 0.3293 F1: 58.69% 0.44 min
> Epoch: 46 Step: 3500, train loss: 0.0003 F1: 99.89% dev loss: 0.3264 F1: 58.50% 0.44 min

>>>>>>>>>>>>>>>>>>>>>2021-07-18 07:00:59.625017>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.25
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3500
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda
>>> class weight(or alpha) [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]
> Epoch: 1 Step: 100, train loss: 0.1923 F1: 9.32% dev loss: 0.0794 F1: 9.40% 0.43 min
> Epoch: 2 Step: 200, train loss: 0.0735 F1: 20.11% dev loss: 0.0500 F1: 23.65% 0.43 min
> Epoch: 3 Step: 300, train loss: 0.0424 F1: 38.92% dev loss: 0.0351 F1: 46.48% 0.43 min
> Epoch: 5 Step: 400, train loss: 0.0243 F1: 63.69% dev loss: 0.0334 F1: 51.35% 0.43 min
> Epoch: 6 Step: 500, train loss: 0.0151 F1: 73.82% dev loss: 0.0360 F1: 51.86% 0.44 min
> Epoch: 7 Step: 600, train loss: 0.0096 F1: 82.62% dev loss: 0.0334 F1: 56.94% 0.44 min
> Epoch: 9 Step: 700, train loss: 0.0058 F1: 92.16% dev loss: 0.0431 F1: 52.83% 0.44 min
> Epoch: 10 Step: 800, train loss: 0.0039 F1: 94.00% dev loss: 0.0383 F1: 56.61% 0.44 min
> Epoch: 11 Step: 900, train loss: 0.0023 F1: 95.54% dev loss: 0.0343 F1: 56.56% 0.44 min
> Epoch: 13 Step: 1000, train loss: 0.0017 F1: 97.34% dev loss: 0.0390 F1: 55.90% 0.44 min
> Epoch: 14 Step: 1100, train loss: 0.0012 F1: 98.18% dev loss: 0.0456 F1: 55.01% 0.44 min
> Epoch: 15 Step: 1200, train loss: 0.0008 F1: 98.59% dev loss: 0.0422 F1: 56.79% 0.44 min
> Epoch: 17 Step: 1300, train loss: 0.0006 F1: 99.57% dev loss: 0.0434 F1: 59.78% 0.44 min
> Epoch: 18 Step: 1400, train loss: 0.0006 F1: 98.72% dev loss: 0.0459 F1: 55.89% 0.44 min
> Epoch: 19 Step: 1500, train loss: 0.0003 F1: 99.47% dev loss: 0.0449 F1: 57.42% 0.44 min
> Epoch: 21 Step: 1600, train loss: 0.0002 F1: 99.66% dev loss: 0.0429 F1: 57.51% 0.44 min
> Epoch: 22 Step: 1700, train loss: 0.0002 F1: 99.51% dev loss: 0.0496 F1: 56.84% 0.44 min
> Epoch: 23 Step: 1800, train loss: 0.0002 F1: 99.52% dev loss: 0.0461 F1: 58.96% 0.44 min
> Epoch: 25 Step: 1900, train loss: 0.0001 F1: 99.47% dev loss: 0.0458 F1: 59.29% 0.44 min
> Epoch: 26 Step: 2000, train loss: 0.0002 F1: 99.67% dev loss: 0.0456 F1: 58.35% 0.44 min
> Epoch: 27 Step: 2100, train loss: 0.0001 F1: 99.85% dev loss: 0.0475 F1: 57.99% 0.44 min
> Epoch: 29 Step: 2200, train loss: 0.0001 F1: 99.98% dev loss: 0.0457 F1: 59.48% 0.44 min
> Epoch: 30 Step: 2300, train loss: 0.0001 F1: 99.86% dev loss: 0.0477 F1: 58.74% 0.44 min
> Epoch: 31 Step: 2400, train loss: 0.0001 F1: 99.98% dev loss: 0.0482 F1: 59.27% 0.44 min
> Epoch: 33 Step: 2500, train loss: 0.0001 F1: 99.92% dev loss: 0.0479 F1: 59.68% 0.44 min
> Epoch: 34 Step: 2600, train loss: 0.0001 F1: 99.92% dev loss: 0.0502 F1: 59.04% 0.44 min
> Epoch: 35 Step: 2700, train loss: 0.0001 F1: 99.89% dev loss: 0.0524 F1: 57.47% 0.44 min
> Epoch: 37 Step: 2800, train loss: 0.0000 F1: 99.74% dev loss: 0.0498 F1: 58.43% 0.44 min
> Epoch: 38 Step: 2900, train loss: 0.0001 F1: 99.92% dev loss: 0.0495 F1: 59.00% 0.44 min
> Epoch: 39 Step: 3000, train loss: 0.0001 F1: 99.85% dev loss: 0.0500 F1: 58.14% 0.44 min
> Epoch: 41 Step: 3100, train loss: 0.0000 F1: 99.76% dev loss: 0.0503 F1: 58.14% 0.44 min
> Epoch: 42 Step: 3200, train loss: 0.0000 F1: 99.82% dev loss: 0.0502 F1: 58.39% 0.44 min
> Epoch: 43 Step: 3300, train loss: 0.0000 F1: 99.96% dev loss: 0.0502 F1: 57.54% 0.44 min
> Epoch: 45 Step: 3400, train loss: 0.0000 F1: 100.00% dev loss: 0.0505 F1: 57.80% 0.44 min
> Epoch: 46 Step: 3500, train loss: 0.0000 F1: 99.86% dev loss: 0.0509 F1: 57.87% 0.44 min

>>>>>>>>>>>>>>>>>>>>>2021-07-18 07:23:13.268602>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 3e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3500
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 3e-05, 'weight_decay': 0.0001}
>>> device: cuda
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.6589 F1: 9.32% dev loss: 0.2629 F1: 9.40% 0.44 min
> Epoch: 2 Step: 200, train loss: 0.2563 F1: 12.85% dev loss: 0.1867 F1: 15.15% 0.43 min
> Epoch: 3 Step: 300, train loss: 0.1724 F1: 24.37% dev loss: 0.1347 F1: 29.94% 0.43 min
> Epoch: 5 Step: 400, train loss: 0.1154 F1: 44.89% dev loss: 0.1059 F1: 45.97% 0.43 min
> Epoch: 6 Step: 500, train loss: 0.0720 F1: 62.35% dev loss: 0.0981 F1: 52.73% 0.44 min
> Epoch: 7 Step: 600, train loss: 0.0435 F1: 74.48% dev loss: 0.0972 F1: 54.50% 0.44 min
> Epoch: 9 Step: 700, train loss: 0.0295 F1: 85.31% dev loss: 0.1231 F1: 50.37% 0.44 min
> Epoch: 10 Step: 800, train loss: 0.0207 F1: 89.19% dev loss: 0.1210 F1: 53.97% 0.44 min
> Epoch: 11 Step: 900, train loss: 0.0152 F1: 89.81% dev loss: 0.1128 F1: 50.38% 0.44 min
> Epoch: 13 Step: 1000, train loss: 0.0120 F1: 93.70% dev loss: 0.1126 F1: 55.71% 0.44 min
> Epoch: 14 Step: 1100, train loss: 0.0085 F1: 94.53% dev loss: 0.1096 F1: 57.85% 0.44 min
> Epoch: 15 Step: 1200, train loss: 0.0061 F1: 95.71% dev loss: 0.1145 F1: 55.63% 0.44 min
> Epoch: 17 Step: 1300, train loss: 0.0045 F1: 97.65% dev loss: 0.1308 F1: 55.47% 0.44 min
> Epoch: 18 Step: 1400, train loss: 0.0035 F1: 97.55% dev loss: 0.1332 F1: 55.48% 0.44 min
> Epoch: 19 Step: 1500, train loss: 0.0027 F1: 97.71% dev loss: 0.1288 F1: 56.15% 0.44 min
> Epoch: 21 Step: 1600, train loss: 0.0026 F1: 98.46% dev loss: 0.1320 F1: 56.87% 0.44 min
> Epoch: 22 Step: 1700, train loss: 0.0023 F1: 98.21% dev loss: 0.1465 F1: 54.82% 0.44 min
> Epoch: 23 Step: 1800, train loss: 0.0021 F1: 98.62% dev loss: 0.1341 F1: 56.79% 0.44 min
> Epoch: 25 Step: 1900, train loss: 0.0016 F1: 99.10% dev loss: 0.1388 F1: 52.78% 0.44 min
> Epoch: 26 Step: 2000, train loss: 0.0013 F1: 99.36% dev loss: 0.1439 F1: 56.51% 0.44 min
> Epoch: 27 Step: 2100, train loss: 0.0010 F1: 99.17% dev loss: 0.1502 F1: 56.35% 0.44 min
> Epoch: 29 Step: 2200, train loss: 0.0010 F1: 99.50% dev loss: 0.1457 F1: 57.05% 0.44 min
> Epoch: 30 Step: 2300, train loss: 0.0010 F1: 99.68% dev loss: 0.1512 F1: 56.14% 0.44 min
> Epoch: 31 Step: 2400, train loss: 0.0009 F1: 99.56% dev loss: 0.1463 F1: 57.70% 0.44 min
> Epoch: 33 Step: 2500, train loss: 0.0007 F1: 99.64% dev loss: 0.1456 F1: 56.55% 0.44 min
> Epoch: 34 Step: 2600, train loss: 0.0007 F1: 99.63% dev loss: 0.1624 F1: 53.97% 0.44 min
> Epoch: 35 Step: 2700, train loss: 0.0006 F1: 99.54% dev loss: 0.1488 F1: 55.77% 0.44 min
> Epoch: 37 Step: 2800, train loss: 0.0004 F1: 99.84% dev loss: 0.1459 F1: 56.60% 0.44 min
> Epoch: 38 Step: 2900, train loss: 0.0006 F1: 99.66% dev loss: 0.1501 F1: 56.46% 0.44 min
> Epoch: 39 Step: 3000, train loss: 0.0005 F1: 99.67% dev loss: 0.1465 F1: 58.49% 0.44 min
> Epoch: 41 Step: 3100, train loss: 0.0005 F1: 99.79% dev loss: 0.1463 F1: 56.53% 0.44 min
> Epoch: 42 Step: 3200, train loss: 0.0004 F1: 99.87% dev loss: 0.1501 F1: 56.25% 0.44 min
> Epoch: 43 Step: 3300, train loss: 0.0003 F1: 99.86% dev loss: 0.1531 F1: 57.54% 0.44 min
> Epoch: 45 Step: 3400, train loss: 0.0003 F1: 100.00% dev loss: 0.1504 F1: 55.58% 0.44 min
> Epoch: 46 Step: 3500, train loss: 0.0004 F1: 99.75% dev loss: 0.1504 F1: 55.74% 0.44 min

>>>>>>>>>>>>>>>>>>>>>2021-07-18 09:03:43.267417>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 1.0
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3500
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda
>>> class weight(or alpha) [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
> Epoch: 1 Step: 100, train loss: 0.7692 F1: 9.32% dev loss: 0.3191 F1: 9.40% 0.43 min
> Epoch: 2 Step: 200, train loss: 0.2962 F1: 19.41% dev loss: 0.2012 F1: 23.46% 0.43 min
> Epoch: 3 Step: 300, train loss: 0.1719 F1: 39.11% dev loss: 0.1412 F1: 45.23% 0.43 min
> Epoch: 5 Step: 400, train loss: 0.0999 F1: 62.88% dev loss: 0.1299 F1: 52.08% 0.43 min
> Epoch: 6 Step: 500, train loss: 0.0627 F1: 72.91% dev loss: 0.1430 F1: 52.33% 0.44 min
> Epoch: 7 Step: 600, train loss: 0.0377 F1: 83.26% dev loss: 0.1354 F1: 55.64% 0.44 min
> Epoch: 9 Step: 700, train loss: 0.0214 F1: 91.84% dev loss: 0.1800 F1: 52.20% 0.44 min
> Epoch: 10 Step: 800, train loss: 0.0150 F1: 94.11% dev loss: 0.1674 F1: 55.61% 0.44 min
> Epoch: 11 Step: 900, train loss: 0.0096 F1: 95.29% dev loss: 0.1476 F1: 54.02% 0.44 min
> Epoch: 13 Step: 1000, train loss: 0.0082 F1: 96.49% dev loss: 0.1515 F1: 56.00% 0.44 min
> Epoch: 14 Step: 1100, train loss: 0.0051 F1: 97.77% dev loss: 0.1763 F1: 57.32% 0.44 min
> Epoch: 15 Step: 1200, train loss: 0.0036 F1: 98.23% dev loss: 0.1665 F1: 57.28% 0.44 min
> Epoch: 17 Step: 1300, train loss: 0.0026 F1: 99.05% dev loss: 0.1686 F1: 58.10% 0.44 min
> Epoch: 18 Step: 1400, train loss: 0.0022 F1: 98.66% dev loss: 0.1756 F1: 56.54% 0.44 min
> Epoch: 19 Step: 1500, train loss: 0.0015 F1: 99.11% dev loss: 0.1822 F1: 55.01% 0.44 min
> Epoch: 21 Step: 1600, train loss: 0.0012 F1: 99.47% dev loss: 0.1743 F1: 58.09% 0.44 min
> Epoch: 22 Step: 1700, train loss: 0.0010 F1: 99.64% dev loss: 0.1813 F1: 57.58% 0.44 min
> Epoch: 23 Step: 1800, train loss: 0.0014 F1: 99.41% dev loss: 0.1857 F1: 58.43% 0.44 min
> Epoch: 25 Step: 1900, train loss: 0.0009 F1: 99.82% dev loss: 0.1840 F1: 59.92% 0.44 min
> Epoch: 26 Step: 2000, train loss: 0.0005 F1: 99.84% dev loss: 0.1909 F1: 58.21% 0.44 min
> Epoch: 27 Step: 2100, train loss: 0.0004 F1: 99.86% dev loss: 0.1938 F1: 58.11% 0.44 min
> Epoch: 29 Step: 2200, train loss: 0.0004 F1: 99.92% dev loss: 0.1884 F1: 58.61% 0.44 min
> Epoch: 30 Step: 2300, train loss: 0.0004 F1: 99.91% dev loss: 0.2061 F1: 57.52% 0.44 min
> Epoch: 31 Step: 2400, train loss: 0.0004 F1: 99.91% dev loss: 0.2017 F1: 57.25% 0.44 min
> Epoch: 33 Step: 2500, train loss: 0.0003 F1: 99.92% dev loss: 0.1928 F1: 58.33% 0.44 min
> Epoch: 34 Step: 2600, train loss: 0.0003 F1: 99.85% dev loss: 0.1999 F1: 57.03% 0.44 min
> Epoch: 35 Step: 2700, train loss: 0.0003 F1: 99.95% dev loss: 0.2010 F1: 56.28% 0.44 min
> Epoch: 37 Step: 2800, train loss: 0.0003 F1: 99.82% dev loss: 0.1984 F1: 56.59% 0.44 min
> Epoch: 38 Step: 2900, train loss: 0.0004 F1: 99.82% dev loss: 0.1967 F1: 56.43% 0.44 min
> Epoch: 39 Step: 3000, train loss: 0.0003 F1: 99.71% dev loss: 0.1994 F1: 59.01% 0.44 min
> Epoch: 41 Step: 3100, train loss: 0.0004 F1: 99.97% dev loss: 0.2012 F1: 57.65% 0.44 min
> Epoch: 42 Step: 3200, train loss: 0.0002 F1: 99.89% dev loss: 0.2012 F1: 56.59% 0.44 min
> Epoch: 43 Step: 3300, train loss: 0.0001 F1: 99.92% dev loss: 0.2017 F1: 58.51% 0.44 min
> Epoch: 45 Step: 3400, train loss: 0.0001 F1: 99.92% dev loss: 0.2021 F1: 58.58% 0.44 min
> Epoch: 46 Step: 3500, train loss: 0.0002 F1: 99.92% dev loss: 0.2027 F1: 58.64% 0.44 min

>>>>>>>>>>>>>>>>>>>>>2021-07-18 09:27:13.174123>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3500
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.5769 F1: 9.32% dev loss: 0.2390 F1: 9.40% 0.43 min
> Epoch: 2 Step: 200, train loss: 0.2217 F1: 19.81% dev loss: 0.1508 F1: 23.55% 0.43 min
> Epoch: 3 Step: 300, train loss: 0.1283 F1: 39.31% dev loss: 0.1056 F1: 44.82% 0.43 min
> Epoch: 5 Step: 400, train loss: 0.0740 F1: 62.74% dev loss: 0.0987 F1: 51.36% 0.43 min
> Epoch: 6 Step: 500, train loss: 0.0464 F1: 73.20% dev loss: 0.1062 F1: 53.14% 0.44 min
> Epoch: 7 Step: 600, train loss: 0.0284 F1: 83.34% dev loss: 0.1009 F1: 56.17% 0.44 min
> Epoch: 9 Step: 700, train loss: 0.0166 F1: 91.68% dev loss: 0.1310 F1: 53.52% 0.44 min
> Epoch: 10 Step: 800, train loss: 0.0113 F1: 94.12% dev loss: 0.1252 F1: 56.50% 0.44 min
> Epoch: 11 Step: 900, train loss: 0.0070 F1: 95.50% dev loss: 0.1030 F1: 56.36% 0.44 min
> Epoch: 13 Step: 1000, train loss: 0.0051 F1: 97.40% dev loss: 0.1116 F1: 59.65% 0.44 min
> Epoch: 14 Step: 1100, train loss: 0.0036 F1: 97.69% dev loss: 0.1347 F1: 57.13% 0.44 min
> Epoch: 15 Step: 1200, train loss: 0.0026 F1: 98.35% dev loss: 0.1196 F1: 56.67% 0.44 min
> Epoch: 17 Step: 1300, train loss: 0.0018 F1: 99.10% dev loss: 0.1268 F1: 61.11% 0.44 min
> Epoch: 18 Step: 1400, train loss: 0.0017 F1: 98.04% dev loss: 0.1324 F1: 57.23% 0.44 min
> Epoch: 19 Step: 1500, train loss: 0.0012 F1: 99.01% dev loss: 0.1321 F1: 56.75% 0.44 min
> Epoch: 21 Step: 1600, train loss: 0.0008 F1: 99.47% dev loss: 0.1335 F1: 59.22% 0.44 min
> Epoch: 22 Step: 1700, train loss: 0.0006 F1: 99.68% dev loss: 0.1461 F1: 57.88% 0.44 min
> Epoch: 23 Step: 1800, train loss: 0.0006 F1: 99.71% dev loss: 0.1367 F1: 60.08% 0.44 min
> Epoch: 25 Step: 1900, train loss: 0.0005 F1: 99.80% dev loss: 0.1366 F1: 58.15% 0.44 min
> Epoch: 26 Step: 2000, train loss: 0.0004 F1: 99.65% dev loss: 0.1444 F1: 58.83% 0.44 min
> Epoch: 27 Step: 2100, train loss: 0.0004 F1: 99.77% dev loss: 0.1412 F1: 58.88% 0.44 min
> Epoch: 29 Step: 2200, train loss: 0.0004 F1: 99.82% dev loss: 0.1382 F1: 57.14% 0.44 min
> Epoch: 30 Step: 2300, train loss: 0.0005 F1: 99.74% dev loss: 0.1457 F1: 57.40% 0.44 min
> Epoch: 31 Step: 2400, train loss: 0.0003 F1: 99.80% dev loss: 0.1494 F1: 57.28% 0.44 min
> Epoch: 33 Step: 2500, train loss: 0.0003 F1: 99.96% dev loss: 0.1466 F1: 57.22% 0.44 min
> Epoch: 34 Step: 2600, train loss: 0.0003 F1: 99.88% dev loss: 0.1448 F1: 59.71% 0.44 min
> Epoch: 35 Step: 2700, train loss: 0.0002 F1: 99.87% dev loss: 0.1446 F1: 59.72% 0.44 min
> Epoch: 37 Step: 2800, train loss: 0.0002 F1: 99.83% dev loss: 0.1468 F1: 59.12% 0.44 min
> Epoch: 38 Step: 2900, train loss: 0.0003 F1: 99.91% dev loss: 0.1475 F1: 57.86% 0.44 min
> Epoch: 39 Step: 3000, train loss: 0.0002 F1: 99.92% dev loss: 0.1510 F1: 58.89% 0.44 min
> Epoch: 41 Step: 3100, train loss: 0.0001 F1: 100.00% dev loss: 0.1500 F1: 59.35% 0.44 min
> Epoch: 42 Step: 3200, train loss: 0.0002 F1: 99.97% dev loss: 0.1476 F1: 59.61% 0.44 min
> Epoch: 43 Step: 3300, train loss: 0.0001 F1: 99.94% dev loss: 0.1491 F1: 59.65% 0.44 min
> Epoch: 45 Step: 3400, train loss: 0.0001 F1: 99.96% dev loss: 0.1502 F1: 59.80% 0.44 min
> Epoch: 46 Step: 3500, train loss: 0.0001 F1: 99.98% dev loss: 0.1506 F1: 59.69% 0.44 min

>>>>>>>>>>>>>>>>>>>>>2021-07-18 09:57:31.168496>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3500
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.5019 F1: 10.59% dev loss: 0.2041 F1: 9.40% 0.43 min
> Epoch: 2 Step: 200, train loss: 0.1907 F1: 19.29% dev loss: 0.1296 F1: 23.42% 0.43 min
> Epoch: 3 Step: 300, train loss: 0.1089 F1: 39.51% dev loss: 0.0868 F1: 46.87% 0.43 min
> Epoch: 5 Step: 400, train loss: 0.0611 F1: 61.94% dev loss: 0.0859 F1: 50.94% 0.43 min
> Epoch: 6 Step: 500, train loss: 0.0360 F1: 74.10% dev loss: 0.0809 F1: 51.02% 0.44 min
> Epoch: 7 Step: 600, train loss: 0.0215 F1: 82.74% dev loss: 0.0864 F1: 53.47% 0.44 min
> Epoch: 9 Step: 700, train loss: 0.0121 F1: 90.94% dev loss: 0.1111 F1: 51.47% 0.44 min
> Epoch: 10 Step: 800, train loss: 0.0088 F1: 92.91% dev loss: 0.1089 F1: 55.84% 0.44 min
> Epoch: 11 Step: 900, train loss: 0.0057 F1: 94.19% dev loss: 0.0982 F1: 50.92% 0.44 min
> Epoch: 13 Step: 1000, train loss: 0.0047 F1: 94.32% dev loss: 0.1013 F1: 57.27% 0.44 min
> Epoch: 14 Step: 1100, train loss: 0.0031 F1: 97.09% dev loss: 0.1035 F1: 56.67% 0.44 min
> Epoch: 15 Step: 1200, train loss: 0.0019 F1: 98.15% dev loss: 0.0988 F1: 56.92% 0.44 min
> Epoch: 17 Step: 1300, train loss: 0.0017 F1: 98.80% dev loss: 0.1046 F1: 58.30% 0.44 min
> Epoch: 18 Step: 1400, train loss: 0.0013 F1: 98.32% dev loss: 0.1109 F1: 56.27% 0.44 min
> Epoch: 19 Step: 1500, train loss: 0.0009 F1: 98.78% dev loss: 0.1064 F1: 58.42% 0.44 min
> Epoch: 21 Step: 1600, train loss: 0.0007 F1: 98.89% dev loss: 0.1093 F1: 58.38% 0.44 min
> Epoch: 22 Step: 1700, train loss: 0.0007 F1: 98.87% dev loss: 0.1197 F1: 55.43% 0.44 min
> Epoch: 23 Step: 1800, train loss: 0.0007 F1: 99.41% dev loss: 0.1149 F1: 55.98% 0.44 min
> Epoch: 25 Step: 1900, train loss: 0.0004 F1: 99.35% dev loss: 0.1168 F1: 55.66% 0.44 min
> Epoch: 26 Step: 2000, train loss: 0.0004 F1: 99.72% dev loss: 0.1206 F1: 56.11% 0.44 min
> Epoch: 27 Step: 2100, train loss: 0.0003 F1: 99.76% dev loss: 0.1217 F1: 57.33% 0.44 min
> Epoch: 29 Step: 2200, train loss: 0.0004 F1: 99.76% dev loss: 0.1173 F1: 56.44% 0.44 min
> Epoch: 30 Step: 2300, train loss: 0.0004 F1: 99.84% dev loss: 0.1271 F1: 55.98% 0.44 min
> Epoch: 31 Step: 2400, train loss: 0.0004 F1: 99.80% dev loss: 0.1258 F1: 56.24% 0.44 min
> Epoch: 33 Step: 2500, train loss: 0.0002 F1: 99.78% dev loss: 0.1186 F1: 56.93% 0.44 min
> Epoch: 34 Step: 2600, train loss: 0.0003 F1: 99.77% dev loss: 0.1293 F1: 56.22% 0.44 min
> Epoch: 35 Step: 2700, train loss: 0.0002 F1: 99.76% dev loss: 0.1263 F1: 56.72% 0.44 min
> Epoch: 37 Step: 2800, train loss: 0.0001 F1: 99.85% dev loss: 0.1215 F1: 58.34% 0.44 min
> Epoch: 38 Step: 2900, train loss: 0.0001 F1: 99.86% dev loss: 0.1226 F1: 58.58% 0.44 min
> Epoch: 39 Step: 3000, train loss: 0.0002 F1: 99.73% dev loss: 0.1250 F1: 58.33% 0.44 min
> Epoch: 41 Step: 3100, train loss: 0.0002 F1: 99.87% dev loss: 0.1214 F1: 58.43% 0.44 min
> Epoch: 42 Step: 3200, train loss: 0.0002 F1: 99.93% dev loss: 0.1251 F1: 57.23% 0.44 min
> Epoch: 43 Step: 3300, train loss: 0.0001 F1: 99.87% dev loss: 0.1304 F1: 57.92% 0.44 min
> Epoch: 45 Step: 3400, train loss: 0.0001 F1: 99.94% dev loss: 0.1254 F1: 57.56% 0.44 min
> Epoch: 46 Step: 3500, train loss: 0.0001 F1: 99.85% dev loss: 0.1249 F1: 57.52% 0.44 min

>>>>>>>>>>>>>>>>>>>>>2021-07-18 10:35:55.518731>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 5.0
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3500
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda
>>> class weight(or alpha) [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0]
> Epoch: 1 Step: 100, train loss: 3.3412 F1: 10.59% dev loss: 1.3496 F1: 9.40% 0.43 min
> Epoch: 2 Step: 200, train loss: 1.2583 F1: 19.65% dev loss: 0.8474 F1: 23.96% 0.43 min
> Epoch: 3 Step: 300, train loss: 0.7042 F1: 39.54% dev loss: 0.5691 F1: 46.80% 0.43 min
> Epoch: 5 Step: 400, train loss: 0.3900 F1: 62.86% dev loss: 0.5988 F1: 52.60% 0.43 min
> Epoch: 6 Step: 500, train loss: 0.2267 F1: 75.53% dev loss: 0.5543 F1: 49.68% 0.43 min
> Epoch: 7 Step: 600, train loss: 0.1397 F1: 82.29% dev loss: 0.5986 F1: 55.16% 0.44 min
> Epoch: 9 Step: 700, train loss: 0.0759 F1: 91.81% dev loss: 0.7842 F1: 52.78% 0.44 min
> Epoch: 10 Step: 800, train loss: 0.0573 F1: 93.27% dev loss: 0.6867 F1: 52.65% 0.44 min
> Epoch: 11 Step: 900, train loss: 0.0318 F1: 95.38% dev loss: 0.6116 F1: 55.78% 0.44 min
> Epoch: 13 Step: 1000, train loss: 0.0247 F1: 96.33% dev loss: 0.6484 F1: 57.69% 0.44 min
> Epoch: 14 Step: 1100, train loss: 0.0184 F1: 97.14% dev loss: 0.7097 F1: 56.57% 0.44 min
> Epoch: 15 Step: 1200, train loss: 0.0134 F1: 97.99% dev loss: 0.6582 F1: 56.66% 0.44 min
> Epoch: 17 Step: 1300, train loss: 0.0088 F1: 98.24% dev loss: 0.6839 F1: 58.55% 0.44 min
> Epoch: 18 Step: 1400, train loss: 0.0086 F1: 98.37% dev loss: 0.7478 F1: 54.08% 0.44 min
> Epoch: 19 Step: 1500, train loss: 0.0066 F1: 98.66% dev loss: 0.7327 F1: 56.65% 0.44 min
> Epoch: 21 Step: 1600, train loss: 0.0054 F1: 98.76% dev loss: 0.7345 F1: 57.77% 0.44 min
> Epoch: 22 Step: 1700, train loss: 0.0062 F1: 99.21% dev loss: 0.8417 F1: 58.15% 0.44 min
> Epoch: 23 Step: 1800, train loss: 0.0039 F1: 99.46% dev loss: 0.7672 F1: 56.51% 0.44 min
> Epoch: 25 Step: 1900, train loss: 0.0031 F1: 99.73% dev loss: 0.7927 F1: 54.20% 0.44 min
> Epoch: 26 Step: 2000, train loss: 0.0027 F1: 99.71% dev loss: 0.7836 F1: 56.62% 0.44 min
> Epoch: 27 Step: 2100, train loss: 0.0015 F1: 99.75% dev loss: 0.8102 F1: 54.02% 0.44 min
> Epoch: 29 Step: 2200, train loss: 0.0019 F1: 99.83% dev loss: 0.8106 F1: 54.70% 0.44 min
> Epoch: 30 Step: 2300, train loss: 0.0017 F1: 99.81% dev loss: 0.8595 F1: 55.91% 0.44 min
> Epoch: 31 Step: 2400, train loss: 0.0016 F1: 99.76% dev loss: 0.8696 F1: 56.01% 0.44 min
> Epoch: 33 Step: 2500, train loss: 0.0015 F1: 99.94% dev loss: 0.7889 F1: 55.04% 0.44 min
> Epoch: 34 Step: 2600, train loss: 0.0015 F1: 99.81% dev loss: 0.8678 F1: 56.56% 0.44 min
> Epoch: 35 Step: 2700, train loss: 0.0012 F1: 99.83% dev loss: 0.8529 F1: 54.66% 0.44 min
> Epoch: 37 Step: 2800, train loss: 0.0009 F1: 99.89% dev loss: 0.8352 F1: 55.96% 0.44 min
> Epoch: 38 Step: 2900, train loss: 0.0011 F1: 99.80% dev loss: 0.8483 F1: 55.11% 0.44 min
> Epoch: 39 Step: 3000, train loss: 0.0011 F1: 99.81% dev loss: 0.8597 F1: 57.90% 0.44 min
> Epoch: 41 Step: 3100, train loss: 0.0010 F1: 99.96% dev loss: 0.8335 F1: 55.92% 0.44 min
> Epoch: 42 Step: 3200, train loss: 0.0008 F1: 99.88% dev loss: 0.8367 F1: 56.35% 0.44 min
> Epoch: 43 Step: 3300, train loss: 0.0005 F1: 99.94% dev loss: 0.8474 F1: 55.87% 0.44 min
> Epoch: 45 Step: 3400, train loss: 0.0005 F1: 100.00% dev loss: 0.8463 F1: 56.23% 0.44 min
> Epoch: 46 Step: 3500, train loss: 0.0006 F1: 99.89% dev loss: 0.8462 F1: 56.25% 0.44 min

>>>>>>>>>>>>>>>>>>>>>2021-07-18 11:14:21.111364>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 2e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 3.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3500
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 2e-05, 'weight_decay': 0.0001}
>>> device: cuda
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.6404 F1: 9.32% dev loss: 0.2465 F1: 9.40% 0.44 min
> Epoch: 2 Step: 200, train loss: 0.2350 F1: 11.58% dev loss: 0.1809 F1: 9.40% 0.43 min
> Epoch: 3 Step: 300, train loss: 0.1734 F1: 17.51% dev loss: 0.1312 F1: 22.76% 0.43 min
> Epoch: 5 Step: 400, train loss: 0.1235 F1: 31.52% dev loss: 0.1030 F1: 34.77% 0.43 min
> Epoch: 6 Step: 500, train loss: 0.0812 F1: 51.83% dev loss: 0.0802 F1: 49.84% 0.44 min
> Epoch: 7 Step: 600, train loss: 0.0496 F1: 62.78% dev loss: 0.0771 F1: 53.07% 0.44 min
> Epoch: 9 Step: 700, train loss: 0.0337 F1: 76.03% dev loss: 0.0964 F1: 49.56% 0.44 min
> Epoch: 10 Step: 800, train loss: 0.0249 F1: 81.49% dev loss: 0.0897 F1: 52.48% 0.44 min
> Epoch: 11 Step: 900, train loss: 0.0175 F1: 84.83% dev loss: 0.0822 F1: 54.03% 0.44 min
> Epoch: 13 Step: 1000, train loss: 0.0134 F1: 89.80% dev loss: 0.0917 F1: 54.88% 0.44 min
> Epoch: 14 Step: 1100, train loss: 0.0108 F1: 90.48% dev loss: 0.0891 F1: 53.89% 0.44 min
> Epoch: 15 Step: 1200, train loss: 0.0092 F1: 91.37% dev loss: 0.0939 F1: 54.60% 0.44 min
> Epoch: 17 Step: 1300, train loss: 0.0079 F1: 93.70% dev loss: 0.0984 F1: 55.02% 0.44 min
> Epoch: 18 Step: 1400, train loss: 0.0064 F1: 94.47% dev loss: 0.0962 F1: 54.62% 0.44 min
> Epoch: 19 Step: 1500, train loss: 0.0059 F1: 93.54% dev loss: 0.0991 F1: 54.25% 0.44 min
> Epoch: 21 Step: 1600, train loss: 0.0046 F1: 95.23% dev loss: 0.0987 F1: 54.40% 0.44 min
> Epoch: 22 Step: 1700, train loss: 0.0035 F1: 95.91% dev loss: 0.1042 F1: 54.30% 0.44 min
> Epoch: 23 Step: 1800, train loss: 0.0031 F1: 96.23% dev loss: 0.1067 F1: 54.25% 0.44 min
> Epoch: 25 Step: 1900, train loss: 0.0027 F1: 97.25% dev loss: 0.1095 F1: 52.44% 0.44 min
> Epoch: 26 Step: 2000, train loss: 0.0022 F1: 97.51% dev loss: 0.1150 F1: 55.30% 0.44 min
> Epoch: 27 Step: 2100, train loss: 0.0017 F1: 97.92% dev loss: 0.1184 F1: 54.41% 0.44 min
> Epoch: 29 Step: 2200, train loss: 0.0018 F1: 98.64% dev loss: 0.1093 F1: 55.43% 0.44 min
> Epoch: 30 Step: 2300, train loss: 0.0016 F1: 98.62% dev loss: 0.1188 F1: 55.07% 0.44 min
> Epoch: 31 Step: 2400, train loss: 0.0015 F1: 98.29% dev loss: 0.1090 F1: 56.99% 0.44 min
> Epoch: 33 Step: 2500, train loss: 0.0012 F1: 98.65% dev loss: 0.1116 F1: 55.11% 0.44 min
> Epoch: 34 Step: 2600, train loss: 0.0013 F1: 98.52% dev loss: 0.1255 F1: 54.13% 0.44 min
> Epoch: 35 Step: 2700, train loss: 0.0011 F1: 98.65% dev loss: 0.1148 F1: 55.55% 0.44 min
> Epoch: 37 Step: 2800, train loss: 0.0008 F1: 99.18% dev loss: 0.1152 F1: 55.27% 0.44 min
> Epoch: 38 Step: 2900, train loss: 0.0010 F1: 98.72% dev loss: 0.1271 F1: 54.59% 0.44 min
> Epoch: 39 Step: 3000, train loss: 0.0009 F1: 98.81% dev loss: 0.1166 F1: 54.93% 0.44 min
> Epoch: 41 Step: 3100, train loss: 0.0009 F1: 99.48% dev loss: 0.1152 F1: 55.25% 0.44 min
> Epoch: 42 Step: 3200, train loss: 0.0008 F1: 99.26% dev loss: 0.1204 F1: 54.40% 0.44 min
> Epoch: 43 Step: 3300, train loss: 0.0007 F1: 99.18% dev loss: 0.1210 F1: 54.27% 0.44 min
> Epoch: 45 Step: 3400, train loss: 0.0006 F1: 99.41% dev loss: 0.1191 F1: 54.79% 0.44 min
> Epoch: 46 Step: 3500, train loss: 0.0007 F1: 99.21% dev loss: 0.1200 F1: 54.66% 0.44 min


>>>>>>>>>>>>>>>>>>>>>2021-07-18 12:36:38.574569>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: CE
>>> gamma: 2.0
>>> alpha: 0.25
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3500
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda
>>> class weight(or alpha) [0.1, 0.8, 1.0, 1.0, 1.2, 1.2, 1.2, 1.0, 1.0, 1.0]
> Epoch: 1 Step: 100, train loss: 1.9041 F1: 15.40% dev loss: 1.4492 F1: 20.41% 0.43 min
> Epoch: 2 Step: 200, train loss: 1.1734 F1: 29.87% dev loss: 0.8294 F1: 44.56% 0.43 min
> Epoch: 3 Step: 300, train loss: 0.6252 F1: 54.18% dev loss: 0.6945 F1: 51.39% 0.43 min
> Epoch: 5 Step: 400, train loss: 0.4011 F1: 71.48% dev loss: 0.9350 F1: 44.86% 0.43 min
> Epoch: 6 Step: 500, train loss: 0.2692 F1: 82.96% dev loss: 0.7886 F1: 49.25% 0.43 min
> Epoch: 7 Step: 600, train loss: 0.1762 F1: 87.20% dev loss: 0.8802 F1: 53.27% 0.43 min
> Epoch: 9 Step: 700, train loss: 0.1168 F1: 93.80% dev loss: 0.9072 F1: 54.37% 0.44 min
> Epoch: 10 Step: 800, train loss: 0.0850 F1: 94.68% dev loss: 1.1477 F1: 48.93% 0.44 min
> Epoch: 11 Step: 900, train loss: 0.0674 F1: 95.90% dev loss: 0.9216 F1: 52.83% 0.44 min
> Epoch: 13 Step: 1000, train loss: 0.0381 F1: 98.33% dev loss: 1.0199 F1: 55.52% 0.44 min
> Epoch: 14 Step: 1100, train loss: 0.0228 F1: 98.42% dev loss: 0.9891 F1: 57.83% 0.43 min
> Epoch: 15 Step: 1200, train loss: 0.0182 F1: 98.34% dev loss: 0.9622 F1: 56.94% 0.44 min
> Epoch: 17 Step: 1300, train loss: 0.0145 F1: 99.02% dev loss: 1.0860 F1: 58.69% 0.44 min
> Epoch: 18 Step: 1400, train loss: 0.0123 F1: 98.90% dev loss: 1.0588 F1: 55.34% 0.44 min
> Epoch: 19 Step: 1500, train loss: 0.0094 F1: 99.47% dev loss: 1.0847 F1: 59.07% 0.43 min
> Epoch: 21 Step: 1600, train loss: 0.0097 F1: 99.31% dev loss: 1.1683 F1: 56.64% 0.44 min
> Epoch: 22 Step: 1700, train loss: 0.0129 F1: 99.19% dev loss: 1.1694 F1: 56.89% 0.43 min
> Epoch: 23 Step: 1800, train loss: 0.0105 F1: 99.58% dev loss: 1.1850 F1: 57.07% 0.43 min
> Epoch: 25 Step: 1900, train loss: 0.0167 F1: 98.16% dev loss: 1.2085 F1: 58.25% 0.43 min
> Epoch: 26 Step: 2000, train loss: 0.0107 F1: 99.39% dev loss: 1.2066 F1: 57.92% 0.43 min
> Epoch: 27 Step: 2100, train loss: 0.0071 F1: 99.52% dev loss: 1.2315 F1: 58.57% 0.44 min
> Epoch: 29 Step: 2200, train loss: 0.0074 F1: 99.40% dev loss: 1.2524 F1: 58.93% 0.44 min
> Epoch: 30 Step: 2300, train loss: 0.0052 F1: 99.63% dev loss: 1.2890 F1: 59.06% 0.44 min
> Epoch: 31 Step: 2400, train loss: 0.0042 F1: 99.79% dev loss: 1.2486 F1: 59.85% 0.44 min
> Epoch: 33 Step: 2500, train loss: 0.0046 F1: 99.71% dev loss: 1.3628 F1: 58.90% 0.44 min
> Epoch: 34 Step: 2600, train loss: 0.0067 F1: 99.67% dev loss: 1.2822 F1: 57.20% 0.44 min
> Epoch: 35 Step: 2700, train loss: 0.0035 F1: 99.83% dev loss: 1.3047 F1: 56.91% 0.44 min
> Epoch: 37 Step: 2800, train loss: 0.0025 F1: 99.95% dev loss: 1.3249 F1: 56.62% 0.44 min
> Epoch: 38 Step: 2900, train loss: 0.0029 F1: 99.74% dev loss: 1.3214 F1: 58.91% 0.44 min
> Epoch: 39 Step: 3000, train loss: 0.0020 F1: 99.87% dev loss: 1.2835 F1: 55.77% 0.44 min
> Epoch: 41 Step: 3100, train loss: 0.0036 F1: 99.95% dev loss: 1.3215 F1: 56.20% 0.44 min
> Epoch: 42 Step: 3200, train loss: 0.0017 F1: 99.88% dev loss: 1.3268 F1: 57.23% 0.44 min
> Epoch: 43 Step: 3300, train loss: 0.0015 F1: 99.94% dev loss: 1.3096 F1: 58.32% 0.44 min
> Epoch: 45 Step: 3400, train loss: 0.0016 F1: 99.97% dev loss: 1.3120 F1: 58.21% 0.44 min
> Epoch: 46 Step: 3500, train loss: 0.0014 F1: 99.91% dev loss: 1.3108 F1: 58.14% 0.44 min

>>>>>>>>>>>>>>>>>>>>>2021-07-18 13:01:52.279352>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 7e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3500
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 7e-05, 'weight_decay': 0.0001}
>>> device: cuda
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.5343 F1: 11.58% dev loss: 0.2288 F1: 9.40% 0.43 min
> Epoch: 2 Step: 200, train loss: 0.2028 F1: 23.85% dev loss: 0.1347 F1: 29.00% 0.43 min
> Epoch: 3 Step: 300, train loss: 0.1080 F1: 43.28% dev loss: 0.0956 F1: 51.12% 0.43 min
> Epoch: 5 Step: 400, train loss: 0.0598 F1: 71.20% dev loss: 0.1122 F1: 50.66% 0.43 min
> Epoch: 6 Step: 500, train loss: 0.0388 F1: 77.73% dev loss: 0.1077 F1: 53.34% 0.44 min
> Epoch: 7 Step: 600, train loss: 0.0238 F1: 85.87% dev loss: 0.1020 F1: 57.80% 0.44 min
> Epoch: 9 Step: 700, train loss: 0.0169 F1: 92.06% dev loss: 0.1226 F1: 55.50% 0.44 min
> Epoch: 10 Step: 800, train loss: 0.0091 F1: 94.29% dev loss: 0.1118 F1: 58.08% 0.44 min
> Epoch: 11 Step: 900, train loss: 0.0051 F1: 95.87% dev loss: 0.1133 F1: 56.47% 0.44 min
> Epoch: 13 Step: 1000, train loss: 0.0035 F1: 98.25% dev loss: 0.1261 F1: 55.96% 0.44 min
> Epoch: 14 Step: 1100, train loss: 0.0019 F1: 99.12% dev loss: 0.1350 F1: 56.91% 0.44 min
> Epoch: 15 Step: 1200, train loss: 0.0014 F1: 99.07% dev loss: 0.1374 F1: 56.73% 0.44 min
> Epoch: 17 Step: 1300, train loss: 0.0018 F1: 99.27% dev loss: 0.1340 F1: 59.17% 0.44 min
> Epoch: 18 Step: 1400, train loss: 0.0014 F1: 99.17% dev loss: 0.1401 F1: 56.06% 0.44 min
> Epoch: 19 Step: 1500, train loss: 0.0009 F1: 98.87% dev loss: 0.1403 F1: 58.19% 0.44 min
> Epoch: 21 Step: 1600, train loss: 0.0007 F1: 99.91% dev loss: 0.1357 F1: 56.11% 0.44 min
> Epoch: 22 Step: 1700, train loss: 0.0005 F1: 99.76% dev loss: 0.1545 F1: 55.80% 0.44 min
> Epoch: 23 Step: 1800, train loss: 0.0005 F1: 99.70% dev loss: 0.1444 F1: 58.32% 0.44 min
> Epoch: 25 Step: 1900, train loss: 0.0003 F1: 99.86% dev loss: 0.1383 F1: 57.67% 0.44 min
> Epoch: 26 Step: 2000, train loss: 0.0004 F1: 99.77% dev loss: 0.1454 F1: 57.57% 0.44 min
> Epoch: 27 Step: 2100, train loss: 0.0003 F1: 99.87% dev loss: 0.1450 F1: 55.65% 0.44 min
> Epoch: 29 Step: 2200, train loss: 0.0002 F1: 99.97% dev loss: 0.1485 F1: 56.70% 0.44 min
> Epoch: 30 Step: 2300, train loss: 0.0003 F1: 99.92% dev loss: 0.1441 F1: 57.04% 0.44 min
> Epoch: 31 Step: 2400, train loss: 0.0002 F1: 99.90% dev loss: 0.1488 F1: 56.25% 0.44 min
> Epoch: 33 Step: 2500, train loss: 0.0003 F1: 99.94% dev loss: 0.1458 F1: 57.26% 0.44 min
> Epoch: 34 Step: 2600, train loss: 0.0003 F1: 99.91% dev loss: 0.1507 F1: 56.04% 0.44 min
> Epoch: 35 Step: 2700, train loss: 0.0002 F1: 99.93% dev loss: 0.1495 F1: 57.03% 0.44 min
> Epoch: 37 Step: 2800, train loss: 0.0001 F1: 100.00% dev loss: 0.1514 F1: 57.37% 0.44 min
> Epoch: 38 Step: 2900, train loss: 0.0001 F1: 100.00% dev loss: 0.1511 F1: 56.29% 0.44 min
> Epoch: 39 Step: 3000, train loss: 0.0001 F1: 99.91% dev loss: 0.1540 F1: 56.47% 0.44 min
> Epoch: 41 Step: 3100, train loss: 0.0001 F1: 99.96% dev loss: 0.1508 F1: 57.70% 0.44 min
> Epoch: 42 Step: 3200, train loss: 0.0001 F1: 99.99% dev loss: 0.1532 F1: 57.34% 0.44 min
> Epoch: 43 Step: 3300, train loss: 0.0001 F1: 99.93% dev loss: 0.1548 F1: 56.13% 0.44 min
> Epoch: 45 Step: 3400, train loss: 0.0001 F1: 99.91% dev loss: 0.1548 F1: 56.32% 0.44 min
> Epoch: 46 Step: 3500, train loss: 0.0001 F1: 99.98% dev loss: 0.1542 F1: 56.44% 0.44 min

>>>>>>>>>>>>>>>>>>>>>2021-07-18 15:23:44.955135>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 5.0
>>> alpha: 5.0
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3500
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda
>>> class weight(or alpha) [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0]
> Epoch: 1 Step: 100, train loss: 2.5819 F1: 10.36% dev loss: 1.0201 F1: 9.40% 0.44 min
> Epoch: 2 Step: 200, train loss: 0.9686 F1: 18.49% dev loss: 0.6564 F1: 22.89% 0.43 min
> Epoch: 3 Step: 300, train loss: 0.5332 F1: 39.46% dev loss: 0.3958 F1: 47.85% 0.43 min
> Epoch: 5 Step: 400, train loss: 0.2757 F1: 62.68% dev loss: 0.4131 F1: 52.08% 0.43 min
> Epoch: 6 Step: 500, train loss: 0.1446 F1: 76.26% dev loss: 0.3756 F1: 53.37% 0.43 min
> Epoch: 7 Step: 600, train loss: 0.0926 F1: 81.18% dev loss: 0.3956 F1: 54.29% 0.43 min
> Epoch: 9 Step: 700, train loss: 0.0498 F1: 91.46% dev loss: 0.4228 F1: 53.95% 0.44 min
> Epoch: 10 Step: 800, train loss: 0.0374 F1: 91.77% dev loss: 0.4670 F1: 54.94% 0.44 min
> Epoch: 11 Step: 900, train loss: 0.0240 F1: 92.42% dev loss: 0.4556 F1: 53.11% 0.44 min
> Epoch: 13 Step: 1000, train loss: 0.0222 F1: 91.34% dev loss: 0.4423 F1: 54.25% 0.44 min
> Epoch: 14 Step: 1100, train loss: 0.0144 F1: 95.23% dev loss: 0.4556 F1: 54.26% 0.44 min
> Epoch: 15 Step: 1200, train loss: 0.0087 F1: 96.97% dev loss: 0.4607 F1: 55.12% 0.44 min
> Epoch: 17 Step: 1300, train loss: 0.0075 F1: 97.23% dev loss: 0.5290 F1: 55.75% 0.44 min
> Epoch: 18 Step: 1400, train loss: 0.0053 F1: 97.60% dev loss: 0.5261 F1: 54.39% 0.44 min
> Epoch: 19 Step: 1500, train loss: 0.0047 F1: 98.26% dev loss: 0.5304 F1: 55.71% 0.44 min
> Epoch: 21 Step: 1600, train loss: 0.0040 F1: 98.47% dev loss: 0.5053 F1: 56.01% 0.44 min
> Epoch: 22 Step: 1700, train loss: 0.0028 F1: 98.69% dev loss: 0.5718 F1: 54.81% 0.44 min
> Epoch: 23 Step: 1800, train loss: 0.0033 F1: 98.76% dev loss: 0.5257 F1: 55.50% 0.44 min
> Epoch: 25 Step: 1900, train loss: 0.0021 F1: 99.09% dev loss: 0.5406 F1: 56.53% 0.44 min
> Epoch: 26 Step: 2000, train loss: 0.0019 F1: 99.27% dev loss: 0.5704 F1: 56.07% 0.44 min
> Epoch: 27 Step: 2100, train loss: 0.0015 F1: 99.10% dev loss: 0.5662 F1: 54.51% 0.44 min
> Epoch: 29 Step: 2200, train loss: 0.0018 F1: 99.65% dev loss: 0.5561 F1: 57.08% 0.44 min
> Epoch: 30 Step: 2300, train loss: 0.0015 F1: 99.56% dev loss: 0.5818 F1: 55.72% 0.44 min
> Epoch: 31 Step: 2400, train loss: 0.0015 F1: 99.29% dev loss: 0.5709 F1: 55.28% 0.44 min
> Epoch: 33 Step: 2500, train loss: 0.0012 F1: 99.44% dev loss: 0.5439 F1: 56.65% 0.44 min
> Epoch: 34 Step: 2600, train loss: 0.0013 F1: 99.49% dev loss: 0.6253 F1: 54.78% 0.44 min
> Epoch: 35 Step: 2700, train loss: 0.0007 F1: 99.64% dev loss: 0.5947 F1: 55.45% 0.44 min
> Epoch: 37 Step: 2800, train loss: 0.0007 F1: 99.71% dev loss: 0.5847 F1: 56.77% 0.44 min
> Epoch: 38 Step: 2900, train loss: 0.0007 F1: 99.67% dev loss: 0.6099 F1: 55.87% 0.44 min
> Epoch: 39 Step: 3000, train loss: 0.0010 F1: 99.56% dev loss: 0.6144 F1: 55.91% 0.44 min
Lock 139866062354896 acquired on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock
Lock 139866062354896 released on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock
Lock 139866062354896 acquired on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock
Lock 139866062354896 released on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock



>>>>>>>>>>>>>>>>>>>>>2021-07-18 16:02:13.240996>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3500
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.5772 F1: 9.32% dev loss: 0.2395 F1: 9.40% 0.43 min
> Epoch: 2 Step: 200, train loss: 0.2224 F1: 19.49% dev loss: 0.1514 F1: 23.73% 0.43 min
> Epoch: 3 Step: 300, train loss: 0.1291 F1: 39.09% dev loss: 0.1056 F1: 45.09% 0.43 min
> Epoch: 5 Step: 400, train loss: 0.0748 F1: 62.55% dev loss: 0.0972 F1: 52.31% 0.43 min
> Epoch: 6 Step: 500, train loss: 0.0469 F1: 73.37% dev loss: 0.1047 F1: 53.11% 0.43 min
> Epoch: 7 Step: 600, train loss: 0.0288 F1: 82.97% dev loss: 0.1013 F1: 55.46% 0.43 min
> Epoch: 9 Step: 700, train loss: 0.0164 F1: 91.40% dev loss: 0.1347 F1: 52.74% 0.44 min
> Epoch: 10 Step: 800, train loss: 0.0110 F1: 93.94% dev loss: 0.1244 F1: 58.18% 0.44 min
> Epoch: 11 Step: 900, train loss: 0.0068 F1: 95.40% dev loss: 0.1082 F1: 55.83% 0.43 min
> Epoch: 13 Step: 1000, train loss: 0.0053 F1: 97.20% dev loss: 0.1149 F1: 57.73% 0.43 min
> Epoch: 14 Step: 1100, train loss: 0.0034 F1: 98.11% dev loss: 0.1283 F1: 56.56% 0.44 min
> Epoch: 15 Step: 1200, train loss: 0.0029 F1: 98.30% dev loss: 0.1209 F1: 58.18% 0.44 min
> Epoch: 17 Step: 1300, train loss: 0.0020 F1: 99.01% dev loss: 0.1277 F1: 55.11% 0.44 min
> Epoch: 18 Step: 1400, train loss: 0.0019 F1: 98.10% dev loss: 0.1301 F1: 57.69% 0.44 min
> Epoch: 19 Step: 1500, train loss: 0.0011 F1: 99.22% dev loss: 0.1332 F1: 58.87% 0.44 min
> Epoch: 21 Step: 1600, train loss: 0.0009 F1: 99.15% dev loss: 0.1319 F1: 57.61% 0.44 min
> Epoch: 22 Step: 1700, train loss: 0.0010 F1: 99.37% dev loss: 0.1350 F1: 59.63% 0.44 min
> Epoch: 23 Step: 1800, train loss: 0.0007 F1: 99.50% dev loss: 0.1361 F1: 59.57% 0.44 min
> Epoch: 25 Step: 1900, train loss: 0.0005 F1: 99.75% dev loss: 0.1313 F1: 60.43% 0.43 min
> Epoch: 26 Step: 2000, train loss: 0.0004 F1: 99.73% dev loss: 0.1425 F1: 57.49% 0.44 min
> Epoch: 27 Step: 2100, train loss: 0.0002 F1: 99.74% dev loss: 0.1471 F1: 59.39% 0.44 min
> Epoch: 29 Step: 2200, train loss: 0.0003 F1: 99.93% dev loss: 0.1419 F1: 59.38% 0.44 min
> Epoch: 30 Step: 2300, train loss: 0.0002 F1: 99.96% dev loss: 0.1483 F1: 56.43% 0.44 min
> Epoch: 31 Step: 2400, train loss: 0.0002 F1: 99.90% dev loss: 0.1502 F1: 56.67% 0.44 min
> Epoch: 33 Step: 2500, train loss: 0.0003 F1: 99.92% dev loss: 0.1437 F1: 58.16% 0.44 min
> Epoch: 34 Step: 2600, train loss: 0.0003 F1: 99.81% dev loss: 0.1584 F1: 56.53% 0.44 min
> Epoch: 35 Step: 2700, train loss: 0.0003 F1: 99.91% dev loss: 0.1494 F1: 56.90% 0.44 min
> Epoch: 37 Step: 2800, train loss: 0.0002 F1: 99.70% dev loss: 0.1471 F1: 58.03% 0.44 min
> Epoch: 38 Step: 2900, train loss: 0.0003 F1: 99.89% dev loss: 0.1483 F1: 57.72% 0.44 min
> Epoch: 39 Step: 3000, train loss: 0.0002 F1: 99.75% dev loss: 0.1434 F1: 59.41% 0.44 min
> Epoch: 41 Step: 3100, train loss: 0.0001 F1: 99.79% dev loss: 0.1426 F1: 57.84% 0.44 min

>>>>>>>>>>>>>>>>>>>>>2021-07-20 07:07:09.629715>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 2e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: CE
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 300
>>> max_steps: 3500
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 2e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.1, 0.8, 1.0, 1.0, 1.2, 1.2, 1.2, 1.0, 1.0, 1.0]
> Epoch: 1 Step: 100, train loss: 1.9648 F1: 11.12% dev loss: 1.5651 F1: 16.22% 0.44 min
> Epoch: 2 Step: 200, train loss: 1.3369 F1: 25.66% dev loss: 0.9454 F1: 35.45% 0.43 min
> Epoch: 3 Step: 300, train loss: 0.7756 F1: 47.53% dev loss: 0.6930 F1: 54.56% 0.43 min
> Epoch: 5 Step: 400, train loss: 0.4797 F1: 69.41% dev loss: 0.7525 F1: 46.59% 0.43 min
> Epoch: 6 Step: 500, train loss: 0.3241 F1: 79.05% dev loss: 0.7567 F1: 50.07% 0.44 min
> Epoch: 7 Step: 600, train loss: 0.2179 F1: 84.64% dev loss: 0.7997 F1: 56.88% 0.44 min
> Epoch: 9 Step: 700, train loss: 0.1624 F1: 90.21% dev loss: 0.7921 F1: 55.14% 0.44 min
> Epoch: 10 Step: 800, train loss: 0.1185 F1: 93.12% dev loss: 0.9162 F1: 52.95% 0.44 min
> Epoch: 11 Step: 900, train loss: 0.0873 F1: 94.37% dev loss: 0.9334 F1: 52.72% 0.44 min
> Epoch: 13 Step: 1000, train loss: 0.0760 F1: 95.63% dev loss: 0.9184 F1: 53.36% 0.44 min
> Epoch: 14 Step: 1100, train loss: 0.0637 F1: 95.91% dev loss: 0.9908 F1: 54.62% 0.44 min
> Epoch: 15 Step: 1200, train loss: 0.0452 F1: 96.71% dev loss: 0.9417 F1: 56.64% 0.44 min
> Epoch: 17 Step: 1300, train loss: 0.0396 F1: 97.72% dev loss: 1.0523 F1: 55.65% 0.44 min
> Epoch: 18 Step: 1400, train loss: 0.0318 F1: 97.93% dev loss: 1.0040 F1: 53.58% 0.44 min
> Epoch: 19 Step: 1500, train loss: 0.0239 F1: 98.01% dev loss: 1.0382 F1: 56.30% 0.44 min
> Epoch: 21 Step: 1600, train loss: 0.0206 F1: 98.53% dev loss: 1.0322 F1: 54.53% 0.44 min
> Epoch: 22 Step: 1700, train loss: 0.0185 F1: 98.48% dev loss: 1.0284 F1: 53.80% 0.44 min
> Epoch: 23 Step: 1800, train loss: 0.0183 F1: 98.66% dev loss: 1.0273 F1: 55.44% 0.44 min
> Epoch: 25 Step: 1900, train loss: 0.0140 F1: 98.86% dev loss: 1.0858 F1: 56.34% 0.44 min
> Epoch: 26 Step: 2000, train loss: 0.0137 F1: 98.88% dev loss: 1.0913 F1: 55.51% 0.44 min
> Epoch: 27 Step: 2100, train loss: 0.0124 F1: 99.01% dev loss: 1.1020 F1: 55.41% 0.44 min
> Epoch: 29 Step: 2200, train loss: 0.0107 F1: 99.37% dev loss: 1.1538 F1: 55.72% 0.44 min
> Epoch: 30 Step: 2300, train loss: 0.0100 F1: 99.28% dev loss: 1.1273 F1: 54.97% 0.44 min
> Epoch: 31 Step: 2400, train loss: 0.0092 F1: 99.47% dev loss: 1.1494 F1: 55.08% 0.44 min
> Epoch: 33 Step: 2500, train loss: 0.0073 F1: 99.48% dev loss: 1.1724 F1: 55.39% 0.44 min
> Epoch: 34 Step: 2600, train loss: 0.0084 F1: 99.44% dev loss: 1.1668 F1: 54.86% 0.44 min
> Epoch: 35 Step: 2700, train loss: 0.0074 F1: 99.56% dev loss: 1.1696 F1: 55.85% 0.44 min
> Epoch: 37 Step: 2800, train loss: 0.0066 F1: 99.66% dev loss: 1.1765 F1: 56.10% 0.44 min
> Epoch: 38 Step: 2900, train loss: 0.0071 F1: 99.53% dev loss: 1.1612 F1: 54.98% 0.44 min
> Epoch: 39 Step: 3000, train loss: 0.0063 F1: 99.54% dev loss: 1.1956 F1: 55.29% 0.44 min
> Epoch: 41 Step: 3100, train loss: 0.0067 F1: 99.55% dev loss: 1.2032 F1: 55.65% 0.44 min
> Epoch: 42 Step: 3200, train loss: 0.0056 F1: 99.63% dev loss: 1.2004 F1: 55.21% 0.44 min
> Epoch: 43 Step: 3300, train loss: 0.0048 F1: 99.54% dev loss: 1.2101 F1: 54.98% 0.44 min
> Epoch: 45 Step: 3400, train loss: 0.0046 F1: 99.55% dev loss: 1.2158 F1: 55.23% 0.44 min
> Epoch: 46 Step: 3500, train loss: 0.0052 F1: 99.68% dev loss: 1.2120 F1: 54.99% 0.44 min

>>>>>>>>>>>>>>>>>>>>>2021-07-20 07:24:34.958368>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 2e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: CE
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3500
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 2e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.1, 0.8, 1.0, 1.0, 1.2, 1.2, 1.2, 1.0, 1.0, 1.0]
> Epoch: 1 Step: 100, train loss: 2.0431 F1: 10.21% dev loss: 1.6747 F1: 12.00% 0.44 min
> Epoch: 2 Step: 200, train loss: 1.5454 F1: 19.88% dev loss: 1.1965 F1: 22.69% 0.43 min
> Epoch: 3 Step: 300, train loss: 1.0316 F1: 35.55% dev loss: 0.8039 F1: 36.72% 0.44 min
> Epoch: 5 Step: 400, train loss: 0.6795 F1: 58.81% dev loss: 0.7094 F1: 47.44% 0.44 min
> Epoch: 6 Step: 500, train loss: 0.4455 F1: 71.15% dev loss: 0.7053 F1: 48.65% 0.44 min
> Epoch: 7 Step: 600, train loss: 0.3079 F1: 78.75% dev loss: 0.7483 F1: 52.16% 0.44 min
> Epoch: 9 Step: 700, train loss: 0.2126 F1: 87.77% dev loss: 0.7815 F1: 51.48% 0.44 min
> Epoch: 10 Step: 800, train loss: 0.1587 F1: 89.99% dev loss: 0.8490 F1: 53.99% 0.44 min
> Epoch: 11 Step: 900, train loss: 0.1074 F1: 92.22% dev loss: 0.9726 F1: 48.73% 0.44 min
> Epoch: 13 Step: 1000, train loss: 0.0909 F1: 94.54% dev loss: 0.8860 F1: 54.36% 0.44 min
> Epoch: 14 Step: 1100, train loss: 0.0887 F1: 94.65% dev loss: 0.9479 F1: 55.16% 0.44 min
> Epoch: 15 Step: 1200, train loss: 0.0616 F1: 95.98% dev loss: 0.9491 F1: 53.18% 0.44 min
> Epoch: 17 Step: 1300, train loss: 0.0456 F1: 97.07% dev loss: 0.9780 F1: 55.32% 0.44 min
> Epoch: 18 Step: 1400, train loss: 0.0365 F1: 97.65% dev loss: 0.9674 F1: 53.14% 0.44 min
> Epoch: 19 Step: 1500, train loss: 0.0292 F1: 97.82% dev loss: 1.0068 F1: 54.91% 0.44 min
> Epoch: 21 Step: 1600, train loss: 0.0233 F1: 98.50% dev loss: 1.0213 F1: 54.56% 0.44 min
> Epoch: 22 Step: 1700, train loss: 0.0207 F1: 98.15% dev loss: 1.0224 F1: 53.34% 0.44 min
> Epoch: 23 Step: 1800, train loss: 0.0203 F1: 98.30% dev loss: 1.0710 F1: 56.04% 0.44 min
> Epoch: 25 Step: 1900, train loss: 0.0183 F1: 98.76% dev loss: 1.1155 F1: 56.40% 0.44 min
> Epoch: 26 Step: 2000, train loss: 0.0127 F1: 99.20% dev loss: 1.0888 F1: 54.77% 0.44 min
> Epoch: 27 Step: 2100, train loss: 0.0115 F1: 98.99% dev loss: 1.1392 F1: 56.43% 0.44 min
> Epoch: 29 Step: 2200, train loss: 0.0104 F1: 99.45% dev loss: 1.1543 F1: 56.39% 0.44 min
> Epoch: 30 Step: 2300, train loss: 0.0111 F1: 99.25% dev loss: 1.0904 F1: 53.59% 0.44 min
> Epoch: 31 Step: 2400, train loss: 0.0105 F1: 99.36% dev loss: 1.1550 F1: 55.99% 0.44 min
> Epoch: 33 Step: 2500, train loss: 0.0077 F1: 99.45% dev loss: 1.1661 F1: 55.84% 0.44 min
> Epoch: 34 Step: 2600, train loss: 0.0080 F1: 99.56% dev loss: 1.1536 F1: 53.99% 0.44 min
> Epoch: 35 Step: 2700, train loss: 0.0077 F1: 99.53% dev loss: 1.1888 F1: 55.19% 0.44 min
> Epoch: 37 Step: 2800, train loss: 0.0071 F1: 99.58% dev loss: 1.1941 F1: 56.47% 0.44 min
> Epoch: 38 Step: 2900, train loss: 0.0059 F1: 99.53% dev loss: 1.1698 F1: 56.22% 0.44 min
> Epoch: 39 Step: 3000, train loss: 0.0058 F1: 99.58% dev loss: 1.1924 F1: 55.48% 0.44 min
> Epoch: 41 Step: 3100, train loss: 0.0059 F1: 99.68% dev loss: 1.2062 F1: 55.98% 0.44 min
> Epoch: 42 Step: 3200, train loss: 0.0062 F1: 99.57% dev loss: 1.2025 F1: 55.36% 0.44 min
> Epoch: 43 Step: 3300, train loss: 0.0052 F1: 99.54% dev loss: 1.2078 F1: 55.06% 0.44 min
> Epoch: 45 Step: 3400, train loss: 0.0054 F1: 99.64% dev loss: 1.2091 F1: 55.07% 0.44 min
> Epoch: 46 Step: 3500, train loss: 0.0056 F1: 99.64% dev loss: 1.2017 F1: 55.03% 0.44 min

>>>>>>>>>>>>>>>>>>>>>2021-07-20 07:44:26.169923>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'torch.optim.adam.Adam'>
>>> loss: CE
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3500
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'amsgrad': False, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.1, 0.8, 1.0, 1.0, 1.2, 1.2, 1.2, 1.0, 1.0, 1.0]
> Epoch: 1 Step: 100, train loss: 1.9007 F1: 15.32% dev loss: 1.4424 F1: 20.81% 0.45 min
> Epoch: 2 Step: 200, train loss: 1.1624 F1: 33.15% dev loss: 0.8233 F1: 39.61% 0.44 min
> Epoch: 3 Step: 300, train loss: 0.6185 F1: 54.49% dev loss: 0.6962 F1: 49.69% 0.44 min
> Epoch: 5 Step: 400, train loss: 0.3877 F1: 75.03% dev loss: 0.8754 F1: 49.82% 0.44 min
> Epoch: 6 Step: 500, train loss: 0.2696 F1: 82.23% dev loss: 0.8243 F1: 47.97% 0.44 min
> Epoch: 7 Step: 600, train loss: 0.1824 F1: 86.16% dev loss: 0.9034 F1: 53.08% 0.45 min
> Epoch: 9 Step: 700, train loss: 0.1261 F1: 94.00% dev loss: 0.8741 F1: 55.52% 0.45 min
> Epoch: 10 Step: 800, train loss: 0.0813 F1: 95.31% dev loss: 1.1401 F1: 46.10% 0.45 min
> Epoch: 11 Step: 900, train loss: 0.0598 F1: 95.66% dev loss: 0.9250 F1: 54.25% 0.45 min
> Epoch: 13 Step: 1000, train loss: 0.0414 F1: 98.15% dev loss: 0.9689 F1: 58.68% 0.45 min
> Epoch: 14 Step: 1100, train loss: 0.0236 F1: 98.31% dev loss: 0.9887 F1: 61.86% 0.45 min
> Epoch: 15 Step: 1200, train loss: 0.0175 F1: 98.63% dev loss: 1.0721 F1: 61.60% 0.45 min
> Epoch: 17 Step: 1300, train loss: 0.0152 F1: 98.84% dev loss: 1.1068 F1: 58.35% 0.45 min
> Epoch: 18 Step: 1400, train loss: 0.0162 F1: 98.92% dev loss: 1.0163 F1: 58.61% 0.45 min
> Epoch: 19 Step: 1500, train loss: 0.0118 F1: 99.14% dev loss: 1.1158 F1: 59.51% 0.45 min
> Epoch: 21 Step: 1600, train loss: 0.0102 F1: 99.56% dev loss: 1.1723 F1: 56.97% 0.45 min
> Epoch: 22 Step: 1700, train loss: 0.0069 F1: 99.42% dev loss: 1.1156 F1: 59.75% 0.45 min
> Epoch: 23 Step: 1800, train loss: 0.0078 F1: 99.46% dev loss: 1.1624 F1: 60.39% 0.45 min
> Epoch: 25 Step: 1900, train loss: 0.0070 F1: 99.71% dev loss: 1.1987 F1: 59.68% 0.45 min
> Epoch: 26 Step: 2000, train loss: 0.0057 F1: 99.70% dev loss: 1.2260 F1: 59.02% 0.45 min
> Epoch: 27 Step: 2100, train loss: 0.0049 F1: 99.72% dev loss: 1.2001 F1: 59.69% 0.45 min
> Epoch: 29 Step: 2200, train loss: 0.0036 F1: 99.78% dev loss: 1.3053 F1: 58.80% 0.45 min
> Epoch: 30 Step: 2300, train loss: 0.0029 F1: 99.78% dev loss: 1.2381 F1: 58.03% 0.45 min
> Epoch: 31 Step: 2400, train loss: 0.0055 F1: 99.63% dev loss: 1.2477 F1: 57.86% 0.45 min
> Epoch: 33 Step: 2500, train loss: 0.0028 F1: 99.93% dev loss: 1.3004 F1: 59.08% 0.45 min
> Epoch: 34 Step: 2600, train loss: 0.0022 F1: 99.90% dev loss: 1.2792 F1: 57.07% 0.45 min
> Epoch: 35 Step: 2700, train loss: 0.0020 F1: 99.93% dev loss: 1.3256 F1: 59.28% 0.45 min
> Epoch: 37 Step: 2800, train loss: 0.0020 F1: 99.78% dev loss: 1.3050 F1: 58.07% 0.45 min
> Epoch: 38 Step: 2900, train loss: 0.0024 F1: 99.90% dev loss: 1.3348 F1: 58.73% 0.45 min
> Epoch: 39 Step: 3000, train loss: 0.0017 F1: 99.95% dev loss: 1.3312 F1: 58.26% 0.45 min
> Epoch: 41 Step: 3100, train loss: 0.0016 F1: 99.96% dev loss: 1.3389 F1: 58.32% 0.45 min
> Epoch: 42 Step: 3200, train loss: 0.0017 F1: 99.92% dev loss: 1.3158 F1: 57.82% 0.45 min
> Epoch: 43 Step: 3300, train loss: 0.0013 F1: 99.95% dev loss: 1.3301 F1: 58.45% 0.45 min
> Epoch: 45 Step: 3400, train loss: 0.0013 F1: 99.82% dev loss: 1.3352 F1: 58.45% 0.45 min
> Epoch: 46 Step: 3500, train loss: 0.0013 F1: 99.97% dev loss: 1.3305 F1: 58.33% 0.45 min

>>>>>>>>>>>>>>>>>>>>>2021-07-20 08:58:09.101378>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.Adafactor'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3500
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'scale_parameter': False, 'relative_step': False, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.5789 F1: 12.15% dev loss: 0.2200 F1: 9.40% 0.51 min
> Epoch: 2 Step: 200, train loss: 0.1911 F1: 29.32% dev loss: 0.1319 F1: 32.35% 0.51 min
> Epoch: 3 Step: 300, train loss: 0.1028 F1: 47.65% dev loss: 0.0951 F1: 46.29% 0.52 min
> Epoch: 5 Step: 400, train loss: 0.0604 F1: 70.69% dev loss: 0.1344 F1: 46.83% 0.52 min
> Epoch: 6 Step: 500, train loss: 0.0359 F1: 79.35% dev loss: 0.1289 F1: 49.43% 0.52 min
> Epoch: 7 Step: 600, train loss: 0.0229 F1: 85.15% dev loss: 0.1012 F1: 58.09% 0.52 min
> Epoch: 9 Step: 700, train loss: 0.0099 F1: 95.07% dev loss: 0.1251 F1: 56.41% 0.52 min
> Epoch: 10 Step: 800, train loss: 0.0074 F1: 96.37% dev loss: 0.1161 F1: 57.11% 0.52 min
> Epoch: 11 Step: 900, train loss: 0.0053 F1: 97.07% dev loss: 0.1259 F1: 55.65% 0.51 min
> Epoch: 13 Step: 1000, train loss: 0.0031 F1: 98.88% dev loss: 0.1428 F1: 54.33% 0.52 min
> Epoch: 14 Step: 1100, train loss: 0.0022 F1: 98.85% dev loss: 0.1467 F1: 57.91% 0.52 min
> Epoch: 15 Step: 1200, train loss: 0.0023 F1: 98.76% dev loss: 0.1311 F1: 59.57% 0.52 min
> Epoch: 17 Step: 1300, train loss: 0.0019 F1: 98.56% dev loss: 0.1485 F1: 57.20% 0.51 min
> Epoch: 18 Step: 1400, train loss: 0.0009 F1: 99.33% dev loss: 0.1464 F1: 54.90% 0.51 min
> Epoch: 19 Step: 1500, train loss: 0.0009 F1: 99.14% dev loss: 0.1449 F1: 57.68% 0.51 min
> Epoch: 21 Step: 1600, train loss: 0.0007 F1: 99.43% dev loss: 0.1627 F1: 54.48% 0.52 min
> Epoch: 22 Step: 1700, train loss: 0.0005 F1: 99.54% dev loss: 0.1544 F1: 56.54% 0.52 min
> Epoch: 23 Step: 1800, train loss: 0.0007 F1: 99.62% dev loss: 0.1574 F1: 55.82% 0.52 min
> Epoch: 25 Step: 1900, train loss: 0.0003 F1: 99.83% dev loss: 0.1675 F1: 55.52% 0.52 min
> Epoch: 26 Step: 2000, train loss: 0.0004 F1: 99.94% dev loss: 0.1582 F1: 55.73% 0.52 min
> Epoch: 27 Step: 2100, train loss: 0.0003 F1: 99.87% dev loss: 0.1662 F1: 54.77% 0.52 min
> Epoch: 29 Step: 2200, train loss: 0.0001 F1: 100.00% dev loss: 0.1691 F1: 55.64% 0.52 min
> Epoch: 30 Step: 2300, train loss: 0.0003 F1: 99.99% dev loss: 0.1675 F1: 54.66% 0.52 min
> Epoch: 31 Step: 2400, train loss: 0.0002 F1: 99.93% dev loss: 0.1717 F1: 55.43% 0.52 min
> Epoch: 33 Step: 2500, train loss: 0.0001 F1: 100.00% dev loss: 0.1684 F1: 56.19% 0.52 min
> Epoch: 34 Step: 2600, train loss: 0.0001 F1: 99.98% dev loss: 0.1725 F1: 55.88% 0.52 min
> Epoch: 35 Step: 2700, train loss: 0.0001 F1: 99.98% dev loss: 0.1798 F1: 54.42% 0.52 min
> Epoch: 37 Step: 2800, train loss: 0.0000 F1: 99.98% dev loss: 0.1733 F1: 56.12% 0.52 min
> Epoch: 38 Step: 2900, train loss: 0.0000 F1: 99.96% dev loss: 0.1821 F1: 54.71% 0.52 min
> Epoch: 39 Step: 3000, train loss: 0.0001 F1: 99.97% dev loss: 0.1743 F1: 54.86% 0.52 min
> Epoch: 41 Step: 3100, train loss: 0.0001 F1: 99.99% dev loss: 0.1767 F1: 55.51% 0.52 min
> Epoch: 42 Step: 3200, train loss: 0.0000 F1: 100.00% dev loss: 0.1778 F1: 55.42% 0.52 min
> Epoch: 43 Step: 3300, train loss: 0.0001 F1: 99.93% dev loss: 0.1780 F1: 55.81% 0.52 min
> Epoch: 45 Step: 3400, train loss: 0.0000 F1: 100.00% dev loss: 0.1763 F1: 55.42% 0.52 min
> Epoch: 46 Step: 3500, train loss: 0.0001 F1: 100.00% dev loss: 0.1777 F1: 55.10% 0.52 min

>>>>>>>>>>>>>>>>>>>>>2021-07-20 11:07:20.570624>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 2e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.25
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3500
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 2e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]
> Epoch: 1 Step: 100, train loss: 0.2453 F1: 9.32% dev loss: 0.0954 F1: 9.40% 0.44 min
> Epoch: 2 Step: 200, train loss: 0.0918 F1: 13.05% dev loss: 0.0711 F1: 9.40% 0.43 min
> Epoch: 3 Step: 300, train loss: 0.0673 F1: 18.29% dev loss: 0.0513 F1: 23.06% 0.43 min
> Epoch: 5 Step: 400, train loss: 0.0484 F1: 31.82% dev loss: 0.0410 F1: 33.78% 0.44 min
> Epoch: 6 Step: 500, train loss: 0.0327 F1: 51.33% dev loss: 0.0333 F1: 49.97% 0.44 min
> Epoch: 7 Step: 600, train loss: 0.0205 F1: 63.41% dev loss: 0.0317 F1: 51.44% 0.44 min
> Epoch: 9 Step: 700, train loss: 0.0141 F1: 77.03% dev loss: 0.0400 F1: 50.00% 0.44 min
> Epoch: 10 Step: 800, train loss: 0.0104 F1: 83.10% dev loss: 0.0362 F1: 53.48% 0.44 min
> Epoch: 11 Step: 900, train loss: 0.0075 F1: 85.80% dev loss: 0.0329 F1: 52.97% 0.44 min
> Epoch: 13 Step: 1000, train loss: 0.0059 F1: 91.13% dev loss: 0.0370 F1: 54.67% 0.44 min
> Epoch: 14 Step: 1100, train loss: 0.0047 F1: 91.85% dev loss: 0.0372 F1: 55.44% 0.44 min
> Epoch: 15 Step: 1200, train loss: 0.0042 F1: 91.74% dev loss: 0.0383 F1: 54.09% 0.44 min
> Epoch: 17 Step: 1300, train loss: 0.0034 F1: 93.71% dev loss: 0.0396 F1: 55.52% 0.44 min
> Epoch: 18 Step: 1400, train loss: 0.0030 F1: 94.12% dev loss: 0.0404 F1: 54.50% 0.44 min
> Epoch: 19 Step: 1500, train loss: 0.0025 F1: 95.11% dev loss: 0.0411 F1: 55.17% 0.44 min
> Epoch: 21 Step: 1600, train loss: 0.0017 F1: 96.64% dev loss: 0.0412 F1: 56.24% 0.44 min
> Epoch: 22 Step: 1700, train loss: 0.0014 F1: 96.45% dev loss: 0.0453 F1: 55.67% 0.44 min
> Epoch: 23 Step: 1800, train loss: 0.0013 F1: 97.12% dev loss: 0.0434 F1: 55.87% 0.44 min
> Epoch: 25 Step: 1900, train loss: 0.0010 F1: 98.14% dev loss: 0.0451 F1: 53.06% 0.44 min
> Epoch: 26 Step: 2000, train loss: 0.0008 F1: 98.79% dev loss: 0.0472 F1: 55.18% 0.44 min
> Epoch: 27 Step: 2100, train loss: 0.0007 F1: 98.54% dev loss: 0.0469 F1: 56.02% 0.44 min
> Epoch: 29 Step: 2200, train loss: 0.0006 F1: 98.87% dev loss: 0.0460 F1: 54.84% 0.44 min
> Epoch: 30 Step: 2300, train loss: 0.0006 F1: 99.21% dev loss: 0.0487 F1: 55.63% 0.44 min
> Epoch: 31 Step: 2400, train loss: 0.0006 F1: 98.74% dev loss: 0.0457 F1: 56.62% 0.44 min
> Epoch: 33 Step: 2500, train loss: 0.0005 F1: 98.99% dev loss: 0.0467 F1: 55.74% 0.44 min
> Epoch: 34 Step: 2600, train loss: 0.0005 F1: 99.10% dev loss: 0.0509 F1: 53.56% 0.44 min
> Epoch: 35 Step: 2700, train loss: 0.0004 F1: 99.24% dev loss: 0.0479 F1: 55.60% 0.44 min
> Epoch: 37 Step: 2800, train loss: 0.0003 F1: 99.43% dev loss: 0.0473 F1: 55.92% 0.44 min
> Epoch: 38 Step: 2900, train loss: 0.0004 F1: 99.41% dev loss: 0.0504 F1: 56.28% 0.44 min
> Epoch: 39 Step: 3000, train loss: 0.0004 F1: 99.42% dev loss: 0.0483 F1: 56.46% 0.44 min
> Epoch: 41 Step: 3100, train loss: 0.0003 F1: 99.66% dev loss: 0.0467 F1: 56.30% 0.44 min
> Epoch: 42 Step: 3200, train loss: 0.0003 F1: 99.34% dev loss: 0.0487 F1: 56.03% 0.44 min
> Epoch: 43 Step: 3300, train loss: 0.0002 F1: 99.58% dev loss: 0.0492 F1: 56.85% 0.44 min
> Epoch: 45 Step: 3400, train loss: 0.0002 F1: 99.63% dev loss: 0.0492 F1: 56.73% 0.44 min
> Epoch: 46 Step: 3500, train loss: 0.0003 F1: 99.37% dev loss: 0.0493 F1: 56.71% 0.44 min


>>>>>>>>>>>>>>>>>>>>>2021-07-20 11:31:13.660394>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 2e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'torch.optim.adam.Adam'>
>>> loss: CE
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3500
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'amsgrad': False, 'lr': 2e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.07, 1.0, 1.0, 1.0, 1.0, 1.0, 1.1, 1.0, 1.0, 1.0]
> Epoch: 1 Step: 100, train loss: 2.0985 F1: 10.95% dev loss: 1.7583 F1: 12.12% 0.45 min
> Epoch: 2 Step: 200, train loss: 1.6149 F1: 20.77% dev loss: 1.2394 F1: 22.12% 0.44 min
> Epoch: 3 Step: 300, train loss: 1.0616 F1: 40.94% dev loss: 0.8376 F1: 41.91% 0.44 min
> Epoch: 5 Step: 400, train loss: 0.7006 F1: 57.02% dev loss: 0.7698 F1: 49.77% 0.44 min
> Epoch: 6 Step: 500, train loss: 0.4619 F1: 70.77% dev loss: 0.7892 F1: 48.03% 0.44 min
> Epoch: 7 Step: 600, train loss: 0.3302 F1: 76.76% dev loss: 0.8005 F1: 50.90% 0.45 min
> Epoch: 9 Step: 700, train loss: 0.2237 F1: 86.60% dev loss: 0.8570 F1: 51.66% 0.45 min
> Epoch: 10 Step: 800, train loss: 0.1716 F1: 88.73% dev loss: 0.9488 F1: 52.79% 0.45 min
> Epoch: 11 Step: 900, train loss: 0.1125 F1: 91.77% dev loss: 0.9620 F1: 54.47% 0.45 min
> Epoch: 13 Step: 1000, train loss: 0.0888 F1: 95.03% dev loss: 0.9390 F1: 52.58% 0.45 min
> Epoch: 14 Step: 1100, train loss: 0.0708 F1: 94.87% dev loss: 1.0254 F1: 55.64% 0.45 min
> Epoch: 15 Step: 1200, train loss: 0.0606 F1: 95.19% dev loss: 1.0223 F1: 55.65% 0.45 min
> Epoch: 17 Step: 1300, train loss: 0.0468 F1: 96.74% dev loss: 1.0449 F1: 55.00% 0.45 min
> Epoch: 18 Step: 1400, train loss: 0.0405 F1: 96.99% dev loss: 1.0943 F1: 53.60% 0.45 min
> Epoch: 19 Step: 1500, train loss: 0.0320 F1: 96.87% dev loss: 1.1042 F1: 55.61% 0.45 min
> Epoch: 21 Step: 1600, train loss: 0.0260 F1: 97.77% dev loss: 1.1114 F1: 54.79% 0.45 min
> Epoch: 22 Step: 1700, train loss: 0.0212 F1: 98.02% dev loss: 1.1227 F1: 53.53% 0.45 min
> Epoch: 23 Step: 1800, train loss: 0.0219 F1: 97.83% dev loss: 1.1466 F1: 56.01% 0.45 min
> Epoch: 25 Step: 1900, train loss: 0.0167 F1: 98.32% dev loss: 1.1790 F1: 55.60% 0.45 min
> Epoch: 26 Step: 2000, train loss: 0.0158 F1: 98.38% dev loss: 1.1964 F1: 55.41% 0.45 min
> Epoch: 27 Step: 2100, train loss: 0.0133 F1: 98.59% dev loss: 1.2172 F1: 56.17% 0.45 min
> Epoch: 29 Step: 2200, train loss: 0.0115 F1: 99.21% dev loss: 1.2426 F1: 55.97% 0.45 min
> Epoch: 30 Step: 2300, train loss: 0.0131 F1: 98.71% dev loss: 1.1754 F1: 56.03% 0.45 min
> Epoch: 31 Step: 2400, train loss: 0.0111 F1: 98.97% dev loss: 1.2173 F1: 55.53% 0.45 min
> Epoch: 33 Step: 2500, train loss: 0.0098 F1: 99.35% dev loss: 1.2642 F1: 56.47% 0.45 min
> Epoch: 34 Step: 2600, train loss: 0.0107 F1: 99.13% dev loss: 1.2162 F1: 55.03% 0.45 min
> Epoch: 35 Step: 2700, train loss: 0.0090 F1: 99.24% dev loss: 1.2570 F1: 55.90% 0.45 min
> Epoch: 37 Step: 2800, train loss: 0.0071 F1: 99.57% dev loss: 1.3005 F1: 55.88% 0.45 min
> Epoch: 38 Step: 2900, train loss: 0.0073 F1: 99.42% dev loss: 1.2534 F1: 55.98% 0.45 min
> Epoch: 39 Step: 3000, train loss: 0.0071 F1: 99.12% dev loss: 1.2885 F1: 55.34% 0.45 min
> Epoch: 41 Step: 3100, train loss: 0.0070 F1: 99.35% dev loss: 1.2991 F1: 56.79% 0.45 min
> Epoch: 42 Step: 3200, train loss: 0.0062 F1: 99.43% dev loss: 1.2897 F1: 56.86% 0.44 min
> Epoch: 43 Step: 3300, train loss: 0.0062 F1: 99.45% dev loss: 1.3003 F1: 56.46% 0.44 min
> Epoch: 45 Step: 3400, train loss: 0.0058 F1: 99.43% dev loss: 1.3002 F1: 55.68% 0.44 min
> Epoch: 46 Step: 3500, train loss: 0.0061 F1: 99.47% dev loss: 1.2942 F1: 55.76% 0.44 min

>>>>>>>>>>>>>>>>>>>>>2021-07-20 11:57:28.951602>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'torch.optim.adam.Adam'>
>>> loss: CE
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3500
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': './data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': './data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': './data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'amsgrad': False, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.07, 1.0, 1.0, 1.0, 1.0, 1.0, 1.1, 1.0, 1.0, 1.0]
> Epoch: 1 Step: 100, train loss: 1.9640 F1: 15.19% dev loss: 1.5111 F1: 21.96% 0.44 min
> Epoch: 2 Step: 200, train loss: 1.2161 F1: 32.56% dev loss: 0.8625 F1: 42.84% 0.44 min
> Epoch: 3 Step: 300, train loss: 0.6586 F1: 54.64% dev loss: 0.7357 F1: 46.77% 0.44 min
> Epoch: 5 Step: 400, train loss: 0.4109 F1: 74.80% dev loss: 1.0046 F1: 47.34% 0.44 min
> Epoch: 6 Step: 500, train loss: 0.3035 F1: 80.32% dev loss: 0.9717 F1: 50.95% 0.44 min
> Epoch: 7 Step: 600, train loss: 0.2064 F1: 85.87% dev loss: 0.9320 F1: 52.79% 0.44 min
> Epoch: 9 Step: 700, train loss: 0.1241 F1: 93.90% dev loss: 0.9329 F1: 54.02% 0.44 min
> Epoch: 10 Step: 800, train loss: 0.1029 F1: 92.08% dev loss: 1.1374 F1: 52.17% 0.45 min
> Epoch: 11 Step: 900, train loss: 0.0734 F1: 93.96% dev loss: 1.0079 F1: 54.93% 0.45 min
> Epoch: 13 Step: 1000, train loss: 0.0403 F1: 97.37% dev loss: 1.1167 F1: 53.51% 0.45 min
> Epoch: 14 Step: 1100, train loss: 0.0262 F1: 98.09% dev loss: 1.1138 F1: 55.59% 0.45 min
> Epoch: 15 Step: 1200, train loss: 0.0180 F1: 98.33% dev loss: 1.3298 F1: 54.00% 0.44 min
> Epoch: 17 Step: 1300, train loss: 0.0174 F1: 98.58% dev loss: 1.2183 F1: 54.61% 0.44 min
> Epoch: 18 Step: 1400, train loss: 0.0098 F1: 99.17% dev loss: 1.2253 F1: 56.91% 0.44 min
> Epoch: 19 Step: 1500, train loss: 0.0120 F1: 98.98% dev loss: 1.2535 F1: 58.65% 0.44 min
> Epoch: 21 Step: 1600, train loss: 0.0096 F1: 99.02% dev loss: 1.2776 F1: 54.73% 0.44 min
> Epoch: 22 Step: 1700, train loss: 0.0129 F1: 98.70% dev loss: 1.1695 F1: 57.87% 0.45 min
> Epoch: 23 Step: 1800, train loss: 0.0164 F1: 99.07% dev loss: 1.2581 F1: 55.73% 0.45 min
> Epoch: 25 Step: 1900, train loss: 0.0096 F1: 99.43% dev loss: 1.3236 F1: 54.09% 0.45 min
> Epoch: 26 Step: 2000, train loss: 0.0103 F1: 99.23% dev loss: 1.3512 F1: 54.34% 0.45 min
> Epoch: 27 Step: 2100, train loss: 0.0085 F1: 99.26% dev loss: 1.4342 F1: 53.84% 0.45 min
> Epoch: 29 Step: 2200, train loss: 0.0091 F1: 99.37% dev loss: 1.3387 F1: 52.94% 0.45 min
> Epoch: 30 Step: 2300, train loss: 0.0078 F1: 99.53% dev loss: 1.3490 F1: 57.79% 0.44 min
> Epoch: 31 Step: 2400, train loss: 0.0089 F1: 99.32% dev loss: 1.3337 F1: 55.79% 0.45 min
> Epoch: 33 Step: 2500, train loss: 0.0032 F1: 99.79% dev loss: 1.3981 F1: 55.55% 0.45 min
> Epoch: 34 Step: 2600, train loss: 0.0029 F1: 99.84% dev loss: 1.3821 F1: 57.69% 0.45 min
> Epoch: 35 Step: 2700, train loss: 0.0041 F1: 99.73% dev loss: 1.3654 F1: 57.65% 0.45 min
> Epoch: 37 Step: 2800, train loss: 0.0026 F1: 99.81% dev loss: 1.4319 F1: 56.19% 0.45 min
> Epoch: 38 Step: 2900, train loss: 0.0035 F1: 99.84% dev loss: 1.4293 F1: 56.77% 0.45 min
> Epoch: 39 Step: 3000, train loss: 0.0028 F1: 99.80% dev loss: 1.4482 F1: 56.30% 0.45 min
> Epoch: 41 Step: 3100, train loss: 0.0023 F1: 99.64% dev loss: 1.4280 F1: 56.89% 0.45 min
> Epoch: 42 Step: 3200, train loss: 0.0028 F1: 99.83% dev loss: 1.4557 F1: 56.48% 0.45 min
> Epoch: 43 Step: 3300, train loss: 0.0021 F1: 99.83% dev loss: 1.4627 F1: 57.20% 0.45 min
> Epoch: 45 Step: 3400, train loss: 0.0021 F1: 99.86% dev loss: 1.4788 F1: 57.19% 0.45 min
> Epoch: 46 Step: 3500, train loss: 0.0039 F1: 99.88% dev loss: 1.4766 F1: 56.50% 0.45 min


>>>>>>>>>>>>>>>>>>>>>2021-07-21 06:10:07.767318>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-lstm_lap14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: lap14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.6755 F1: 9.61% dev loss: 0.1821 F1: 9.65% 0.47 min
> Epoch: 2 Step: 200, train loss: 0.1872 F1: 9.61% dev loss: 0.1637 F1: 9.65% 0.47 min
> Epoch: 3 Step: 300, train loss: 0.1600 F1: 9.62% dev loss: 0.1195 F1: 10.70% 0.48 min
> Epoch: 5 Step: 400, train loss: 0.1141 F1: 27.23% dev loss: 0.0877 F1: 28.74% 0.48 min
> Epoch: 6 Step: 500, train loss: 0.0718 F1: 48.61% dev loss: 0.0718 F1: 43.59% 0.48 min
> Epoch: 7 Step: 600, train loss: 0.0509 F1: 66.86% dev loss: 0.0661 F1: 58.25% 0.48 min
> Epoch: 9 Step: 700, train loss: 0.0344 F1: 86.06% dev loss: 0.0644 F1: 60.48% 0.48 min
> Epoch: 10 Step: 800, train loss: 0.0178 F1: 87.53% dev loss: 0.0648 F1: 60.46% 0.48 min
> Epoch: 11 Step: 900, train loss: 0.0128 F1: 90.32% dev loss: 0.0634 F1: 60.23% 0.48 min
> Epoch: 12 Step: 1000, train loss: 0.0091 F1: 94.33% dev loss: 0.0707 F1: 65.20% 0.48 min
> Epoch: 14 Step: 1100, train loss: 0.0063 F1: 95.43% dev loss: 0.0665 F1: 59.88% 0.48 min
> Epoch: 15 Step: 1200, train loss: 0.0045 F1: 96.68% dev loss: 0.0671 F1: 61.56% 0.48 min
> Epoch: 16 Step: 1300, train loss: 0.0030 F1: 97.44% dev loss: 0.0670 F1: 62.66% 0.48 min
> Epoch: 18 Step: 1400, train loss: 0.0027 F1: 97.37% dev loss: 0.0711 F1: 68.55% 0.48 min
> Epoch: 19 Step: 1500, train loss: 0.0022 F1: 97.65% dev loss: 0.0639 F1: 64.68% 0.48 min
> Epoch: 20 Step: 1600, train loss: 0.0015 F1: 98.75% dev loss: 0.0651 F1: 65.63% 0.48 min
> Epoch: 22 Step: 1700, train loss: 0.0013 F1: 99.66% dev loss: 0.0708 F1: 64.08% 0.48 min
> Epoch: 23 Step: 1800, train loss: 0.0010 F1: 99.43% dev loss: 0.0729 F1: 67.66% 0.48 min
> Epoch: 24 Step: 1900, train loss: 0.0009 F1: 99.46% dev loss: 0.0700 F1: 67.25% 0.48 min
> Epoch: 25 Step: 2000, train loss: 0.0007 F1: 99.57% dev loss: 0.0731 F1: 65.82% 0.48 min
> Epoch: 27 Step: 2100, train loss: 0.0006 F1: 99.58% dev loss: 0.0761 F1: 66.04% 0.48 min
> Epoch: 28 Step: 2200, train loss: 0.0008 F1: 99.38% dev loss: 0.0764 F1: 67.64% 0.48 min
> Epoch: 29 Step: 2300, train loss: 0.0006 F1: 99.70% dev loss: 0.0762 F1: 67.30% 0.48 min
> Epoch: 31 Step: 2400, train loss: 0.0005 F1: 99.90% dev loss: 0.0755 F1: 66.74% 0.48 min
> Epoch: 32 Step: 2500, train loss: 0.0005 F1: 99.64% dev loss: 0.0788 F1: 66.09% 0.48 min
> Epoch: 33 Step: 2600, train loss: 0.0004 F1: 99.72% dev loss: 0.0771 F1: 66.62% 0.48 min
> Epoch: 35 Step: 2700, train loss: 0.0005 F1: 99.83% dev loss: 0.0778 F1: 66.87% 0.48 min
> Epoch: 36 Step: 2800, train loss: 0.0003 F1: 99.87% dev loss: 0.0779 F1: 66.66% 0.48 min
> Epoch: 37 Step: 2900, train loss: 0.0005 F1: 99.74% dev loss: 0.0770 F1: 66.82% 0.48 min
> Epoch: 38 Step: 3000, train loss: 0.0003 F1: 99.86% dev loss: 0.0769 F1: 67.07% 0.48 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 06:23:32.097874>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 50
>>> step: 100
>>> model_name: bert
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-lstm_res16_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res16
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': './data/Semeval2016/processed/ABSA16_Restaurants_Train_SB1_v2.csv', 'dev': './data/Semeval2016/processed/ABSA16_Restaurants_dev_SB1_v2.csv', 'test': './data/Semeval2016/processed/ABSA16_Restaurants_test_SB1_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 2 Step: 100, train loss: 0.7116 F1: 9.65% dev loss: 0.2012 F1: 9.58% 1.35 min
> Epoch: 4 Step: 200, train loss: 0.2333 F1: 9.55% dev loss: 0.1752 F1: 9.58% 1.38 min
> Epoch: 6 Step: 300, train loss: 0.1769 F1: 21.21% dev loss: 0.1155 F1: 21.39% 1.38 min
> Epoch: 8 Step: 400, train loss: 0.1048 F1: 36.05% dev loss: 0.1076 F1: 35.32% 1.38 min
> Epoch: 10 Step: 500, train loss: 0.0667 F1: 56.11% dev loss: 0.0847 F1: 42.04% 1.38 min
> Epoch: 12 Step: 600, train loss: 0.0328 F1: 58.61% dev loss: 0.0818 F1: 45.91% 1.38 min
> Epoch: 14 Step: 700, train loss: 0.0198 F1: 63.20% dev loss: 0.1086 F1: 41.85% 1.38 min
> Epoch: 16 Step: 800, train loss: 0.0174 F1: 67.33% dev loss: 0.0801 F1: 50.96% 1.38 min
> Epoch: 18 Step: 900, train loss: 0.0086 F1: 75.39% dev loss: 0.0857 F1: 52.03% 1.38 min
> Epoch: 20 Step: 1000, train loss: 0.0055 F1: 91.97% dev loss: 0.0817 F1: 50.81% 1.38 min
> Epoch: 22 Step: 1100, train loss: 0.0034 F1: 97.39% dev loss: 0.0846 F1: 50.16% 1.38 min
> Epoch: 24 Step: 1200, train loss: 0.0026 F1: 97.21% dev loss: 0.0930 F1: 53.29% 1.38 min
> Epoch: 26 Step: 1300, train loss: 0.0024 F1: 96.97% dev loss: 0.0896 F1: 52.76% 1.38 min
> Epoch: 28 Step: 1400, train loss: 0.0015 F1: 97.74% dev loss: 0.0879 F1: 53.04% 1.38 min
> Epoch: 30 Step: 1500, train loss: 0.0012 F1: 98.26% dev loss: 0.0936 F1: 51.51% 1.38 min
> Epoch: 32 Step: 1600, train loss: 0.0009 F1: 98.46% dev loss: 0.0928 F1: 53.78% 1.38 min
> Epoch: 34 Step: 1700, train loss: 0.0007 F1: 99.70% dev loss: 0.0958 F1: 50.41% 1.38 min
> Epoch: 36 Step: 1800, train loss: 0.0007 F1: 99.78% dev loss: 0.0954 F1: 53.54% 1.38 min
> Epoch: 38 Step: 1900, train loss: 0.0004 F1: 99.81% dev loss: 0.0964 F1: 53.72% 1.38 min
> Epoch: 40 Step: 2000, train loss: 0.0005 F1: 99.85% dev loss: 0.1002 F1: 52.03% 1.39 min
> Epoch: 42 Step: 2100, train loss: 0.0004 F1: 99.92% dev loss: 0.1006 F1: 53.50% 1.39 min
> Epoch: 44 Step: 2200, train loss: 0.0004 F1: 99.93% dev loss: 0.1049 F1: 53.24% 1.39 min
> Epoch: 46 Step: 2300, train loss: 0.0003 F1: 99.88% dev loss: 0.1033 F1: 53.78% 1.38 min
> Epoch: 48 Step: 2400, train loss: 0.0003 F1: 99.88% dev loss: 0.1048 F1: 52.73% 1.38 min


>>>>>>>>>>>>>>>>>>>>>2021-07-21 07:42:15.090636>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: san
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-san_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.5306 F1: 10.28% dev loss: 0.2305 F1: 9.40% 0.46 min
> Epoch: 2 Step: 200, train loss: 0.2209 F1: 19.24% dev loss: 0.1538 F1: 22.86% 0.45 min
> Epoch: 3 Step: 300, train loss: 0.1339 F1: 37.96% dev loss: 0.1054 F1: 49.59% 0.46 min
> Epoch: 5 Step: 400, train loss: 0.0800 F1: 59.81% dev loss: 0.1081 F1: 48.92% 0.46 min
> Epoch: 6 Step: 500, train loss: 0.0451 F1: 74.66% dev loss: 0.1033 F1: 51.59% 0.46 min
> Epoch: 7 Step: 600, train loss: 0.0265 F1: 83.57% dev loss: 0.1199 F1: 50.25% 0.46 min
> Epoch: 9 Step: 700, train loss: 0.0208 F1: 89.60% dev loss: 0.1421 F1: 50.95% 0.46 min
> Epoch: 10 Step: 800, train loss: 0.0137 F1: 92.62% dev loss: 0.1285 F1: 54.89% 0.46 min
> Epoch: 11 Step: 900, train loss: 0.0081 F1: 94.14% dev loss: 0.1203 F1: 55.14% 0.46 min
> Epoch: 13 Step: 1000, train loss: 0.0058 F1: 96.05% dev loss: 0.1429 F1: 54.45% 0.46 min
> Epoch: 14 Step: 1100, train loss: 0.0041 F1: 97.39% dev loss: 0.1529 F1: 50.66% 0.46 min
> Epoch: 15 Step: 1200, train loss: 0.0026 F1: 98.18% dev loss: 0.1420 F1: 54.36% 0.46 min
> Epoch: 17 Step: 1300, train loss: 0.0020 F1: 98.58% dev loss: 0.1385 F1: 56.92% 0.46 min
> Epoch: 18 Step: 1400, train loss: 0.0014 F1: 99.32% dev loss: 0.1633 F1: 51.76% 0.46 min
> Epoch: 19 Step: 1500, train loss: 0.0016 F1: 98.87% dev loss: 0.1407 F1: 56.00% 0.46 min
> Epoch: 21 Step: 1600, train loss: 0.0011 F1: 99.03% dev loss: 0.1517 F1: 54.58% 0.46 min
> Epoch: 22 Step: 1700, train loss: 0.0011 F1: 99.04% dev loss: 0.1568 F1: 57.90% 0.46 min
> Epoch: 23 Step: 1800, train loss: 0.0008 F1: 99.69% dev loss: 0.1537 F1: 55.85% 0.46 min
> Epoch: 25 Step: 1900, train loss: 0.0006 F1: 99.75% dev loss: 0.1580 F1: 55.28% 0.46 min
> Epoch: 26 Step: 2000, train loss: 0.0007 F1: 99.43% dev loss: 0.1656 F1: 54.75% 0.46 min
> Epoch: 27 Step: 2100, train loss: 0.0004 F1: 99.79% dev loss: 0.1585 F1: 55.40% 0.46 min
> Epoch: 29 Step: 2200, train loss: 0.0003 F1: 99.95% dev loss: 0.1621 F1: 55.58% 0.46 min
> Epoch: 30 Step: 2300, train loss: 0.0005 F1: 99.63% dev loss: 0.1725 F1: 54.70% 0.46 min
> Epoch: 31 Step: 2400, train loss: 0.0004 F1: 99.89% dev loss: 0.1683 F1: 55.29% 0.46 min
> Epoch: 33 Step: 2500, train loss: 0.0004 F1: 99.91% dev loss: 0.1605 F1: 55.34% 0.46 min
> Epoch: 34 Step: 2600, train loss: 0.0002 F1: 99.98% dev loss: 0.1630 F1: 56.43% 0.46 min
> Epoch: 35 Step: 2700, train loss: 0.0003 F1: 99.91% dev loss: 0.1667 F1: 55.57% 0.46 min
> Epoch: 37 Step: 2800, train loss: 0.0002 F1: 99.91% dev loss: 0.1654 F1: 55.86% 0.46 min
> Epoch: 38 Step: 2900, train loss: 0.0002 F1: 99.89% dev loss: 0.1677 F1: 54.79% 0.46 min
> Epoch: 39 Step: 3000, train loss: 0.0002 F1: 99.91% dev loss: 0.1670 F1: 54.85% 0.46 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 07:56:59.512749>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: san
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-san_lap14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: lap14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.4301 F1: 9.61% dev loss: 0.1650 F1: 9.65% 0.47 min
> Epoch: 2 Step: 200, train loss: 0.1613 F1: 13.41% dev loss: 0.1216 F1: 13.17% 0.46 min
> Epoch: 3 Step: 300, train loss: 0.1160 F1: 29.06% dev loss: 0.0784 F1: 34.24% 0.46 min
> Epoch: 5 Step: 400, train loss: 0.0720 F1: 54.86% dev loss: 0.0551 F1: 54.03% 0.46 min
> Epoch: 6 Step: 500, train loss: 0.0410 F1: 71.56% dev loss: 0.0567 F1: 59.17% 0.46 min
> Epoch: 7 Step: 600, train loss: 0.0241 F1: 80.30% dev loss: 0.0596 F1: 55.62% 0.46 min
> Epoch: 9 Step: 700, train loss: 0.0144 F1: 91.61% dev loss: 0.0559 F1: 63.17% 0.46 min
> Epoch: 10 Step: 800, train loss: 0.0074 F1: 92.38% dev loss: 0.0719 F1: 62.71% 0.46 min
> Epoch: 11 Step: 900, train loss: 0.0057 F1: 94.76% dev loss: 0.0630 F1: 65.27% 0.46 min
> Epoch: 12 Step: 1000, train loss: 0.0046 F1: 95.10% dev loss: 0.0737 F1: 63.53% 0.46 min
> Epoch: 14 Step: 1100, train loss: 0.0034 F1: 98.27% dev loss: 0.0712 F1: 60.72% 0.46 min
> Epoch: 15 Step: 1200, train loss: 0.0020 F1: 98.23% dev loss: 0.0760 F1: 58.69% 0.46 min
> Epoch: 16 Step: 1300, train loss: 0.0020 F1: 98.22% dev loss: 0.0810 F1: 62.35% 0.46 min
> Epoch: 18 Step: 1400, train loss: 0.0015 F1: 98.24% dev loss: 0.0780 F1: 58.72% 0.46 min
> Epoch: 19 Step: 1500, train loss: 0.0013 F1: 98.28% dev loss: 0.0749 F1: 61.02% 0.46 min
> Epoch: 20 Step: 1600, train loss: 0.0011 F1: 98.34% dev loss: 0.0788 F1: 63.02% 0.46 min
> Epoch: 22 Step: 1700, train loss: 0.0010 F1: 98.86% dev loss: 0.0797 F1: 62.94% 0.46 min
> Epoch: 23 Step: 1800, train loss: 0.0007 F1: 99.12% dev loss: 0.0838 F1: 60.51% 0.46 min
> Epoch: 24 Step: 1900, train loss: 0.0005 F1: 99.29% dev loss: 0.0830 F1: 66.31% 0.46 min
> Epoch: 25 Step: 2000, train loss: 0.0007 F1: 99.19% dev loss: 0.0838 F1: 66.29% 0.46 min
> Epoch: 27 Step: 2100, train loss: 0.0004 F1: 99.71% dev loss: 0.0830 F1: 65.65% 0.46 min
> Epoch: 28 Step: 2200, train loss: 0.0006 F1: 99.08% dev loss: 0.0955 F1: 62.84% 0.46 min
> Epoch: 29 Step: 2300, train loss: 0.0007 F1: 99.75% dev loss: 0.0807 F1: 62.55% 0.46 min
> Epoch: 31 Step: 2400, train loss: 0.0005 F1: 99.52% dev loss: 0.0848 F1: 66.41% 0.46 min
> Epoch: 32 Step: 2500, train loss: 0.0003 F1: 99.70% dev loss: 0.0825 F1: 61.96% 0.46 min
> Epoch: 33 Step: 2600, train loss: 0.0002 F1: 99.71% dev loss: 0.0826 F1: 61.39% 0.46 min
> Epoch: 35 Step: 2700, train loss: 0.0002 F1: 100.00% dev loss: 0.0837 F1: 63.20% 0.46 min
> Epoch: 36 Step: 2800, train loss: 0.0002 F1: 99.61% dev loss: 0.0836 F1: 62.28% 0.46 min
> Epoch: 37 Step: 2900, train loss: 0.0001 F1: 99.84% dev loss: 0.0840 F1: 63.23% 0.46 min
> Epoch: 38 Step: 3000, train loss: 0.0002 F1: 99.81% dev loss: 0.0843 F1: 63.24% 0.46 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 08:11:26.215273>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: san
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-san_res16_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res16
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_Train_SB1_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_dev_SB1_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_test_SB1_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 2 Step: 100, train loss: 0.4952 F1: 9.65% dev loss: 0.1891 F1: 9.58% 0.46 min
> Epoch: 4 Step: 200, train loss: 0.1872 F1: 17.36% dev loss: 0.1217 F1: 18.64% 0.46 min
> Epoch: 6 Step: 300, train loss: 0.1161 F1: 36.31% dev loss: 0.0951 F1: 31.55% 0.46 min
> Epoch: 8 Step: 400, train loss: 0.0621 F1: 53.48% dev loss: 0.0810 F1: 37.99% 0.46 min
> Epoch: 10 Step: 500, train loss: 0.0321 F1: 64.19% dev loss: 0.1036 F1: 40.30% 0.46 min
> Epoch: 12 Step: 600, train loss: 0.0267 F1: 83.26% dev loss: 0.0882 F1: 40.05% 0.46 min
> Epoch: 14 Step: 700, train loss: 0.0144 F1: 91.49% dev loss: 0.0857 F1: 44.31% 0.46 min
> Epoch: 16 Step: 800, train loss: 0.0049 F1: 96.64% dev loss: 0.0875 F1: 47.83% 0.46 min
> Epoch: 18 Step: 900, train loss: 0.0022 F1: 97.42% dev loss: 0.0894 F1: 49.33% 0.46 min
> Epoch: 20 Step: 1000, train loss: 0.0017 F1: 98.56% dev loss: 0.0896 F1: 51.18% 0.46 min
> Epoch: 22 Step: 1100, train loss: 0.0014 F1: 99.04% dev loss: 0.0900 F1: 52.07% 0.46 min
> Epoch: 24 Step: 1200, train loss: 0.0010 F1: 99.28% dev loss: 0.0929 F1: 51.07% 0.46 min
> Epoch: 26 Step: 1300, train loss: 0.0009 F1: 99.01% dev loss: 0.0978 F1: 48.56% 0.46 min
> Epoch: 28 Step: 1400, train loss: 0.0006 F1: 99.33% dev loss: 0.0984 F1: 48.19% 0.46 min
> Epoch: 30 Step: 1500, train loss: 0.0005 F1: 99.72% dev loss: 0.0989 F1: 52.10% 0.46 min
> Epoch: 32 Step: 1600, train loss: 0.0003 F1: 99.85% dev loss: 0.1019 F1: 48.73% 0.46 min
> Epoch: 34 Step: 1700, train loss: 0.0004 F1: 99.74% dev loss: 0.1053 F1: 51.65% 0.46 min
> Epoch: 36 Step: 1800, train loss: 0.0003 F1: 99.69% dev loss: 0.1058 F1: 51.19% 0.46 min
> Epoch: 38 Step: 1900, train loss: 0.0002 F1: 99.79% dev loss: 0.1052 F1: 50.27% 0.46 min
> Epoch: 40 Step: 2000, train loss: 0.0002 F1: 99.58% dev loss: 0.1062 F1: 50.00% 0.46 min
> Epoch: 42 Step: 2100, train loss: 0.0003 F1: 99.83% dev loss: 0.1055 F1: 51.49% 0.46 min
> Epoch: 44 Step: 2200, train loss: 0.0002 F1: 99.80% dev loss: 0.1057 F1: 51.87% 0.46 min
> Epoch: 46 Step: 2300, train loss: 0.0002 F1: 99.94% dev loss: 0.1091 F1: 51.75% 0.46 min
> Epoch: 48 Step: 2400, train loss: 0.0002 F1: 99.88% dev loss: 0.1107 F1: 48.99% 0.46 min
> Epoch: 51 Step: 2500, train loss: 0.0002 F1: 70.00% dev loss: 0.1090 F1: 51.39% 0.46 min
> Epoch: 53 Step: 2600, train loss: 0.0001 F1: 80.00% dev loss: 0.1108 F1: 52.95% 0.46 min
> Epoch: 55 Step: 2700, train loss: 0.0002 F1: 80.00% dev loss: 0.1102 F1: 49.77% 0.46 min
> Epoch: 57 Step: 2800, train loss: 0.0002 F1: 80.00% dev loss: 0.1078 F1: 50.36% 0.46 min
> Epoch: 59 Step: 2900, train loss: 0.0001 F1: 99.85% dev loss: 0.1075 F1: 50.36% 0.46 min
> Epoch: 61 Step: 3000, train loss: 0.0002 F1: 100.00% dev loss: 0.1085 F1: 51.01% 0.46 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 08:25:53.901971>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: crf
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-crf_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 19.6394 F1: 9.32% dev loss: 9.9610 F1: 9.40% 0.58 min
> Epoch: 2 Step: 200, train loss: 8.2837 F1: 15.36% dev loss: 5.8227 F1: 18.94% 0.58 min
> Epoch: 3 Step: 300, train loss: 5.0883 F1: 28.38% dev loss: 4.4099 F1: 34.57% 0.58 min
> Epoch: 5 Step: 400, train loss: 3.2733 F1: 56.23% dev loss: 3.8292 F1: 47.81% 0.58 min
> Epoch: 6 Step: 500, train loss: 2.1783 F1: 67.98% dev loss: 3.9598 F1: 52.75% 0.58 min
> Epoch: 7 Step: 600, train loss: 1.5690 F1: 77.05% dev loss: 3.9411 F1: 55.26% 0.58 min
> Epoch: 9 Step: 700, train loss: 0.9813 F1: 90.96% dev loss: 6.0830 F1: 53.66% 0.58 min
> Epoch: 10 Step: 800, train loss: 0.7549 F1: 92.30% dev loss: 5.3897 F1: 54.89% 0.58 min
> Epoch: 11 Step: 900, train loss: 0.5246 F1: 94.09% dev loss: 4.5310 F1: 61.00% 0.58 min
> Epoch: 13 Step: 1000, train loss: 0.3569 F1: 96.91% dev loss: 5.2990 F1: 60.22% 0.58 min
> Epoch: 14 Step: 1100, train loss: 0.2576 F1: 97.14% dev loss: 4.9426 F1: 57.42% 0.58 min
> Epoch: 15 Step: 1200, train loss: 0.1956 F1: 98.44% dev loss: 5.3022 F1: 58.33% 0.58 min
> Epoch: 17 Step: 1300, train loss: 0.1300 F1: 99.16% dev loss: 5.5540 F1: 58.40% 0.58 min
> Epoch: 18 Step: 1400, train loss: 0.1183 F1: 98.88% dev loss: 5.9440 F1: 58.82% 0.58 min
> Epoch: 19 Step: 1500, train loss: 0.0758 F1: 98.93% dev loss: 5.6717 F1: 59.04% 0.58 min
> Epoch: 21 Step: 1600, train loss: 0.0573 F1: 99.54% dev loss: 5.5554 F1: 58.11% 0.58 min
> Epoch: 22 Step: 1700, train loss: 0.0404 F1: 99.84% dev loss: 5.9215 F1: 58.18% 0.58 min
> Epoch: 23 Step: 1800, train loss: 0.0517 F1: 99.64% dev loss: 5.5165 F1: 59.72% 0.58 min
> Epoch: 25 Step: 1900, train loss: 0.0454 F1: 99.69% dev loss: 5.5744 F1: 61.05% 0.57 min
> Epoch: 26 Step: 2000, train loss: 0.0279 F1: 99.87% dev loss: 5.8940 F1: 60.21% 0.58 min
> Epoch: 27 Step: 2100, train loss: 0.0245 F1: 99.90% dev loss: 6.1429 F1: 59.44% 0.58 min
> Epoch: 29 Step: 2200, train loss: 0.0260 F1: 99.95% dev loss: 5.9584 F1: 59.56% 0.58 min
> Epoch: 30 Step: 2300, train loss: 0.0308 F1: 99.85% dev loss: 6.1743 F1: 59.00% 0.58 min
> Epoch: 31 Step: 2400, train loss: 0.0262 F1: 99.89% dev loss: 6.1652 F1: 59.27% 0.58 min
> Epoch: 33 Step: 2500, train loss: 0.0222 F1: 99.90% dev loss: 6.1233 F1: 60.39% 0.57 min
> Epoch: 34 Step: 2600, train loss: 0.0222 F1: 99.94% dev loss: 6.1470 F1: 59.06% 0.57 min
> Epoch: 35 Step: 2700, train loss: 0.0141 F1: 99.97% dev loss: 6.2172 F1: 59.06% 0.58 min
> Epoch: 37 Step: 2800, train loss: 0.0168 F1: 99.99% dev loss: 6.1574 F1: 59.52% 0.58 min
> Epoch: 38 Step: 2900, train loss: 0.0151 F1: 99.95% dev loss: 6.1708 F1: 59.27% 0.58 min
> Epoch: 39 Step: 3000, train loss: 0.0151 F1: 99.95% dev loss: 6.1853 F1: 59.26% 0.58 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 08:44:13.726094>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: crf
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-crf_lap14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: lap14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 18.0885 F1: 9.61% dev loss: 6.7916 F1: 9.65% 0.58 min
> Epoch: 2 Step: 200, train loss: 6.4973 F1: 12.57% dev loss: 4.2428 F1: 14.67% 0.57 min
> Epoch: 3 Step: 300, train loss: 4.2253 F1: 25.34% dev loss: 3.0603 F1: 32.63% 0.57 min
> Epoch: 5 Step: 400, train loss: 2.6920 F1: 55.59% dev loss: 2.8231 F1: 50.02% 0.58 min
> Epoch: 6 Step: 500, train loss: 1.7616 F1: 65.76% dev loss: 2.5649 F1: 55.30% 0.58 min
> Epoch: 7 Step: 600, train loss: 1.2753 F1: 78.04% dev loss: 2.9524 F1: 57.98% 0.58 min
> Epoch: 9 Step: 700, train loss: 0.8512 F1: 87.80% dev loss: 2.8719 F1: 55.63% 0.58 min
> Epoch: 10 Step: 800, train loss: 0.5030 F1: 90.87% dev loss: 2.9400 F1: 61.30% 0.58 min
> Epoch: 11 Step: 900, train loss: 0.3947 F1: 90.75% dev loss: 2.9958 F1: 61.02% 0.58 min
> Epoch: 12 Step: 1000, train loss: 0.2488 F1: 96.05% dev loss: 3.2843 F1: 69.85% 0.58 min
> Epoch: 14 Step: 1100, train loss: 0.1679 F1: 98.94% dev loss: 3.1415 F1: 66.16% 0.58 min
> Epoch: 15 Step: 1200, train loss: 0.1493 F1: 98.31% dev loss: 3.0656 F1: 67.94% 0.59 min
> Epoch: 16 Step: 1300, train loss: 0.1005 F1: 99.04% dev loss: 3.2808 F1: 66.33% 0.58 min
> Epoch: 18 Step: 1400, train loss: 0.0775 F1: 99.49% dev loss: 3.3615 F1: 68.68% 0.58 min
> Epoch: 19 Step: 1500, train loss: 0.0660 F1: 99.07% dev loss: 3.4538 F1: 65.97% 0.59 min
> Epoch: 20 Step: 1600, train loss: 0.0591 F1: 99.37% dev loss: 3.5298 F1: 68.17% 0.59 min
> Epoch: 22 Step: 1700, train loss: 0.0555 F1: 99.90% dev loss: 3.6935 F1: 62.15% 0.59 min
> Epoch: 23 Step: 1800, train loss: 0.0349 F1: 99.70% dev loss: 3.7858 F1: 65.71% 0.59 min
> Epoch: 24 Step: 1900, train loss: 0.0451 F1: 99.51% dev loss: 3.6670 F1: 65.28% 0.61 min
> Epoch: 25 Step: 2000, train loss: 0.0251 F1: 99.84% dev loss: 3.8335 F1: 63.46% 0.59 min
> Epoch: 27 Step: 2100, train loss: 0.0205 F1: 99.97% dev loss: 3.7154 F1: 64.28% 0.59 min
> Epoch: 28 Step: 2200, train loss: 0.0341 F1: 99.71% dev loss: 3.8088 F1: 64.58% 0.59 min
> Epoch: 29 Step: 2300, train loss: 0.0164 F1: 99.86% dev loss: 3.7904 F1: 66.12% 0.59 min
> Epoch: 31 Step: 2400, train loss: 0.0164 F1: 100.00% dev loss: 3.8274 F1: 65.31% 0.59 min
> Epoch: 32 Step: 2500, train loss: 0.0154 F1: 99.72% dev loss: 3.8795 F1: 66.07% 0.59 min
> Epoch: 33 Step: 2600, train loss: 0.0169 F1: 99.83% dev loss: 3.8686 F1: 66.68% 0.60 min
> Epoch: 35 Step: 2700, train loss: 0.0151 F1: 100.00% dev loss: 3.8077 F1: 65.27% 0.60 min
> Epoch: 36 Step: 2800, train loss: 0.0153 F1: 99.75% dev loss: 3.8229 F1: 65.78% 0.60 min
> Epoch: 37 Step: 2900, train loss: 0.0146 F1: 99.95% dev loss: 3.8053 F1: 65.66% 0.60 min
> Epoch: 38 Step: 3000, train loss: 0.0096 F1: 100.00% dev loss: 3.8106 F1: 65.27% 0.60 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 09:02:24.456640>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: crf
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-crf_res16_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res16
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_Train_SB1_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_dev_SB1_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_test_SB1_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 2 Step: 100, train loss: 17.6623 F1: 9.65% dev loss: 7.3984 F1: 9.58% 0.58 min
> Epoch: 4 Step: 200, train loss: 7.2827 F1: 18.80% dev loss: 4.3231 F1: 17.92% 0.58 min
> Epoch: 6 Step: 300, train loss: 4.1320 F1: 34.52% dev loss: 3.5198 F1: 28.13% 0.58 min
> Epoch: 8 Step: 400, train loss: 2.1660 F1: 51.22% dev loss: 3.5334 F1: 40.90% 0.58 min
> Epoch: 10 Step: 500, train loss: 1.2855 F1: 59.00% dev loss: 4.8337 F1: 36.97% 0.58 min
> Epoch: 12 Step: 600, train loss: 1.1729 F1: 62.60% dev loss: 4.1944 F1: 40.55% 0.57 min
> Epoch: 14 Step: 700, train loss: 0.5372 F1: 73.81% dev loss: 4.5983 F1: 40.25% 0.57 min
> Epoch: 16 Step: 800, train loss: 0.4142 F1: 85.65% dev loss: 4.2122 F1: 45.43% 0.57 min
> Epoch: 18 Step: 900, train loss: 0.2362 F1: 95.95% dev loss: 3.9907 F1: 48.55% 0.58 min
> Epoch: 20 Step: 1000, train loss: 0.1049 F1: 99.10% dev loss: 4.2998 F1: 48.07% 0.58 min
> Epoch: 22 Step: 1100, train loss: 0.0724 F1: 99.46% dev loss: 4.4678 F1: 50.16% 0.57 min
> Epoch: 24 Step: 1200, train loss: 0.0600 F1: 99.19% dev loss: 4.5929 F1: 50.56% 0.57 min
> Epoch: 26 Step: 1300, train loss: 0.0564 F1: 99.60% dev loss: 4.5071 F1: 49.91% 0.58 min
> Epoch: 28 Step: 1400, train loss: 0.0361 F1: 99.75% dev loss: 4.7938 F1: 51.39% 0.58 min
> Epoch: 30 Step: 1500, train loss: 0.0353 F1: 99.58% dev loss: 4.9011 F1: 51.22% 0.57 min
> Epoch: 32 Step: 1600, train loss: 0.0306 F1: 99.96% dev loss: 4.8433 F1: 47.19% 0.57 min
> Epoch: 34 Step: 1700, train loss: 0.0371 F1: 99.66% dev loss: 5.0908 F1: 46.60% 0.59 min
> Epoch: 36 Step: 1800, train loss: 0.0302 F1: 99.60% dev loss: 5.0990 F1: 48.25% 0.58 min
> Epoch: 38 Step: 1900, train loss: 0.0330 F1: 99.85% dev loss: 4.9259 F1: 48.45% 0.57 min
> Epoch: 40 Step: 2000, train loss: 0.0249 F1: 99.94% dev loss: 5.2340 F1: 47.01% 0.57 min
> Epoch: 42 Step: 2100, train loss: 0.0211 F1: 99.95% dev loss: 5.3035 F1: 46.87% 0.57 min
> Epoch: 44 Step: 2200, train loss: 0.0203 F1: 99.93% dev loss: 5.0112 F1: 48.40% 0.58 min
> Epoch: 46 Step: 2300, train loss: 0.0219 F1: 99.95% dev loss: 5.3024 F1: 46.89% 0.57 min
> Epoch: 48 Step: 2400, train loss: 0.0166 F1: 99.97% dev loss: 5.2163 F1: 47.04% 0.58 min
> Epoch: 51 Step: 2500, train loss: 0.0136 F1: 70.00% dev loss: 5.1593 F1: 47.30% 0.58 min
> Epoch: 53 Step: 2600, train loss: 0.0173 F1: 80.00% dev loss: 5.4815 F1: 47.23% 0.58 min
> Epoch: 55 Step: 2700, train loss: 0.0142 F1: 80.00% dev loss: 5.2449 F1: 44.92% 0.58 min
> Epoch: 57 Step: 2800, train loss: 0.0132 F1: 80.00% dev loss: 5.3997 F1: 47.21% 0.58 min
> Epoch: 59 Step: 2900, train loss: 0.0106 F1: 100.00% dev loss: 5.2332 F1: 48.29% 0.58 min
> Epoch: 61 Step: 3000, train loss: 0.0097 F1: 99.98% dev loss: 5.2520 F1: 48.41% 0.58 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 09:26:12.621364>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res14_seed77.pth
>>> seed: 77
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.5689 F1: 9.34% dev loss: 0.2529 F1: 9.34% 0.43 min
> Epoch: 2 Step: 200, train loss: 0.2230 F1: 18.63% dev loss: 0.1677 F1: 23.34% 0.43 min
> Epoch: 3 Step: 300, train loss: 0.1316 F1: 35.09% dev loss: 0.1088 F1: 32.69% 0.43 min
> Epoch: 5 Step: 400, train loss: 0.0737 F1: 63.94% dev loss: 0.0885 F1: 52.53% 0.43 min
> Epoch: 6 Step: 500, train loss: 0.0468 F1: 77.42% dev loss: 0.0959 F1: 47.39% 0.43 min
> Epoch: 7 Step: 600, train loss: 0.0289 F1: 83.77% dev loss: 0.0966 F1: 48.31% 0.43 min
> Epoch: 9 Step: 700, train loss: 0.0202 F1: 86.78% dev loss: 0.1256 F1: 52.51% 0.44 min
> Epoch: 10 Step: 800, train loss: 0.0132 F1: 90.76% dev loss: 0.1175 F1: 51.74% 0.44 min
> Epoch: 11 Step: 900, train loss: 0.0072 F1: 95.04% dev loss: 0.1218 F1: 52.79% 0.44 min
> Epoch: 12 Step: 1000, train loss: 0.0046 F1: 97.22% dev loss: 0.1202 F1: 51.16% 0.44 min
> Epoch: 14 Step: 1100, train loss: 0.0035 F1: 97.91% dev loss: 0.1273 F1: 53.71% 0.44 min
> Epoch: 15 Step: 1200, train loss: 0.0027 F1: 98.41% dev loss: 0.1382 F1: 51.05% 0.44 min
> Epoch: 16 Step: 1300, train loss: 0.0023 F1: 98.31% dev loss: 0.1400 F1: 53.66% 0.44 min
> Epoch: 18 Step: 1400, train loss: 0.0017 F1: 98.82% dev loss: 0.1453 F1: 49.54% 0.43 min
> Epoch: 19 Step: 1500, train loss: 0.0016 F1: 99.23% dev loss: 0.1348 F1: 53.53% 0.44 min
> Epoch: 20 Step: 1600, train loss: 0.0013 F1: 99.39% dev loss: 0.1383 F1: 53.70% 0.44 min
> Epoch: 22 Step: 1700, train loss: 0.0009 F1: 99.82% dev loss: 0.1336 F1: 51.59% 0.43 min
> Epoch: 23 Step: 1800, train loss: 0.0007 F1: 99.52% dev loss: 0.1427 F1: 52.51% 0.44 min
> Epoch: 24 Step: 1900, train loss: 0.0009 F1: 99.46% dev loss: 0.1472 F1: 53.90% 0.43 min
> Epoch: 25 Step: 2000, train loss: 0.0008 F1: 99.40% dev loss: 0.1492 F1: 53.15% 0.43 min
> Epoch: 27 Step: 2100, train loss: 0.0005 F1: 99.85% dev loss: 0.1437 F1: 52.59% 0.43 min
> Epoch: 28 Step: 2200, train loss: 0.0005 F1: 99.79% dev loss: 0.1460 F1: 51.94% 0.43 min
> Epoch: 29 Step: 2300, train loss: 0.0004 F1: 99.74% dev loss: 0.1464 F1: 53.62% 0.44 min
> Epoch: 31 Step: 2400, train loss: 0.0003 F1: 99.98% dev loss: 0.1449 F1: 51.78% 0.44 min
> Epoch: 32 Step: 2500, train loss: 0.0004 F1: 99.95% dev loss: 0.1466 F1: 52.37% 0.44 min
> Epoch: 33 Step: 2600, train loss: 0.0003 F1: 99.76% dev loss: 0.1498 F1: 51.90% 0.44 min
> Epoch: 35 Step: 2700, train loss: 0.0005 F1: 99.33% dev loss: 0.1483 F1: 51.52% 0.44 min
> Epoch: 36 Step: 2800, train loss: 0.0003 F1: 99.92% dev loss: 0.1473 F1: 52.83% 0.44 min
> Epoch: 37 Step: 2900, train loss: 0.0003 F1: 99.98% dev loss: 0.1447 F1: 54.27% 0.44 min
> Epoch: 38 Step: 3000, train loss: 0.0002 F1: 99.92% dev loss: 0.1452 F1: 53.39% 0.44 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 09:39:48.592960>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_lap14_seed77.pth
>>> seed: 77
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: lap14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.5198 F1: 9.59% dev loss: 0.1862 F1: 9.61% 0.44 min
> Epoch: 2 Step: 200, train loss: 0.1726 F1: 9.61% dev loss: 0.1500 F1: 9.61% 0.44 min
> Epoch: 3 Step: 300, train loss: 0.1296 F1: 22.63% dev loss: 0.1085 F1: 33.32% 0.44 min
> Epoch: 5 Step: 400, train loss: 0.0811 F1: 57.67% dev loss: 0.0697 F1: 53.42% 0.44 min
> Epoch: 6 Step: 500, train loss: 0.0440 F1: 70.26% dev loss: 0.0641 F1: 58.63% 0.44 min
> Epoch: 7 Step: 600, train loss: 0.0305 F1: 75.71% dev loss: 0.0626 F1: 60.43% 0.44 min
> Epoch: 9 Step: 700, train loss: 0.0195 F1: 88.46% dev loss: 0.0595 F1: 60.70% 0.44 min
> Epoch: 10 Step: 800, train loss: 0.0104 F1: 91.16% dev loss: 0.0723 F1: 61.30% 0.44 min
> Epoch: 11 Step: 900, train loss: 0.0059 F1: 95.42% dev loss: 0.0694 F1: 65.93% 0.44 min
> Epoch: 13 Step: 1000, train loss: 0.0034 F1: 97.66% dev loss: 0.0717 F1: 62.59% 0.44 min
> Epoch: 14 Step: 1100, train loss: 0.0027 F1: 97.65% dev loss: 0.0833 F1: 61.88% 0.44 min
> Epoch: 15 Step: 1200, train loss: 0.0022 F1: 98.02% dev loss: 0.0711 F1: 63.95% 0.44 min
> Epoch: 17 Step: 1300, train loss: 0.0017 F1: 98.93% dev loss: 0.0758 F1: 65.26% 0.44 min
> Epoch: 18 Step: 1400, train loss: 0.0015 F1: 98.81% dev loss: 0.0857 F1: 63.64% 0.44 min
> Epoch: 19 Step: 1500, train loss: 0.0013 F1: 98.71% dev loss: 0.0775 F1: 65.05% 0.44 min
> Epoch: 21 Step: 1600, train loss: 0.0008 F1: 99.55% dev loss: 0.0874 F1: 64.94% 0.44 min
> Epoch: 22 Step: 1700, train loss: 0.0007 F1: 99.35% dev loss: 0.0809 F1: 65.32% 0.44 min
> Epoch: 23 Step: 1800, train loss: 0.0008 F1: 99.16% dev loss: 0.0854 F1: 64.20% 0.44 min
> Epoch: 25 Step: 1900, train loss: 0.0006 F1: 99.59% dev loss: 0.0855 F1: 66.06% 0.44 min
> Epoch: 26 Step: 2000, train loss: 0.0005 F1: 99.43% dev loss: 0.0828 F1: 63.27% 0.44 min
> Epoch: 27 Step: 2100, train loss: 0.0005 F1: 99.61% dev loss: 0.0818 F1: 65.87% 0.44 min
> Epoch: 29 Step: 2200, train loss: 0.0004 F1: 99.86% dev loss: 0.0859 F1: 66.23% 0.44 min
> Epoch: 30 Step: 2300, train loss: 0.0003 F1: 99.73% dev loss: 0.0885 F1: 64.07% 0.44 min
> Epoch: 31 Step: 2400, train loss: 0.0003 F1: 99.72% dev loss: 0.0851 F1: 65.20% 0.44 min
> Epoch: 33 Step: 2500, train loss: 0.0003 F1: 99.52% dev loss: 0.0912 F1: 64.29% 0.44 min
> Epoch: 34 Step: 2600, train loss: 0.0003 F1: 99.91% dev loss: 0.0905 F1: 65.18% 0.44 min
> Epoch: 35 Step: 2700, train loss: 0.0002 F1: 99.87% dev loss: 0.0883 F1: 65.15% 0.44 min
> Epoch: 37 Step: 2800, train loss: 0.0002 F1: 99.80% dev loss: 0.0912 F1: 65.44% 0.44 min
> Epoch: 38 Step: 2900, train loss: 0.0002 F1: 99.80% dev loss: 0.0897 F1: 65.84% 0.44 min
> Epoch: 39 Step: 3000, train loss: 0.0002 F1: 99.93% dev loss: 0.0893 F1: 65.50% 0.44 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 09:53:56.147948>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res16_seed77.pth
>>> seed: 77
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res16
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_Train_SB1_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_dev_SB1_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_test_SB1_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 2 Step: 100, train loss: 0.5369 F1: 9.61% dev loss: 0.2391 F1: 9.42% 0.44 min
> Epoch: 4 Step: 200, train loss: 0.1997 F1: 14.73% dev loss: 0.1511 F1: 16.70% 0.43 min
> Epoch: 6 Step: 300, train loss: 0.1208 F1: 35.17% dev loss: 0.0920 F1: 33.45% 0.43 min
> Epoch: 8 Step: 400, train loss: 0.0568 F1: 49.50% dev loss: 0.0775 F1: 43.54% 0.43 min
> Epoch: 10 Step: 500, train loss: 0.0312 F1: 68.79% dev loss: 0.0776 F1: 45.27% 0.43 min
> Epoch: 12 Step: 600, train loss: 0.0245 F1: 80.99% dev loss: 0.1032 F1: 41.85% 0.43 min
> Epoch: 14 Step: 700, train loss: 0.0110 F1: 92.24% dev loss: 0.0848 F1: 52.68% 0.43 min
> Epoch: 16 Step: 800, train loss: 0.0042 F1: 98.00% dev loss: 0.0954 F1: 55.97% 0.43 min
> Epoch: 18 Step: 900, train loss: 0.0017 F1: 98.74% dev loss: 0.0958 F1: 53.34% 0.43 min
> Epoch: 20 Step: 1000, train loss: 0.0012 F1: 98.78% dev loss: 0.1012 F1: 54.62% 0.43 min
> Epoch: 22 Step: 1100, train loss: 0.0008 F1: 99.57% dev loss: 0.0999 F1: 56.46% 0.43 min
> Epoch: 24 Step: 1200, train loss: 0.0006 F1: 98.44% dev loss: 0.1059 F1: 56.13% 0.43 min
> Epoch: 26 Step: 1300, train loss: 0.0007 F1: 99.54% dev loss: 0.1061 F1: 57.81% 0.43 min
> Epoch: 28 Step: 1400, train loss: 0.0008 F1: 99.64% dev loss: 0.1111 F1: 56.38% 0.43 min
> Epoch: 30 Step: 1500, train loss: 0.0007 F1: 99.36% dev loss: 0.1123 F1: 56.86% 0.43 min
> Epoch: 32 Step: 1600, train loss: 0.0005 F1: 99.47% dev loss: 0.1122 F1: 55.90% 0.43 min
> Epoch: 34 Step: 1700, train loss: 0.0003 F1: 99.61% dev loss: 0.1114 F1: 56.84% 0.43 min
> Epoch: 36 Step: 1800, train loss: 0.0003 F1: 99.74% dev loss: 0.1111 F1: 54.15% 0.44 min
> Epoch: 38 Step: 1900, train loss: 0.0003 F1: 99.45% dev loss: 0.1151 F1: 56.86% 0.43 min
> Epoch: 40 Step: 2000, train loss: 0.0003 F1: 99.84% dev loss: 0.1129 F1: 57.66% 0.43 min
> Epoch: 42 Step: 2100, train loss: 0.0003 F1: 99.61% dev loss: 0.1159 F1: 56.47% 0.43 min
> Epoch: 44 Step: 2200, train loss: 0.0002 F1: 99.78% dev loss: 0.1200 F1: 56.30% 0.43 min
> Epoch: 46 Step: 2300, train loss: 0.0002 F1: 99.65% dev loss: 0.1170 F1: 56.42% 0.44 min
> Epoch: 48 Step: 2400, train loss: 0.0002 F1: 99.96% dev loss: 0.1186 F1: 52.54% 0.44 min
> Epoch: 51 Step: 2500, train loss: 0.0002 F1: 70.00% dev loss: 0.1201 F1: 57.01% 0.44 min
> Epoch: 53 Step: 2600, train loss: 0.0001 F1: 80.00% dev loss: 0.1188 F1: 55.99% 0.44 min
> Epoch: 55 Step: 2700, train loss: 0.0001 F1: 80.00% dev loss: 0.1196 F1: 56.98% 0.43 min
> Epoch: 57 Step: 2800, train loss: 0.0001 F1: 80.00% dev loss: 0.1193 F1: 57.01% 0.43 min
> Epoch: 59 Step: 2900, train loss: 0.0001 F1: 100.00% dev loss: 0.1201 F1: 56.55% 0.43 min
> Epoch: 61 Step: 3000, train loss: 0.0001 F1: 99.98% dev loss: 0.1195 F1: 56.75% 0.43 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 10:07:42.183673>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-lstm_res14_seed77.pth
>>> seed: 77
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.7428 F1: 9.34% dev loss: 0.2879 F1: 9.34% 0.47 min
> Epoch: 2 Step: 200, train loss: 0.2772 F1: 9.32% dev loss: 0.2447 F1: 9.34% 0.48 min
> Epoch: 3 Step: 300, train loss: 0.1922 F1: 21.46% dev loss: 0.1617 F1: 24.72% 0.48 min
> Epoch: 5 Step: 400, train loss: 0.1240 F1: 38.34% dev loss: 0.1383 F1: 39.97% 0.48 min
> Epoch: 6 Step: 500, train loss: 0.0811 F1: 62.81% dev loss: 0.1207 F1: 49.10% 0.48 min
> Epoch: 7 Step: 600, train loss: 0.0565 F1: 70.36% dev loss: 0.1105 F1: 47.33% 0.48 min
> Epoch: 9 Step: 700, train loss: 0.0320 F1: 87.55% dev loss: 0.1213 F1: 50.49% 0.48 min
> Epoch: 10 Step: 800, train loss: 0.0251 F1: 86.75% dev loss: 0.1264 F1: 52.20% 0.48 min
> Epoch: 11 Step: 900, train loss: 0.0176 F1: 91.01% dev loss: 0.1109 F1: 54.23% 0.48 min
> Epoch: 12 Step: 1000, train loss: 0.0102 F1: 94.37% dev loss: 0.1079 F1: 49.71% 0.48 min
> Epoch: 14 Step: 1100, train loss: 0.0067 F1: 95.09% dev loss: 0.1134 F1: 57.49% 0.48 min
> Epoch: 15 Step: 1200, train loss: 0.0053 F1: 97.59% dev loss: 0.1280 F1: 55.65% 0.48 min
> Epoch: 16 Step: 1300, train loss: 0.0043 F1: 97.43% dev loss: 0.1198 F1: 54.83% 0.48 min
> Epoch: 18 Step: 1400, train loss: 0.0032 F1: 97.53% dev loss: 0.1190 F1: 53.00% 0.48 min
> Epoch: 19 Step: 1500, train loss: 0.0027 F1: 98.51% dev loss: 0.1280 F1: 55.49% 0.48 min
> Epoch: 20 Step: 1600, train loss: 0.0022 F1: 98.56% dev loss: 0.1282 F1: 55.12% 0.48 min
> Epoch: 22 Step: 1700, train loss: 0.0020 F1: 99.93% dev loss: 0.1296 F1: 54.30% 0.48 min
> Epoch: 23 Step: 1800, train loss: 0.0014 F1: 99.46% dev loss: 0.1255 F1: 53.48% 0.48 min
> Epoch: 24 Step: 1900, train loss: 0.0014 F1: 99.35% dev loss: 0.1339 F1: 53.04% 0.48 min
> Epoch: 25 Step: 2000, train loss: 0.0012 F1: 99.13% dev loss: 0.1368 F1: 53.01% 0.48 min
> Epoch: 27 Step: 2100, train loss: 0.0010 F1: 99.52% dev loss: 0.1343 F1: 53.21% 0.48 min
> Epoch: 28 Step: 2200, train loss: 0.0012 F1: 99.38% dev loss: 0.1346 F1: 53.93% 0.48 min
> Epoch: 29 Step: 2300, train loss: 0.0008 F1: 99.49% dev loss: 0.1354 F1: 54.42% 0.48 min
> Epoch: 31 Step: 2400, train loss: 0.0006 F1: 99.88% dev loss: 0.1376 F1: 53.59% 0.48 min
> Epoch: 32 Step: 2500, train loss: 0.0006 F1: 99.84% dev loss: 0.1382 F1: 52.72% 0.48 min
> Epoch: 33 Step: 2600, train loss: 0.0006 F1: 99.64% dev loss: 0.1375 F1: 51.86% 0.48 min
> Epoch: 35 Step: 2700, train loss: 0.0004 F1: 100.00% dev loss: 0.1395 F1: 53.95% 0.48 min
> Epoch: 36 Step: 2800, train loss: 0.0005 F1: 100.00% dev loss: 0.1371 F1: 53.66% 0.48 min
> Epoch: 37 Step: 2900, train loss: 0.0005 F1: 99.66% dev loss: 0.1366 F1: 54.11% 0.48 min
> Epoch: 38 Step: 3000, train loss: 0.0004 F1: 99.78% dev loss: 0.1362 F1: 55.10% 0.48 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 10:22:42.001674>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-lstm_lap14_seed77.pth
>>> seed: 77
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: lap14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.6847 F1: 9.59% dev loss: 0.2004 F1: 9.61% 0.48 min
> Epoch: 2 Step: 200, train loss: 0.1889 F1: 9.61% dev loss: 0.1800 F1: 9.61% 0.48 min
> Epoch: 3 Step: 300, train loss: 0.1651 F1: 9.62% dev loss: 0.1347 F1: 13.40% 0.48 min
> Epoch: 5 Step: 400, train loss: 0.1142 F1: 31.95% dev loss: 0.0963 F1: 28.75% 0.48 min
> Epoch: 6 Step: 500, train loss: 0.0733 F1: 54.03% dev loss: 0.0919 F1: 37.95% 0.48 min
> Epoch: 7 Step: 600, train loss: 0.0485 F1: 70.13% dev loss: 0.0767 F1: 56.43% 0.48 min
> Epoch: 9 Step: 700, train loss: 0.0316 F1: 83.41% dev loss: 0.0705 F1: 59.41% 0.48 min
> Epoch: 10 Step: 800, train loss: 0.0221 F1: 86.14% dev loss: 0.0769 F1: 61.79% 0.48 min
> Epoch: 11 Step: 900, train loss: 0.0117 F1: 91.29% dev loss: 0.0676 F1: 64.83% 0.48 min
> Epoch: 13 Step: 1000, train loss: 0.0079 F1: 96.38% dev loss: 0.0752 F1: 63.87% 0.48 min
> Epoch: 14 Step: 1100, train loss: 0.0051 F1: 97.30% dev loss: 0.0753 F1: 66.27% 0.48 min
> Epoch: 15 Step: 1200, train loss: 0.0036 F1: 97.86% dev loss: 0.0772 F1: 66.70% 0.48 min
> Epoch: 17 Step: 1300, train loss: 0.0028 F1: 97.43% dev loss: 0.0767 F1: 64.54% 0.48 min
> Epoch: 18 Step: 1400, train loss: 0.0024 F1: 98.37% dev loss: 0.0773 F1: 66.82% 0.48 min
> Epoch: 19 Step: 1500, train loss: 0.0019 F1: 98.76% dev loss: 0.0793 F1: 65.91% 0.48 min
> Epoch: 21 Step: 1600, train loss: 0.0016 F1: 98.81% dev loss: 0.0777 F1: 64.89% 0.48 min
> Epoch: 22 Step: 1700, train loss: 0.0015 F1: 99.10% dev loss: 0.0750 F1: 65.20% 0.48 min
> Epoch: 23 Step: 1800, train loss: 0.0013 F1: 99.28% dev loss: 0.0803 F1: 64.96% 0.48 min
> Epoch: 25 Step: 1900, train loss: 0.0009 F1: 99.61% dev loss: 0.0783 F1: 65.54% 0.48 min
> Epoch: 26 Step: 2000, train loss: 0.0009 F1: 99.58% dev loss: 0.0782 F1: 65.37% 0.48 min
> Epoch: 27 Step: 2100, train loss: 0.0006 F1: 99.68% dev loss: 0.0802 F1: 64.96% 0.48 min
> Epoch: 29 Step: 2200, train loss: 0.0006 F1: 99.59% dev loss: 0.0810 F1: 65.06% 0.48 min
> Epoch: 30 Step: 2300, train loss: 0.0005 F1: 99.86% dev loss: 0.0835 F1: 63.20% 0.48 min
> Epoch: 31 Step: 2400, train loss: 0.0005 F1: 99.72% dev loss: 0.0832 F1: 63.82% 0.48 min
> Epoch: 33 Step: 2500, train loss: 0.0004 F1: 99.71% dev loss: 0.0836 F1: 66.49% 0.48 min
> Epoch: 34 Step: 2600, train loss: 0.0004 F1: 99.78% dev loss: 0.0842 F1: 64.75% 0.48 min
> Epoch: 35 Step: 2700, train loss: 0.0004 F1: 99.75% dev loss: 0.0837 F1: 64.64% 0.48 min
> Epoch: 37 Step: 2800, train loss: 0.0003 F1: 99.86% dev loss: 0.0842 F1: 64.93% 0.48 min
> Epoch: 38 Step: 2900, train loss: 0.0003 F1: 99.79% dev loss: 0.0846 F1: 64.47% 0.48 min
> Epoch: 39 Step: 3000, train loss: 0.0003 F1: 99.86% dev loss: 0.0850 F1: 64.16% 0.48 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 10:38:05.330769>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-lstm_res16_seed77.pth
>>> seed: 77
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res16
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_Train_SB1_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_dev_SB1_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_test_SB1_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 2 Step: 100, train loss: 0.7185 F1: 9.61% dev loss: 0.2626 F1: 9.42% 0.47 min
> Epoch: 4 Step: 200, train loss: 0.2322 F1: 9.57% dev loss: 0.2203 F1: 9.42% 0.47 min
> Epoch: 6 Step: 300, train loss: 0.1918 F1: 18.35% dev loss: 0.1424 F1: 17.03% 0.47 min
> Epoch: 8 Step: 400, train loss: 0.1107 F1: 40.91% dev loss: 0.1001 F1: 33.48% 0.47 min
> Epoch: 10 Step: 500, train loss: 0.0609 F1: 53.53% dev loss: 0.0842 F1: 48.46% 0.48 min
> Epoch: 12 Step: 600, train loss: 0.0351 F1: 61.66% dev loss: 0.1025 F1: 47.87% 0.47 min
> Epoch: 14 Step: 700, train loss: 0.0232 F1: 62.66% dev loss: 0.1002 F1: 50.63% 0.48 min
> Epoch: 16 Step: 800, train loss: 0.0127 F1: 78.40% dev loss: 0.1014 F1: 52.71% 0.48 min
> Epoch: 18 Step: 900, train loss: 0.0061 F1: 95.12% dev loss: 0.1047 F1: 56.00% 0.48 min
> Epoch: 20 Step: 1000, train loss: 0.0039 F1: 96.61% dev loss: 0.1015 F1: 52.54% 0.48 min
> Epoch: 22 Step: 1100, train loss: 0.0025 F1: 97.70% dev loss: 0.1070 F1: 57.08% 0.48 min
> Epoch: 24 Step: 1200, train loss: 0.0019 F1: 98.09% dev loss: 0.1029 F1: 51.94% 0.48 min
> Epoch: 26 Step: 1300, train loss: 0.0013 F1: 99.63% dev loss: 0.1067 F1: 56.52% 0.48 min
> Epoch: 28 Step: 1400, train loss: 0.0011 F1: 99.46% dev loss: 0.1062 F1: 51.96% 0.48 min
> Epoch: 30 Step: 1500, train loss: 0.0010 F1: 98.23% dev loss: 0.1085 F1: 55.07% 0.48 min
> Epoch: 32 Step: 1600, train loss: 0.0009 F1: 99.56% dev loss: 0.1141 F1: 56.87% 0.48 min
> Epoch: 34 Step: 1700, train loss: 0.0006 F1: 99.15% dev loss: 0.1112 F1: 57.12% 0.48 min
> Epoch: 36 Step: 1800, train loss: 0.0005 F1: 99.47% dev loss: 0.1118 F1: 57.27% 0.48 min
> Epoch: 38 Step: 1900, train loss: 0.0005 F1: 99.62% dev loss: 0.1115 F1: 52.90% 0.48 min
> Epoch: 40 Step: 2000, train loss: 0.0004 F1: 99.60% dev loss: 0.1162 F1: 57.15% 0.48 min
> Epoch: 42 Step: 2100, train loss: 0.0004 F1: 99.58% dev loss: 0.1158 F1: 56.99% 0.48 min
> Epoch: 44 Step: 2200, train loss: 0.0004 F1: 99.70% dev loss: 0.1163 F1: 56.79% 0.48 min
> Epoch: 46 Step: 2300, train loss: 0.0003 F1: 99.67% dev loss: 0.1178 F1: 56.65% 0.48 min
> Epoch: 48 Step: 2400, train loss: 0.0004 F1: 99.87% dev loss: 0.1185 F1: 56.68% 0.48 min
> Epoch: 51 Step: 2500, train loss: 0.0003 F1: 70.00% dev loss: 0.1222 F1: 56.71% 0.48 min
> Epoch: 53 Step: 2600, train loss: 0.0005 F1: 80.00% dev loss: 0.1156 F1: 54.43% 0.48 min
> Epoch: 55 Step: 2700, train loss: 0.0003 F1: 80.00% dev loss: 0.1176 F1: 54.44% 0.48 min
> Epoch: 57 Step: 2800, train loss: 0.0002 F1: 79.04% dev loss: 0.1190 F1: 54.05% 0.48 min
> Epoch: 59 Step: 2900, train loss: 0.0002 F1: 99.40% dev loss: 0.1193 F1: 53.91% 0.48 min
> Epoch: 61 Step: 3000, train loss: 0.0003 F1: 99.32% dev loss: 0.1188 F1: 54.23% 0.48 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 10:53:07.146747>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: san
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-san_res14_seed77.pth
>>> seed: 77
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.4979 F1: 13.59% dev loss: 0.2327 F1: 10.40% 0.46 min
> Epoch: 2 Step: 200, train loss: 0.2060 F1: 22.12% dev loss: 0.1613 F1: 25.38% 0.46 min
> Epoch: 3 Step: 300, train loss: 0.1239 F1: 40.82% dev loss: 0.1124 F1: 35.73% 0.46 min
> Epoch: 5 Step: 400, train loss: 0.0729 F1: 66.61% dev loss: 0.0939 F1: 47.48% 0.46 min
> Epoch: 6 Step: 500, train loss: 0.0487 F1: 74.56% dev loss: 0.0971 F1: 50.78% 0.46 min
> Epoch: 7 Step: 600, train loss: 0.0345 F1: 80.24% dev loss: 0.1038 F1: 52.87% 0.46 min
> Epoch: 9 Step: 700, train loss: 0.0190 F1: 91.01% dev loss: 0.1047 F1: 51.14% 0.46 min
> Epoch: 10 Step: 800, train loss: 0.0104 F1: 93.11% dev loss: 0.1081 F1: 54.29% 0.46 min
> Epoch: 11 Step: 900, train loss: 0.0067 F1: 96.13% dev loss: 0.1232 F1: 51.21% 0.46 min
> Epoch: 12 Step: 1000, train loss: 0.0050 F1: 96.14% dev loss: 0.1200 F1: 54.59% 0.46 min
> Epoch: 14 Step: 1100, train loss: 0.0041 F1: 97.29% dev loss: 0.1224 F1: 57.08% 0.46 min
> Epoch: 15 Step: 1200, train loss: 0.0038 F1: 97.58% dev loss: 0.1256 F1: 52.26% 0.46 min
> Epoch: 16 Step: 1300, train loss: 0.0023 F1: 98.69% dev loss: 0.1250 F1: 55.28% 0.46 min
> Epoch: 18 Step: 1400, train loss: 0.0016 F1: 98.90% dev loss: 0.1315 F1: 51.72% 0.46 min
> Epoch: 19 Step: 1500, train loss: 0.0017 F1: 99.11% dev loss: 0.1271 F1: 54.49% 0.46 min
> Epoch: 20 Step: 1600, train loss: 0.0012 F1: 99.43% dev loss: 0.1253 F1: 54.39% 0.46 min
> Epoch: 22 Step: 1700, train loss: 0.0010 F1: 99.31% dev loss: 0.1297 F1: 56.57% 0.46 min
> Epoch: 23 Step: 1800, train loss: 0.0009 F1: 99.50% dev loss: 0.1284 F1: 53.88% 0.46 min
> Epoch: 24 Step: 1900, train loss: 0.0007 F1: 99.53% dev loss: 0.1317 F1: 55.34% 0.46 min
> Epoch: 25 Step: 2000, train loss: 0.0006 F1: 99.56% dev loss: 0.1351 F1: 55.47% 0.46 min
> Epoch: 27 Step: 2100, train loss: 0.0004 F1: 99.83% dev loss: 0.1386 F1: 55.33% 0.46 min
> Epoch: 28 Step: 2200, train loss: 0.0006 F1: 99.42% dev loss: 0.1418 F1: 55.95% 0.46 min
> Epoch: 29 Step: 2300, train loss: 0.0006 F1: 99.77% dev loss: 0.1392 F1: 56.92% 0.46 min
> Epoch: 31 Step: 2400, train loss: 0.0005 F1: 99.77% dev loss: 0.1388 F1: 57.36% 0.46 min
> Epoch: 32 Step: 2500, train loss: 0.0005 F1: 99.82% dev loss: 0.1376 F1: 56.44% 0.46 min
> Epoch: 33 Step: 2600, train loss: 0.0003 F1: 99.82% dev loss: 0.1405 F1: 55.28% 0.46 min
> Epoch: 35 Step: 2700, train loss: 0.0003 F1: 99.76% dev loss: 0.1374 F1: 56.24% 0.46 min
> Epoch: 36 Step: 2800, train loss: 0.0003 F1: 99.89% dev loss: 0.1406 F1: 57.27% 0.46 min
> Epoch: 37 Step: 2900, train loss: 0.0003 F1: 99.75% dev loss: 0.1401 F1: 56.52% 0.46 min
> Epoch: 38 Step: 3000, train loss: 0.0002 F1: 99.91% dev loss: 0.1401 F1: 56.54% 0.46 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 11:07:33.038156>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: san
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-san_lap14_seed77.pth
>>> seed: 77
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: lap14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.4280 F1: 9.59% dev loss: 0.1809 F1: 9.61% 0.47 min
> Epoch: 2 Step: 200, train loss: 0.1598 F1: 12.43% dev loss: 0.1288 F1: 16.21% 0.46 min
> Epoch: 3 Step: 300, train loss: 0.1114 F1: 30.09% dev loss: 0.0883 F1: 40.91% 0.46 min
> Epoch: 5 Step: 400, train loss: 0.0696 F1: 61.31% dev loss: 0.0673 F1: 50.58% 0.46 min
> Epoch: 6 Step: 500, train loss: 0.0365 F1: 75.58% dev loss: 0.0669 F1: 57.36% 0.46 min
> Epoch: 7 Step: 600, train loss: 0.0227 F1: 82.78% dev loss: 0.0685 F1: 59.23% 0.46 min
> Epoch: 9 Step: 700, train loss: 0.0142 F1: 91.77% dev loss: 0.0664 F1: 62.51% 0.46 min
> Epoch: 10 Step: 800, train loss: 0.0091 F1: 90.80% dev loss: 0.0725 F1: 63.09% 0.46 min
> Epoch: 11 Step: 900, train loss: 0.0050 F1: 95.90% dev loss: 0.0789 F1: 62.00% 0.46 min
> Epoch: 13 Step: 1000, train loss: 0.0037 F1: 96.40% dev loss: 0.0772 F1: 62.34% 0.46 min
> Epoch: 14 Step: 1100, train loss: 0.0024 F1: 98.22% dev loss: 0.0765 F1: 64.91% 0.46 min
> Epoch: 15 Step: 1200, train loss: 0.0018 F1: 98.19% dev loss: 0.0759 F1: 65.03% 0.46 min
> Epoch: 17 Step: 1300, train loss: 0.0018 F1: 98.84% dev loss: 0.0858 F1: 65.59% 0.46 min
> Epoch: 18 Step: 1400, train loss: 0.0015 F1: 98.59% dev loss: 0.0843 F1: 65.27% 0.46 min
> Epoch: 19 Step: 1500, train loss: 0.0011 F1: 98.92% dev loss: 0.0888 F1: 64.81% 0.46 min
> Epoch: 21 Step: 1600, train loss: 0.0008 F1: 99.35% dev loss: 0.0932 F1: 65.41% 0.46 min
> Epoch: 22 Step: 1700, train loss: 0.0008 F1: 99.27% dev loss: 0.0913 F1: 64.16% 0.46 min
> Epoch: 23 Step: 1800, train loss: 0.0007 F1: 99.54% dev loss: 0.0925 F1: 63.63% 0.46 min
> Epoch: 25 Step: 1900, train loss: 0.0005 F1: 99.96% dev loss: 0.0896 F1: 63.61% 0.46 min
> Epoch: 26 Step: 2000, train loss: 0.0004 F1: 99.71% dev loss: 0.0926 F1: 62.73% 0.46 min
> Epoch: 27 Step: 2100, train loss: 0.0004 F1: 99.69% dev loss: 0.0907 F1: 65.19% 0.46 min
> Epoch: 29 Step: 2200, train loss: 0.0003 F1: 99.65% dev loss: 0.0942 F1: 64.32% 0.46 min
> Epoch: 30 Step: 2300, train loss: 0.0003 F1: 99.77% dev loss: 0.0945 F1: 65.45% 0.46 min
> Epoch: 31 Step: 2400, train loss: 0.0002 F1: 99.73% dev loss: 0.0908 F1: 63.80% 0.46 min
> Epoch: 33 Step: 2500, train loss: 0.0002 F1: 99.90% dev loss: 0.0924 F1: 64.72% 0.46 min
> Epoch: 34 Step: 2600, train loss: 0.0002 F1: 99.88% dev loss: 0.0948 F1: 64.23% 0.46 min
> Epoch: 35 Step: 2700, train loss: 0.0002 F1: 99.79% dev loss: 0.0925 F1: 64.64% 0.46 min
> Epoch: 37 Step: 2800, train loss: 0.0002 F1: 99.91% dev loss: 0.0921 F1: 64.54% 0.46 min
> Epoch: 38 Step: 2900, train loss: 0.0002 F1: 99.87% dev loss: 0.0925 F1: 65.00% 0.46 min
> Epoch: 39 Step: 3000, train loss: 0.0002 F1: 99.94% dev loss: 0.0926 F1: 64.77% 0.46 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 11:22:25.042425>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: san
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-san_res16_seed77.pth
>>> seed: 77
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res16
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_Train_SB1_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_dev_SB1_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_test_SB1_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 2 Step: 100, train loss: 0.4677 F1: 9.61% dev loss: 0.2437 F1: 9.42% 0.46 min
> Epoch: 4 Step: 200, train loss: 0.1840 F1: 19.94% dev loss: 0.1374 F1: 22.61% 0.45 min
> Epoch: 6 Step: 300, train loss: 0.1139 F1: 35.67% dev loss: 0.0910 F1: 35.07% 0.46 min
> Epoch: 8 Step: 400, train loss: 0.0603 F1: 51.16% dev loss: 0.0797 F1: 46.76% 0.46 min
> Epoch: 10 Step: 500, train loss: 0.0357 F1: 67.62% dev loss: 0.1065 F1: 41.26% 0.46 min
> Epoch: 12 Step: 600, train loss: 0.0193 F1: 89.26% dev loss: 0.0934 F1: 46.63% 0.46 min
> Epoch: 14 Step: 700, train loss: 0.0113 F1: 93.04% dev loss: 0.0916 F1: 50.82% 0.46 min
> Epoch: 16 Step: 800, train loss: 0.0108 F1: 92.04% dev loss: 0.1025 F1: 52.33% 0.46 min
> Epoch: 18 Step: 900, train loss: 0.0032 F1: 97.90% dev loss: 0.1035 F1: 56.71% 0.46 min
> Epoch: 20 Step: 1000, train loss: 0.0015 F1: 98.65% dev loss: 0.1096 F1: 58.77% 0.46 min
> Epoch: 22 Step: 1100, train loss: 0.0012 F1: 99.52% dev loss: 0.1134 F1: 56.04% 0.46 min
> Epoch: 24 Step: 1200, train loss: 0.0009 F1: 99.29% dev loss: 0.1150 F1: 57.14% 0.46 min
> Epoch: 26 Step: 1300, train loss: 0.0008 F1: 99.44% dev loss: 0.1185 F1: 56.01% 0.46 min
> Epoch: 28 Step: 1400, train loss: 0.0008 F1: 99.67% dev loss: 0.1151 F1: 51.84% 0.46 min
> Epoch: 30 Step: 1500, train loss: 0.0007 F1: 99.73% dev loss: 0.1179 F1: 57.08% 0.46 min
> Epoch: 32 Step: 1600, train loss: 0.0005 F1: 99.26% dev loss: 0.1286 F1: 53.30% 0.46 min
> Epoch: 34 Step: 1700, train loss: 0.0006 F1: 99.54% dev loss: 0.1277 F1: 57.49% 0.46 min
> Epoch: 36 Step: 1800, train loss: 0.0005 F1: 99.89% dev loss: 0.1314 F1: 52.91% 0.46 min
> Epoch: 38 Step: 1900, train loss: 0.0003 F1: 99.77% dev loss: 0.1251 F1: 53.06% 0.46 min
> Epoch: 40 Step: 2000, train loss: 0.0004 F1: 99.75% dev loss: 0.1267 F1: 53.11% 0.46 min
> Epoch: 42 Step: 2100, train loss: 0.0003 F1: 99.89% dev loss: 0.1238 F1: 52.86% 0.46 min
> Epoch: 44 Step: 2200, train loss: 0.0004 F1: 99.92% dev loss: 0.1321 F1: 53.01% 0.46 min
> Epoch: 46 Step: 2300, train loss: 0.0002 F1: 99.94% dev loss: 0.1322 F1: 52.82% 0.46 min
> Epoch: 48 Step: 2400, train loss: 0.0002 F1: 99.89% dev loss: 0.1352 F1: 53.38% 0.46 min
> Epoch: 51 Step: 2500, train loss: 0.0003 F1: 70.00% dev loss: 0.1367 F1: 52.89% 0.46 min
> Epoch: 53 Step: 2600, train loss: 0.0002 F1: 80.00% dev loss: 0.1368 F1: 56.39% 0.46 min
> Epoch: 55 Step: 2700, train loss: 0.0002 F1: 80.00% dev loss: 0.1368 F1: 53.20% 0.46 min
> Epoch: 57 Step: 2800, train loss: 0.0001 F1: 79.96% dev loss: 0.1368 F1: 53.04% 0.46 min
> Epoch: 59 Step: 2900, train loss: 0.0001 F1: 100.00% dev loss: 0.1362 F1: 53.17% 0.46 min
> Epoch: 61 Step: 3000, train loss: 0.0001 F1: 99.96% dev loss: 0.1366 F1: 53.07% 0.46 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 11:36:52.926094>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: crf
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-crf_res14_seed77.pth
>>> seed: 77
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 15.7326 F1: 9.34% dev loss: 8.5851 F1: 9.34% 0.59 min
> Epoch: 2 Step: 200, train loss: 7.2872 F1: 23.19% dev loss: 5.3912 F1: 21.32% 0.58 min
> Epoch: 3 Step: 300, train loss: 4.4040 F1: 40.41% dev loss: 3.7020 F1: 32.39% 0.58 min
> Epoch: 5 Step: 400, train loss: 2.7308 F1: 61.97% dev loss: 3.5117 F1: 49.77% 0.58 min
> Epoch: 6 Step: 500, train loss: 1.8750 F1: 75.21% dev loss: 4.0812 F1: 46.37% 0.59 min
> Epoch: 7 Step: 600, train loss: 1.3948 F1: 81.72% dev loss: 3.6118 F1: 51.19% 0.58 min
> Epoch: 9 Step: 700, train loss: 1.0997 F1: 84.75% dev loss: 3.9884 F1: 56.83% 0.58 min
> Epoch: 10 Step: 800, train loss: 0.6314 F1: 94.97% dev loss: 3.9610 F1: 57.76% 0.58 min
> Epoch: 11 Step: 900, train loss: 0.4016 F1: 96.10% dev loss: 4.2646 F1: 53.72% 0.58 min
> Epoch: 12 Step: 1000, train loss: 0.2869 F1: 97.38% dev loss: 4.8110 F1: 53.53% 0.58 min
> Epoch: 14 Step: 1100, train loss: 0.2095 F1: 98.45% dev loss: 4.8213 F1: 54.86% 0.58 min
> Epoch: 15 Step: 1200, train loss: 0.2123 F1: 98.49% dev loss: 4.8996 F1: 55.60% 0.58 min
> Epoch: 16 Step: 1300, train loss: 0.1420 F1: 98.53% dev loss: 5.5569 F1: 52.86% 0.58 min
> Epoch: 18 Step: 1400, train loss: 0.1039 F1: 99.06% dev loss: 5.4547 F1: 53.40% 0.59 min
> Epoch: 19 Step: 1500, train loss: 0.0832 F1: 99.45% dev loss: 5.2966 F1: 55.73% 0.58 min
> Epoch: 20 Step: 1600, train loss: 0.0574 F1: 99.67% dev loss: 5.4813 F1: 57.50% 0.58 min
> Epoch: 22 Step: 1700, train loss: 0.0457 F1: 99.86% dev loss: 5.4085 F1: 56.91% 0.58 min
> Epoch: 23 Step: 1800, train loss: 0.0430 F1: 99.85% dev loss: 5.4405 F1: 56.08% 0.58 min
> Epoch: 24 Step: 1900, train loss: 0.0426 F1: 99.57% dev loss: 5.5673 F1: 55.93% 0.58 min
> Epoch: 25 Step: 2000, train loss: 0.0301 F1: 99.69% dev loss: 5.6785 F1: 55.00% 0.58 min
> Epoch: 27 Step: 2100, train loss: 0.0298 F1: 99.86% dev loss: 5.8144 F1: 57.73% 0.58 min
> Epoch: 28 Step: 2200, train loss: 0.0284 F1: 99.79% dev loss: 5.5665 F1: 56.35% 0.58 min
> Epoch: 29 Step: 2300, train loss: 0.0441 F1: 99.67% dev loss: 5.7717 F1: 56.06% 0.58 min
> Epoch: 31 Step: 2400, train loss: 0.0186 F1: 99.94% dev loss: 5.7927 F1: 56.07% 0.58 min
> Epoch: 32 Step: 2500, train loss: 0.0147 F1: 99.99% dev loss: 5.7075 F1: 57.09% 0.58 min
> Epoch: 33 Step: 2600, train loss: 0.0216 F1: 99.88% dev loss: 5.6365 F1: 57.39% 0.58 min
> Epoch: 35 Step: 2700, train loss: 0.0198 F1: 99.88% dev loss: 5.7167 F1: 56.24% 0.58 min
> Epoch: 36 Step: 2800, train loss: 0.0246 F1: 99.90% dev loss: 5.6749 F1: 57.37% 0.58 min
> Epoch: 37 Step: 2900, train loss: 0.0155 F1: 100.00% dev loss: 5.6843 F1: 56.92% 0.58 min
> Epoch: 38 Step: 3000, train loss: 0.0130 F1: 99.96% dev loss: 5.6786 F1: 56.55% 0.58 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 11:54:54.909144>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: crf
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-crf_lap14_seed77.pth
>>> seed: 77
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: lap14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 14.3004 F1: 9.59% dev loss: 6.9909 F1: 9.61% 0.59 min
> Epoch: 2 Step: 200, train loss: 6.1584 F1: 9.61% dev loss: 4.8552 F1: 11.10% 0.59 min
> Epoch: 3 Step: 300, train loss: 4.1368 F1: 27.07% dev loss: 3.8557 F1: 36.73% 0.59 min
> Epoch: 5 Step: 400, train loss: 2.6669 F1: 58.12% dev loss: 2.8152 F1: 56.69% 0.59 min
> Epoch: 6 Step: 500, train loss: 1.8230 F1: 66.98% dev loss: 2.5075 F1: 60.59% 0.59 min
> Epoch: 7 Step: 600, train loss: 1.1435 F1: 80.87% dev loss: 2.7912 F1: 61.25% 0.58 min
> Epoch: 9 Step: 700, train loss: 0.6981 F1: 91.67% dev loss: 2.8587 F1: 61.13% 0.58 min
> Epoch: 10 Step: 800, train loss: 0.4561 F1: 93.48% dev loss: 3.4517 F1: 60.99% 0.59 min
> Epoch: 11 Step: 900, train loss: 0.3163 F1: 95.08% dev loss: 3.6713 F1: 62.48% 0.59 min
> Epoch: 13 Step: 1000, train loss: 0.1993 F1: 98.08% dev loss: 3.2826 F1: 56.94% 0.58 min
> Epoch: 14 Step: 1100, train loss: 0.1231 F1: 98.85% dev loss: 3.4182 F1: 63.98% 0.59 min
> Epoch: 15 Step: 1200, train loss: 0.1000 F1: 98.94% dev loss: 3.5661 F1: 65.44% 0.59 min
> Epoch: 17 Step: 1300, train loss: 0.0936 F1: 99.51% dev loss: 4.1837 F1: 59.89% 0.59 min
> Epoch: 18 Step: 1400, train loss: 0.0612 F1: 99.45% dev loss: 3.7588 F1: 66.45% 0.59 min
> Epoch: 19 Step: 1500, train loss: 0.0581 F1: 99.35% dev loss: 3.7241 F1: 63.23% 0.59 min
> Epoch: 21 Step: 1600, train loss: 0.0655 F1: 99.78% dev loss: 3.8009 F1: 63.74% 0.59 min
> Epoch: 22 Step: 1700, train loss: 0.0424 F1: 99.60% dev loss: 3.6773 F1: 64.52% 0.59 min
> Epoch: 23 Step: 1800, train loss: 0.0336 F1: 99.48% dev loss: 3.8494 F1: 66.16% 0.59 min
> Epoch: 25 Step: 1900, train loss: 0.0344 F1: 99.81% dev loss: 3.9091 F1: 64.50% 0.59 min
> Epoch: 26 Step: 2000, train loss: 0.0287 F1: 99.56% dev loss: 4.0974 F1: 63.92% 0.59 min
> Epoch: 27 Step: 2100, train loss: 0.0293 F1: 99.67% dev loss: 3.9747 F1: 64.72% 0.59 min
> Epoch: 29 Step: 2200, train loss: 0.0176 F1: 100.00% dev loss: 4.2154 F1: 63.37% 0.59 min
> Epoch: 30 Step: 2300, train loss: 0.0194 F1: 99.85% dev loss: 4.1813 F1: 63.88% 0.58 min
> Epoch: 31 Step: 2400, train loss: 0.0144 F1: 99.84% dev loss: 4.0740 F1: 65.28% 0.59 min
> Epoch: 33 Step: 2500, train loss: 0.0125 F1: 100.00% dev loss: 4.1649 F1: 65.12% 0.59 min
> Epoch: 34 Step: 2600, train loss: 0.0171 F1: 99.85% dev loss: 4.1965 F1: 64.91% 0.59 min
> Epoch: 35 Step: 2700, train loss: 0.0117 F1: 99.91% dev loss: 4.1667 F1: 64.64% 0.59 min
> Epoch: 37 Step: 2800, train loss: 0.0190 F1: 100.00% dev loss: 4.2024 F1: 63.95% 0.59 min
> Epoch: 38 Step: 2900, train loss: 0.0123 F1: 99.86% dev loss: 4.2378 F1: 64.08% 0.59 min
> Epoch: 39 Step: 3000, train loss: 0.0132 F1: 99.88% dev loss: 4.2409 F1: 64.31% 0.58 min

>>>>>>>>>>>>>>>>>>>>>2021-07-21 12:13:33.464943>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: crf
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-crf_res16_seed77.pth
>>> seed: 77
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res16
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_Train_SB1_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_dev_SB1_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_test_SB1_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 2 Step: 100, train loss: 13.7187 F1: 9.61% dev loss: 7.8523 F1: 9.42% 0.58 min
> Epoch: 4 Step: 200, train loss: 6.6315 F1: 16.76% dev loss: 4.7378 F1: 17.08% 0.58 min
> Epoch: 6 Step: 300, train loss: 3.7694 F1: 35.72% dev loss: 3.2177 F1: 33.73% 0.58 min
> Epoch: 8 Step: 400, train loss: 1.9617 F1: 51.57% dev loss: 2.9874 F1: 41.38% 0.59 min
> Epoch: 10 Step: 500, train loss: 1.1353 F1: 58.32% dev loss: 3.8983 F1: 41.22% 0.58 min
> Epoch: 12 Step: 600, train loss: 0.8135 F1: 63.94% dev loss: 3.7068 F1: 39.15% 0.58 min
> Epoch: 14 Step: 700, train loss: 0.4580 F1: 89.23% dev loss: 3.3239 F1: 55.66% 0.58 min
> Epoch: 16 Step: 800, train loss: 0.2373 F1: 96.03% dev loss: 3.8010 F1: 53.88% 0.58 min
> Epoch: 18 Step: 900, train loss: 0.1161 F1: 98.93% dev loss: 4.1090 F1: 52.02% 0.59 min
> Epoch: 20 Step: 1000, train loss: 0.0808 F1: 99.10% dev loss: 4.0168 F1: 53.97% 0.59 min
> Epoch: 22 Step: 1100, train loss: 0.0583 F1: 99.42% dev loss: 4.0075 F1: 58.65% 0.59 min
> Epoch: 24 Step: 1200, train loss: 0.0570 F1: 99.09% dev loss: 3.8870 F1: 53.32% 0.58 min
> Epoch: 26 Step: 1300, train loss: 0.0452 F1: 98.68% dev loss: 4.2249 F1: 58.88% 0.58 min
> Epoch: 28 Step: 1400, train loss: 0.0320 F1: 99.74% dev loss: 4.0284 F1: 61.72% 0.58 min
> Epoch: 30 Step: 1500, train loss: 0.0508 F1: 99.69% dev loss: 4.4098 F1: 54.19% 0.58 min
> Epoch: 32 Step: 1600, train loss: 0.0369 F1: 99.89% dev loss: 4.1939 F1: 59.00% 0.58 min
> Epoch: 34 Step: 1700, train loss: 0.0230 F1: 99.82% dev loss: 4.3640 F1: 61.70% 0.58 min
> Epoch: 36 Step: 1800, train loss: 0.0196 F1: 99.64% dev loss: 4.6612 F1: 54.88% 0.59 min
> Epoch: 38 Step: 1900, train loss: 0.0187 F1: 99.52% dev loss: 4.7357 F1: 63.01% 0.58 min
> Epoch: 40 Step: 2000, train loss: 0.0237 F1: 99.99% dev loss: 4.3408 F1: 62.37% 0.59 min
> Epoch: 42 Step: 2100, train loss: 0.0234 F1: 99.96% dev loss: 4.7869 F1: 61.68% 0.60 min
> Epoch: 44 Step: 2200, train loss: 0.0156 F1: 99.83% dev loss: 4.6569 F1: 59.79% 0.59 min
> Epoch: 46 Step: 2300, train loss: 0.0245 F1: 99.93% dev loss: 4.1953 F1: 62.37% 0.59 min
> Epoch: 48 Step: 2400, train loss: 0.0143 F1: 99.97% dev loss: 4.2410 F1: 62.86% 0.59 min
> Epoch: 51 Step: 2500, train loss: 0.0135 F1: 70.00% dev loss: 4.2680 F1: 59.18% 0.58 min
> Epoch: 53 Step: 2600, train loss: 0.0141 F1: 80.00% dev loss: 4.1628 F1: 60.23% 0.58 min
> Epoch: 55 Step: 2700, train loss: 0.0100 F1: 80.00% dev loss: 4.2936 F1: 62.75% 0.58 min
> Epoch: 57 Step: 2800, train loss: 0.0127 F1: 80.00% dev loss: 4.2673 F1: 60.50% 0.58 min
> Epoch: 59 Step: 2900, train loss: 0.0076 F1: 99.97% dev loss: 4.3011 F1: 62.62% 0.58 min
> Epoch: 61 Step: 3000, train loss: 0.0080 F1: 99.98% dev loss: 4.2996 F1: 62.65% 0.58 min

----with clip_large_grad----

>>>>>>>>>>>>>>>>>>>>>2021-07-22 02:19:52.985448>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res14_seed77.pth
>>> seed: 77
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.5528 F1: 13.09% dev loss: 0.2144 F1: 13.98% 0.45 min
> Epoch: 2 Step: 200, train loss: 0.1689 F1: 30.29% dev loss: 0.1193 F1: 34.51% 0.44 min
> Epoch: 3 Step: 300, train loss: 0.0829 F1: 55.31% dev loss: 0.0957 F1: 46.97% 0.45 min
> Epoch: 5 Step: 400, train loss: 0.0462 F1: 76.89% dev loss: 0.0857 F1: 50.61% 0.45 min
> Epoch: 6 Step: 500, train loss: 0.0330 F1: 85.45% dev loss: 0.1014 F1: 48.55% 0.45 min
> Epoch: 7 Step: 600, train loss: 0.0230 F1: 86.65% dev loss: 0.1033 F1: 48.93% 0.45 min
> Epoch: 9 Step: 700, train loss: 0.0165 F1: 89.11% dev loss: 0.1223 F1: 52.51% 0.45 min
> Epoch: 10 Step: 800, train loss: 0.0085 F1: 90.56% dev loss: 0.1238 F1: 53.57% 0.45 min
> Epoch: 11 Step: 900, train loss: 0.0051 F1: 96.26% dev loss: 0.1221 F1: 55.55% 0.45 min
> Epoch: 12 Step: 1000, train loss: 0.0027 F1: 98.13% dev loss: 0.1208 F1: 50.00% 0.45 min
> Epoch: 14 Step: 1100, train loss: 0.0021 F1: 98.23% dev loss: 0.1342 F1: 52.25% 0.44 min
> Epoch: 15 Step: 1200, train loss: 0.0018 F1: 99.32% dev loss: 0.1334 F1: 56.40% 0.45 min
> Epoch: 16 Step: 1300, train loss: 0.0011 F1: 99.25% dev loss: 0.1451 F1: 52.82% 0.45 min
> Epoch: 18 Step: 1400, train loss: 0.0009 F1: 99.42% dev loss: 0.1370 F1: 52.37% 0.45 min
> Epoch: 19 Step: 1500, train loss: 0.0007 F1: 99.76% dev loss: 0.1345 F1: 54.42% 0.45 min
> Epoch: 20 Step: 1600, train loss: 0.0005 F1: 99.77% dev loss: 0.1433 F1: 54.47% 0.45 min
> Epoch: 22 Step: 1700, train loss: 0.0004 F1: 99.75% dev loss: 0.1431 F1: 53.90% 0.45 min
> Epoch: 23 Step: 1800, train loss: 0.0005 F1: 99.30% dev loss: 0.1402 F1: 56.08% 0.45 min
> Epoch: 24 Step: 1900, train loss: 0.0007 F1: 99.72% dev loss: 0.1361 F1: 55.81% 0.45 min
> Epoch: 25 Step: 2000, train loss: 0.0005 F1: 99.67% dev loss: 0.1368 F1: 55.56% 0.45 min
> Epoch: 27 Step: 2100, train loss: 0.0003 F1: 99.95% dev loss: 0.1444 F1: 55.60% 0.45 min
> Epoch: 28 Step: 2200, train loss: 0.0003 F1: 99.94% dev loss: 0.1447 F1: 57.04% 0.45 min
> Epoch: 29 Step: 2300, train loss: 0.0003 F1: 99.88% dev loss: 0.1495 F1: 55.04% 0.45 min
> Epoch: 31 Step: 2400, train loss: 0.0002 F1: 99.76% dev loss: 0.1451 F1: 55.97% 0.45 min
> Epoch: 32 Step: 2500, train loss: 0.0002 F1: 99.91% dev loss: 0.1468 F1: 56.34% 0.45 min
> Epoch: 33 Step: 2600, train loss: 0.0001 F1: 99.93% dev loss: 0.1491 F1: 55.90% 0.45 min
> Epoch: 35 Step: 2700, train loss: 0.0002 F1: 99.54% dev loss: 0.1496 F1: 53.87% 0.45 min
> Epoch: 36 Step: 2800, train loss: 0.0001 F1: 99.92% dev loss: 0.1510 F1: 54.72% 0.45 min
> Epoch: 37 Step: 2900, train loss: 0.0002 F1: 99.93% dev loss: 0.1477 F1: 56.86% 0.45 min
> Epoch: 38 Step: 3000, train loss: 0.0002 F1: 99.92% dev loss: 0.1478 F1: 56.97% 0.45 min

>>>>>>>>>>>>>>>>>>>>>2021-07-22 02:55:24.357527>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: san
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-san_res14_seed77.pth
>>> seed: 77
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> working_path: ./
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.4678 F1: 19.28% dev loss: 0.1818 F1: 20.85% 0.48 min
> Epoch: 2 Step: 200, train loss: 0.1473 F1: 37.89% dev loss: 0.1233 F1: 44.46% 0.47 min
> Epoch: 3 Step: 300, train loss: 0.0742 F1: 61.01% dev loss: 0.1017 F1: 50.50% 0.47 min
> Epoch: 5 Step: 400, train loss: 0.0451 F1: 76.36% dev loss: 0.0954 F1: 49.78% 0.47 min
> Epoch: 6 Step: 500, train loss: 0.0312 F1: 85.44% dev loss: 0.0899 F1: 54.18% 0.47 min
> Epoch: 7 Step: 600, train loss: 0.0200 F1: 88.01% dev loss: 0.1160 F1: 49.37% 0.47 min
> Epoch: 9 Step: 700, train loss: 0.0119 F1: 95.25% dev loss: 0.1264 F1: 52.00% 0.47 min
> Epoch: 10 Step: 800, train loss: 0.0068 F1: 96.33% dev loss: 0.0980 F1: 57.23% 0.47 min
> Epoch: 11 Step: 900, train loss: 0.0039 F1: 97.44% dev loss: 0.1145 F1: 56.70% 0.47 min
> Epoch: 12 Step: 1000, train loss: 0.0025 F1: 98.31% dev loss: 0.1085 F1: 56.83% 0.47 min
> Epoch: 14 Step: 1100, train loss: 0.0016 F1: 98.76% dev loss: 0.1157 F1: 54.83% 0.47 min
> Epoch: 15 Step: 1200, train loss: 0.0015 F1: 99.12% dev loss: 0.1204 F1: 54.86% 0.47 min
> Epoch: 16 Step: 1300, train loss: 0.0009 F1: 99.25% dev loss: 0.1352 F1: 51.98% 0.47 min
> Epoch: 18 Step: 1400, train loss: 0.0010 F1: 98.82% dev loss: 0.1201 F1: 58.35% 0.47 min
> Epoch: 19 Step: 1500, train loss: 0.0009 F1: 99.59% dev loss: 0.1227 F1: 56.57% 0.47 min
> Epoch: 20 Step: 1600, train loss: 0.0006 F1: 99.86% dev loss: 0.1219 F1: 57.31% 0.47 min
> Epoch: 22 Step: 1700, train loss: 0.0004 F1: 99.80% dev loss: 0.1268 F1: 59.20% 0.47 min
> Epoch: 23 Step: 1800, train loss: 0.0004 F1: 99.85% dev loss: 0.1281 F1: 56.44% 0.47 min
> Epoch: 24 Step: 1900, train loss: 0.0004 F1: 99.72% dev loss: 0.1402 F1: 57.86% 0.47 min
> Epoch: 25 Step: 2000, train loss: 0.0005 F1: 99.71% dev loss: 0.1367 F1: 58.40% 0.47 min
> Epoch: 27 Step: 2100, train loss: 0.0003 F1: 99.94% dev loss: 0.1354 F1: 58.45% 0.47 min
> Epoch: 28 Step: 2200, train loss: 0.0004 F1: 99.84% dev loss: 0.1343 F1: 58.30% 0.48 min
> Epoch: 29 Step: 2300, train loss: 0.0005 F1: 99.79% dev loss: 0.1312 F1: 58.65% 0.48 min
> Epoch: 31 Step: 2400, train loss: 0.0002 F1: 99.98% dev loss: 0.1347 F1: 59.44% 0.48 min
> Epoch: 32 Step: 2500, train loss: 0.0002 F1: 99.89% dev loss: 0.1356 F1: 59.25% 0.47 min
> Epoch: 33 Step: 2600, train loss: 0.0001 F1: 99.97% dev loss: 0.1378 F1: 57.84% 0.47 min
> Epoch: 35 Step: 2700, train loss: 0.0001 F1: 100.00% dev loss: 0.1378 F1: 58.15% 0.47 min
> Epoch: 36 Step: 2800, train loss: 0.0001 F1: 100.00% dev loss: 0.1393 F1: 58.54% 0.47 min
> Epoch: 37 Step: 2900, train loss: 0.0001 F1: 99.91% dev loss: 0.1397 F1: 59.07% 0.47 min
> Epoch: 38 Step: 3000, train loss: 0.0001 F1: 99.93% dev loss: 0.1392 F1: 59.18% 0.48 min

>>>>>>>>>>>>>>>>>>>>>2021-07-22 03:44:54.132223>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-lstm_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.7258 F1: 9.32% dev loss: 0.2612 F1: 9.40% 0.49 min
> Epoch: 2 Step: 200, train loss: 0.2562 F1: 12.11% dev loss: 0.1823 F1: 13.08% 0.49 min
> Epoch: 3 Step: 300, train loss: 0.1573 F1: 27.25% dev loss: 0.1366 F1: 29.09% 0.49 min
> Epoch: 5 Step: 400, train loss: 0.1076 F1: 48.31% dev loss: 0.1260 F1: 36.69% 0.49 min
> Epoch: 6 Step: 500, train loss: 0.0640 F1: 68.49% dev loss: 0.1142 F1: 51.19% 0.49 min
> Epoch: 7 Step: 600, train loss: 0.0410 F1: 77.19% dev loss: 0.1045 F1: 56.74% 0.50 min
> Epoch: 9 Step: 700, train loss: 0.0262 F1: 91.32% dev loss: 0.1025 F1: 58.00% 0.50 min
> Epoch: 10 Step: 800, train loss: 0.0143 F1: 92.93% dev loss: 0.1201 F1: 46.79% 0.50 min
> Epoch: 11 Step: 900, train loss: 0.0109 F1: 93.40% dev loss: 0.1165 F1: 51.23% 0.50 min
> Epoch: 13 Step: 1000, train loss: 0.0066 F1: 96.75% dev loss: 0.1125 F1: 56.21% 0.50 min
> Epoch: 14 Step: 1100, train loss: 0.0051 F1: 97.80% dev loss: 0.1224 F1: 55.24% 0.50 min
> Epoch: 15 Step: 1200, train loss: 0.0033 F1: 98.38% dev loss: 0.1270 F1: 57.71% 0.50 min
> Epoch: 17 Step: 1300, train loss: 0.0025 F1: 99.31% dev loss: 0.1217 F1: 55.03% 0.50 min
> Epoch: 18 Step: 1400, train loss: 0.0021 F1: 98.79% dev loss: 0.1318 F1: 54.83% 0.50 min
> Epoch: 19 Step: 1500, train loss: 0.0015 F1: 99.43% dev loss: 0.1333 F1: 56.56% 0.50 min
> Epoch: 21 Step: 1600, train loss: 0.0010 F1: 99.48% dev loss: 0.1287 F1: 57.43% 0.50 min
> Epoch: 22 Step: 1700, train loss: 0.0010 F1: 99.65% dev loss: 0.1316 F1: 54.84% 0.50 min
> Epoch: 23 Step: 1800, train loss: 0.0014 F1: 98.91% dev loss: 0.1343 F1: 58.79% 0.50 min
> Epoch: 25 Step: 1900, train loss: 0.0016 F1: 99.19% dev loss: 0.1612 F1: 56.23% 0.50 min
> Epoch: 26 Step: 2000, train loss: 0.0008 F1: 99.71% dev loss: 0.1343 F1: 57.84% 0.50 min
> Epoch: 27 Step: 2100, train loss: 0.0007 F1: 99.65% dev loss: 0.1440 F1: 55.58% 0.50 min
> Epoch: 29 Step: 2200, train loss: 0.0005 F1: 99.93% dev loss: 0.1390 F1: 56.46% 0.50 min
> Epoch: 30 Step: 2300, train loss: 0.0004 F1: 99.91% dev loss: 0.1428 F1: 57.19% 0.50 min
> Epoch: 31 Step: 2400, train loss: 0.0005 F1: 99.72% dev loss: 0.1380 F1: 56.06% 0.50 min
> Epoch: 33 Step: 2500, train loss: 0.0003 F1: 99.96% dev loss: 0.1388 F1: 56.42% 0.50 min
> Epoch: 34 Step: 2600, train loss: 0.0003 F1: 99.95% dev loss: 0.1375 F1: 57.51% 0.50 min
> Epoch: 35 Step: 2700, train loss: 0.0003 F1: 99.92% dev loss: 0.1377 F1: 57.42% 0.50 min
> Epoch: 37 Step: 2800, train loss: 0.0002 F1: 99.98% dev loss: 0.1381 F1: 56.96% 0.50 min
> Epoch: 38 Step: 2900, train loss: 0.0003 F1: 99.93% dev loss: 0.1395 F1: 57.02% 0.50 min
> Epoch: 39 Step: 3000, train loss: 0.0003 F1: 99.90% dev loss: 0.1407 F1: 56.30% 0.50 min

>>>>>>>>>>>>>>>>>>>>>2021-07-22 04:00:27.675917>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-lstm_lap14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: lap14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.6743 F1: 9.61% dev loss: 0.1780 F1: 9.65% 0.50 min
> Epoch: 2 Step: 200, train loss: 0.1827 F1: 9.61% dev loss: 0.1627 F1: 9.65% 0.50 min
> Epoch: 3 Step: 300, train loss: 0.1338 F1: 15.25% dev loss: 0.0966 F1: 21.28% 0.50 min
> Epoch: 5 Step: 400, train loss: 0.0887 F1: 43.69% dev loss: 0.0763 F1: 38.67% 0.50 min
> Epoch: 6 Step: 500, train loss: 0.0567 F1: 59.67% dev loss: 0.0660 F1: 54.96% 0.50 min
> Epoch: 7 Step: 600, train loss: 0.0307 F1: 78.38% dev loss: 0.0531 F1: 63.40% 0.50 min
> Epoch: 9 Step: 700, train loss: 0.0186 F1: 85.81% dev loss: 0.0570 F1: 66.44% 0.50 min
> Epoch: 10 Step: 800, train loss: 0.0135 F1: 89.02% dev loss: 0.0621 F1: 58.12% 0.50 min
> Epoch: 11 Step: 900, train loss: 0.0098 F1: 93.30% dev loss: 0.0658 F1: 65.24% 0.50 min
> Epoch: 12 Step: 1000, train loss: 0.0057 F1: 96.11% dev loss: 0.0737 F1: 65.40% 0.50 min
> Epoch: 14 Step: 1100, train loss: 0.0030 F1: 98.24% dev loss: 0.0711 F1: 68.40% 0.50 min
> Epoch: 15 Step: 1200, train loss: 0.0024 F1: 98.52% dev loss: 0.0693 F1: 64.70% 0.50 min
> Epoch: 16 Step: 1300, train loss: 0.0018 F1: 98.85% dev loss: 0.0666 F1: 69.06% 0.50 min
> Epoch: 18 Step: 1400, train loss: 0.0016 F1: 98.63% dev loss: 0.0688 F1: 69.70% 0.50 min
> Epoch: 19 Step: 1500, train loss: 0.0012 F1: 99.30% dev loss: 0.0708 F1: 68.17% 0.50 min
> Epoch: 20 Step: 1600, train loss: 0.0007 F1: 99.59% dev loss: 0.0676 F1: 64.89% 0.50 min
> Epoch: 22 Step: 1700, train loss: 0.0008 F1: 99.84% dev loss: 0.0686 F1: 65.91% 0.50 min
> Epoch: 23 Step: 1800, train loss: 0.0006 F1: 99.42% dev loss: 0.0706 F1: 69.80% 0.50 min
> Epoch: 24 Step: 1900, train loss: 0.0006 F1: 99.75% dev loss: 0.0768 F1: 68.11% 0.50 min
> Epoch: 25 Step: 2000, train loss: 0.0006 F1: 99.71% dev loss: 0.0750 F1: 68.54% 0.50 min
> Epoch: 27 Step: 2100, train loss: 0.0005 F1: 99.91% dev loss: 0.0771 F1: 65.77% 0.50 min
> Epoch: 28 Step: 2200, train loss: 0.0005 F1: 99.69% dev loss: 0.0783 F1: 67.88% 0.50 min
> Epoch: 29 Step: 2300, train loss: 0.0005 F1: 99.86% dev loss: 0.0773 F1: 68.41% 0.50 min
> Epoch: 31 Step: 2400, train loss: 0.0003 F1: 100.00% dev loss: 0.0786 F1: 67.87% 0.50 min
> Epoch: 32 Step: 2500, train loss: 0.0003 F1: 99.82% dev loss: 0.0796 F1: 68.60% 0.50 min
> Epoch: 33 Step: 2600, train loss: 0.0003 F1: 99.71% dev loss: 0.0783 F1: 68.66% 0.50 min
> Epoch: 35 Step: 2700, train loss: 0.0004 F1: 100.00% dev loss: 0.0774 F1: 68.84% 0.50 min
> Epoch: 36 Step: 2800, train loss: 0.0002 F1: 100.00% dev loss: 0.0784 F1: 68.40% 0.50 min
> Epoch: 37 Step: 2900, train loss: 0.0002 F1: 99.90% dev loss: 0.0782 F1: 69.09% 0.50 min
> Epoch: 38 Step: 3000, train loss: 0.0002 F1: 99.93% dev loss: 0.0783 F1: 68.93% 0.50 min

>>>>>>>>>>>>>>>>>>>>>2021-07-22 04:15:44.650288>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-lstm_res16_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: res16
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_Train_SB1_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_dev_SB1_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_test_SB1_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 2 Step: 100, train loss: 0.7110 F1: 9.65% dev loss: 0.1961 F1: 9.58% 0.49 min
> Epoch: 4 Step: 200, train loss: 0.2215 F1: 9.55% dev loss: 0.1495 F1: 9.58% 0.49 min
> Epoch: 6 Step: 300, train loss: 0.1430 F1: 24.73% dev loss: 0.1132 F1: 23.40% 0.49 min
> Epoch: 8 Step: 400, train loss: 0.0779 F1: 53.95% dev loss: 0.0824 F1: 43.12% 0.49 min
> Epoch: 10 Step: 500, train loss: 0.0441 F1: 54.38% dev loss: 0.1130 F1: 41.70% 0.49 min
> Epoch: 12 Step: 600, train loss: 0.0252 F1: 63.36% dev loss: 0.1146 F1: 38.47% 0.49 min
> Epoch: 14 Step: 700, train loss: 0.0162 F1: 67.77% dev loss: 0.1119 F1: 43.14% 0.49 min
> Epoch: 16 Step: 800, train loss: 0.0126 F1: 85.73% dev loss: 0.1079 F1: 45.49% 0.49 min
> Epoch: 18 Step: 900, train loss: 0.0064 F1: 93.81% dev loss: 0.0849 F1: 52.10% 0.49 min
> Epoch: 20 Step: 1000, train loss: 0.0031 F1: 96.32% dev loss: 0.0913 F1: 51.15% 0.49 min
> Epoch: 22 Step: 1100, train loss: 0.0025 F1: 97.98% dev loss: 0.0968 F1: 52.85% 0.49 min
> Epoch: 24 Step: 1200, train loss: 0.0015 F1: 98.32% dev loss: 0.0987 F1: 51.90% 0.49 min
> Epoch: 26 Step: 1300, train loss: 0.0010 F1: 99.35% dev loss: 0.0974 F1: 52.21% 0.49 min
> Epoch: 28 Step: 1400, train loss: 0.0011 F1: 99.63% dev loss: 0.1250 F1: 49.47% 0.49 min
> Epoch: 30 Step: 1500, train loss: 0.0009 F1: 99.54% dev loss: 0.1142 F1: 50.96% 0.49 min
> Epoch: 32 Step: 1600, train loss: 0.0007 F1: 99.66% dev loss: 0.1002 F1: 52.65% 0.49 min
> Epoch: 34 Step: 1700, train loss: 0.0005 F1: 98.86% dev loss: 0.1042 F1: 52.66% 0.49 min
> Epoch: 36 Step: 1800, train loss: 0.0003 F1: 100.00% dev loss: 0.1060 F1: 51.80% 0.49 min
> Epoch: 38 Step: 1900, train loss: 0.0003 F1: 99.99% dev loss: 0.1061 F1: 52.65% 0.49 min
> Epoch: 40 Step: 2000, train loss: 0.0004 F1: 99.84% dev loss: 0.1023 F1: 53.92% 0.49 min
> Epoch: 42 Step: 2100, train loss: 0.0002 F1: 99.85% dev loss: 0.1072 F1: 53.61% 0.49 min
> Epoch: 44 Step: 2200, train loss: 0.0003 F1: 99.97% dev loss: 0.1008 F1: 53.85% 0.49 min
> Epoch: 46 Step: 2300, train loss: 0.0002 F1: 99.88% dev loss: 0.1026 F1: 54.41% 0.49 min
> Epoch: 48 Step: 2400, train loss: 0.0003 F1: 99.99% dev loss: 0.1031 F1: 54.69% 0.49 min
> Epoch: 51 Step: 2500, train loss: 0.0003 F1: 70.00% dev loss: 0.1045 F1: 53.68% 0.49 min
> Epoch: 53 Step: 2600, train loss: 0.0002 F1: 80.00% dev loss: 0.1063 F1: 54.29% 0.49 min
> Epoch: 55 Step: 2700, train loss: 0.0002 F1: 80.00% dev loss: 0.1057 F1: 54.46% 0.49 min
> Epoch: 57 Step: 2800, train loss: 0.0002 F1: 80.00% dev loss: 0.1068 F1: 54.46% 0.49 min
> Epoch: 59 Step: 2900, train loss: 0.0002 F1: 99.97% dev loss: 0.1073 F1: 54.43% 0.49 min
> Epoch: 61 Step: 3000, train loss: 0.0002 F1: 99.98% dev loss: 0.1069 F1: 54.43% 0.49 min

>>>>>>>>>>>>>>>>>>>>>2021-07-22 04:31:04.826177>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: san
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-san_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.4998 F1: 13.67% dev loss: 0.1822 F1: 20.15% 0.48 min
> Epoch: 2 Step: 200, train loss: 0.1551 F1: 36.58% dev loss: 0.1063 F1: 49.98% 0.48 min
> Epoch: 3 Step: 300, train loss: 0.0757 F1: 57.36% dev loss: 0.0909 F1: 50.47% 0.48 min
> Epoch: 5 Step: 400, train loss: 0.0425 F1: 81.88% dev loss: 0.1545 F1: 52.00% 0.48 min
> Epoch: 6 Step: 500, train loss: 0.0277 F1: 82.96% dev loss: 0.1141 F1: 55.06% 0.48 min
> Epoch: 7 Step: 600, train loss: 0.0227 F1: 84.62% dev loss: 0.1233 F1: 56.74% 0.48 min
> Epoch: 9 Step: 700, train loss: 0.0111 F1: 95.33% dev loss: 0.1278 F1: 56.11% 0.48 min
> Epoch: 10 Step: 800, train loss: 0.0071 F1: 96.52% dev loss: 0.1289 F1: 55.51% 0.48 min
> Epoch: 11 Step: 900, train loss: 0.0036 F1: 97.85% dev loss: 0.1245 F1: 55.88% 0.48 min
> Epoch: 13 Step: 1000, train loss: 0.0026 F1: 97.27% dev loss: 0.1339 F1: 54.68% 0.48 min
> Epoch: 14 Step: 1100, train loss: 0.0017 F1: 98.85% dev loss: 0.1418 F1: 53.34% 0.48 min
> Epoch: 15 Step: 1200, train loss: 0.0011 F1: 99.41% dev loss: 0.1407 F1: 54.27% 0.48 min
> Epoch: 17 Step: 1300, train loss: 0.0007 F1: 99.40% dev loss: 0.1371 F1: 54.64% 0.48 min
> Epoch: 18 Step: 1400, train loss: 0.0005 F1: 99.63% dev loss: 0.1471 F1: 54.58% 0.48 min
> Epoch: 19 Step: 1500, train loss: 0.0006 F1: 99.83% dev loss: 0.1433 F1: 53.90% 0.48 min
> Epoch: 21 Step: 1600, train loss: 0.0005 F1: 99.84% dev loss: 0.1466 F1: 54.16% 0.48 min
> Epoch: 22 Step: 1700, train loss: 0.0004 F1: 99.68% dev loss: 0.1519 F1: 54.95% 0.48 min
> Epoch: 23 Step: 1800, train loss: 0.0003 F1: 99.87% dev loss: 0.1511 F1: 55.87% 0.48 min
> Epoch: 25 Step: 1900, train loss: 0.0003 F1: 99.92% dev loss: 0.1367 F1: 56.26% 0.48 min
> Epoch: 26 Step: 2000, train loss: 0.0002 F1: 99.97% dev loss: 0.1460 F1: 55.80% 0.48 min
> Epoch: 27 Step: 2100, train loss: 0.0002 F1: 99.93% dev loss: 0.1488 F1: 56.81% 0.48 min
> Epoch: 29 Step: 2200, train loss: 0.0002 F1: 99.93% dev loss: 0.1488 F1: 56.85% 0.48 min
> Epoch: 30 Step: 2300, train loss: 0.0002 F1: 99.98% dev loss: 0.1517 F1: 55.07% 0.48 min
> Epoch: 31 Step: 2400, train loss: 0.0002 F1: 99.93% dev loss: 0.1534 F1: 54.73% 0.48 min
> Epoch: 33 Step: 2500, train loss: 0.0002 F1: 99.93% dev loss: 0.1541 F1: 54.59% 0.47 min
> Epoch: 34 Step: 2600, train loss: 0.0002 F1: 99.89% dev loss: 0.1518 F1: 57.23% 0.48 min
> Epoch: 35 Step: 2700, train loss: 0.0002 F1: 99.94% dev loss: 0.1562 F1: 56.37% 0.47 min
> Epoch: 37 Step: 2800, train loss: 0.0001 F1: 99.91% dev loss: 0.1521 F1: 56.55% 0.47 min
> Epoch: 38 Step: 2900, train loss: 0.0001 F1: 99.98% dev loss: 0.1530 F1: 55.89% 0.47 min
> Epoch: 39 Step: 3000, train loss: 0.0001 F1: 99.99% dev loss: 0.1532 F1: 56.22% 0.47 min

>>>>>>>>>>>>>>>>>>>>>2021-07-22 04:46:05.949013>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: san
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-san_lap14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: lap14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.4121 F1: 10.83% dev loss: 0.1374 F1: 11.33% 0.48 min
> Epoch: 2 Step: 200, train loss: 0.1192 F1: 32.22% dev loss: 0.0716 F1: 40.28% 0.48 min
> Epoch: 3 Step: 300, train loss: 0.0620 F1: 58.05% dev loss: 0.0523 F1: 55.40% 0.47 min
> Epoch: 5 Step: 400, train loss: 0.0322 F1: 76.67% dev loss: 0.0605 F1: 58.27% 0.47 min
> Epoch: 6 Step: 500, train loss: 0.0215 F1: 80.60% dev loss: 0.0670 F1: 62.90% 0.47 min
> Epoch: 7 Step: 600, train loss: 0.0179 F1: 85.78% dev loss: 0.0729 F1: 63.36% 0.47 min
> Epoch: 9 Step: 700, train loss: 0.0083 F1: 94.33% dev loss: 0.0603 F1: 64.46% 0.48 min
> Epoch: 10 Step: 800, train loss: 0.0039 F1: 95.96% dev loss: 0.0643 F1: 61.99% 0.47 min
> Epoch: 11 Step: 900, train loss: 0.0026 F1: 97.00% dev loss: 0.0747 F1: 62.31% 0.48 min
> Epoch: 12 Step: 1000, train loss: 0.0019 F1: 98.36% dev loss: 0.0698 F1: 65.82% 0.47 min
> Epoch: 14 Step: 1100, train loss: 0.0014 F1: 98.86% dev loss: 0.0782 F1: 62.76% 0.48 min
> Epoch: 15 Step: 1200, train loss: 0.0008 F1: 99.25% dev loss: 0.0735 F1: 62.56% 0.47 min
> Epoch: 16 Step: 1300, train loss: 0.0006 F1: 99.31% dev loss: 0.0750 F1: 67.13% 0.47 min
> Epoch: 18 Step: 1400, train loss: 0.0005 F1: 99.93% dev loss: 0.0763 F1: 63.73% 0.48 min
> Epoch: 19 Step: 1500, train loss: 0.0004 F1: 99.65% dev loss: 0.0830 F1: 60.78% 0.47 min
> Epoch: 20 Step: 1600, train loss: 0.0003 F1: 99.70% dev loss: 0.0758 F1: 63.07% 0.47 min
> Epoch: 22 Step: 1700, train loss: 0.0002 F1: 99.92% dev loss: 0.0775 F1: 66.38% 0.48 min
> Epoch: 23 Step: 1800, train loss: 0.0003 F1: 99.59% dev loss: 0.0862 F1: 64.14% 0.48 min
> Epoch: 24 Step: 1900, train loss: 0.0003 F1: 99.71% dev loss: 0.0835 F1: 61.85% 0.48 min
> Epoch: 25 Step: 2000, train loss: 0.0004 F1: 99.65% dev loss: 0.0799 F1: 63.38% 0.48 min
> Epoch: 27 Step: 2100, train loss: 0.0004 F1: 99.66% dev loss: 0.0822 F1: 62.46% 0.47 min
> Epoch: 28 Step: 2200, train loss: 0.0002 F1: 99.95% dev loss: 0.0810 F1: 61.05% 0.48 min
> Epoch: 29 Step: 2300, train loss: 0.0001 F1: 99.98% dev loss: 0.0830 F1: 59.69% 0.47 min
> Epoch: 31 Step: 2400, train loss: 0.0001 F1: 100.00% dev loss: 0.0810 F1: 64.29% 0.48 min
> Epoch: 32 Step: 2500, train loss: 0.0001 F1: 100.00% dev loss: 0.0811 F1: 60.56% 0.48 min
> Epoch: 33 Step: 2600, train loss: 0.0001 F1: 99.95% dev loss: 0.0820 F1: 61.57% 0.48 min
> Epoch: 35 Step: 2700, train loss: 0.0001 F1: 100.00% dev loss: 0.0821 F1: 61.64% 0.48 min
> Epoch: 36 Step: 2800, train loss: 0.0001 F1: 100.00% dev loss: 0.0822 F1: 61.87% 0.48 min
> Epoch: 37 Step: 2900, train loss: 0.0001 F1: 99.98% dev loss: 0.0829 F1: 61.16% 0.48 min
> Epoch: 38 Step: 3000, train loss: 0.0001 F1: 99.85% dev loss: 0.0831 F1: 61.60% 0.48 min

>>>>>>>>>>>>>>>>>>>>>2021-07-22 05:00:46.257655>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: san
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-san_res16_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: res16
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_Train_SB1_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_dev_SB1_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_test_SB1_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 2 Step: 100, train loss: 0.4627 F1: 14.90% dev loss: 0.1381 F1: 13.55% 0.48 min
> Epoch: 4 Step: 200, train loss: 0.1252 F1: 41.62% dev loss: 0.0893 F1: 31.79% 0.47 min
> Epoch: 6 Step: 300, train loss: 0.0556 F1: 51.94% dev loss: 0.0979 F1: 40.24% 0.47 min
> Epoch: 8 Step: 400, train loss: 0.0285 F1: 88.65% dev loss: 0.1040 F1: 42.15% 0.47 min
> Epoch: 10 Step: 500, train loss: 0.0234 F1: 73.36% dev loss: 0.1089 F1: 45.65% 0.47 min
> Epoch: 12 Step: 600, train loss: 0.0093 F1: 95.09% dev loss: 0.0927 F1: 47.51% 0.47 min
> Epoch: 14 Step: 700, train loss: 0.0044 F1: 98.34% dev loss: 0.0872 F1: 51.71% 0.47 min
> Epoch: 16 Step: 800, train loss: 0.0010 F1: 99.19% dev loss: 0.0917 F1: 50.90% 0.47 min
> Epoch: 18 Step: 900, train loss: 0.0008 F1: 99.71% dev loss: 0.1026 F1: 51.23% 0.47 min
> Epoch: 20 Step: 1000, train loss: 0.0006 F1: 99.50% dev loss: 0.0951 F1: 53.01% 0.47 min
> Epoch: 22 Step: 1100, train loss: 0.0005 F1: 99.80% dev loss: 0.0934 F1: 53.03% 0.47 min
> Epoch: 24 Step: 1200, train loss: 0.0004 F1: 99.67% dev loss: 0.1127 F1: 48.98% 0.47 min
> Epoch: 26 Step: 1300, train loss: 0.0006 F1: 99.91% dev loss: 0.1041 F1: 51.36% 0.47 min
> Epoch: 28 Step: 1400, train loss: 0.0003 F1: 99.36% dev loss: 0.1014 F1: 51.83% 0.47 min
> Epoch: 30 Step: 1500, train loss: 0.0002 F1: 99.94% dev loss: 0.1011 F1: 49.92% 0.47 min
> Epoch: 32 Step: 1600, train loss: 0.0001 F1: 99.99% dev loss: 0.1021 F1: 49.87% 0.47 min
> Epoch: 34 Step: 1700, train loss: 0.0002 F1: 99.99% dev loss: 0.1031 F1: 53.89% 0.47 min
> Epoch: 36 Step: 1800, train loss: 0.0001 F1: 99.85% dev loss: 0.1061 F1: 52.65% 0.47 min
> Epoch: 38 Step: 1900, train loss: 0.0001 F1: 99.97% dev loss: 0.1059 F1: 54.04% 0.47 min
> Epoch: 40 Step: 2000, train loss: 0.0001 F1: 99.95% dev loss: 0.1060 F1: 50.87% 0.47 min
> Epoch: 42 Step: 2100, train loss: 0.0001 F1: 99.99% dev loss: 0.1076 F1: 52.13% 0.47 min
> Epoch: 44 Step: 2200, train loss: 0.0001 F1: 99.89% dev loss: 0.1094 F1: 52.71% 0.47 min
> Epoch: 46 Step: 2300, train loss: 0.0002 F1: 99.97% dev loss: 0.1075 F1: 53.03% 0.47 min
> Epoch: 48 Step: 2400, train loss: 0.0001 F1: 99.99% dev loss: 0.1079 F1: 51.81% 0.47 min
> Epoch: 51 Step: 2500, train loss: 0.0001 F1: 70.00% dev loss: 0.1108 F1: 53.15% 0.47 min
> Epoch: 53 Step: 2600, train loss: 0.0001 F1: 80.00% dev loss: 0.1105 F1: 52.82% 0.47 min
> Epoch: 55 Step: 2700, train loss: 0.0001 F1: 80.00% dev loss: 0.1116 F1: 52.57% 0.47 min
> Epoch: 57 Step: 2800, train loss: 0.0001 F1: 80.00% dev loss: 0.1096 F1: 52.79% 0.47 min
> Epoch: 59 Step: 2900, train loss: 0.0001 F1: 100.00% dev loss: 0.1096 F1: 53.07% 0.47 min
> Epoch: 61 Step: 3000, train loss: 0.0000 F1: 99.93% dev loss: 0.1099 F1: 53.18% 0.47 min

>>>>>>>>>>>>>>>>>>>>>2021-07-22 05:18:55.058330>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: crf
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-crf_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 18.9098 F1: 12.50% dev loss: 7.2310 F1: 14.28% 0.61 min
> Epoch: 2 Step: 200, train loss: 6.0674 F1: 28.71% dev loss: 4.6161 F1: 32.52% 0.61 min
> Epoch: 3 Step: 300, train loss: 3.5011 F1: 47.55% dev loss: 4.0074 F1: 46.56% 0.62 min
> Epoch: 5 Step: 400, train loss: 2.2334 F1: 69.39% dev loss: 5.0197 F1: 48.82% 0.62 min
> Epoch: 6 Step: 500, train loss: 1.5345 F1: 79.32% dev loss: 4.6190 F1: 52.90% 0.61 min
> Epoch: 7 Step: 600, train loss: 1.1633 F1: 82.59% dev loss: 4.8025 F1: 57.77% 0.61 min
> Epoch: 9 Step: 700, train loss: 0.8813 F1: 91.57% dev loss: 5.1991 F1: 56.34% 0.62 min
> Epoch: 10 Step: 800, train loss: 0.5591 F1: 94.71% dev loss: 5.1551 F1: 57.38% 0.62 min
> Epoch: 11 Step: 900, train loss: 0.3376 F1: 96.37% dev loss: 5.4997 F1: 56.67% 0.62 min
> Epoch: 13 Step: 1000, train loss: 0.3273 F1: 96.31% dev loss: 5.9577 F1: 58.09% 0.61 min
> Epoch: 14 Step: 1100, train loss: 0.2307 F1: 97.31% dev loss: 6.4463 F1: 57.85% 0.61 min
> Epoch: 15 Step: 1200, train loss: 0.1547 F1: 98.38% dev loss: 6.8181 F1: 53.72% 0.62 min
> Epoch: 17 Step: 1300, train loss: 0.1244 F1: 99.18% dev loss: 6.6105 F1: 59.32% 0.60 min
> Epoch: 18 Step: 1400, train loss: 0.1121 F1: 99.31% dev loss: 7.3432 F1: 57.80% 0.59 min
> Epoch: 19 Step: 1500, train loss: 0.0680 F1: 99.42% dev loss: 7.0746 F1: 61.50% 0.59 min
> Epoch: 21 Step: 1600, train loss: 0.0413 F1: 99.63% dev loss: 7.3568 F1: 58.06% 0.59 min
> Epoch: 22 Step: 1700, train loss: 0.0610 F1: 99.77% dev loss: 7.7565 F1: 58.48% 0.59 min
> Epoch: 23 Step: 1800, train loss: 0.0363 F1: 99.76% dev loss: 7.7034 F1: 59.46% 0.61 min
> Epoch: 25 Step: 1900, train loss: 0.0270 F1: 99.87% dev loss: 7.6440 F1: 58.66% 0.61 min
> Epoch: 26 Step: 2000, train loss: 0.0285 F1: 99.76% dev loss: 7.4842 F1: 58.40% 0.61 min
> Epoch: 27 Step: 2100, train loss: 0.0176 F1: 99.86% dev loss: 8.1803 F1: 60.59% 0.61 min
> Epoch: 29 Step: 2200, train loss: 0.0141 F1: 99.94% dev loss: 7.4566 F1: 59.75% 0.61 min
> Epoch: 30 Step: 2300, train loss: 0.0212 F1: 99.95% dev loss: 8.0702 F1: 57.82% 0.61 min
> Epoch: 31 Step: 2400, train loss: 0.0077 F1: 99.98% dev loss: 7.7489 F1: 59.31% 0.61 min
> Epoch: 33 Step: 2500, train loss: 0.0083 F1: 99.97% dev loss: 7.8835 F1: 59.19% 0.61 min
> Epoch: 34 Step: 2600, train loss: 0.0023 F1: 100.00% dev loss: 8.3295 F1: 57.42% 0.61 min
> Epoch: 35 Step: 2700, train loss: 0.0057 F1: 99.98% dev loss: 8.1795 F1: 57.69% 0.60 min
> Epoch: 37 Step: 2800, train loss: 0.0123 F1: 100.00% dev loss: 8.2457 F1: 57.65% 0.59 min
> Epoch: 38 Step: 2900, train loss: 0.0039 F1: 100.00% dev loss: 8.2447 F1: 57.69% 0.59 min
> Epoch: 39 Step: 3000, train loss: 0.0032 F1: 99.98% dev loss: 8.2715 F1: 57.68% 0.60 min

>>>>>>>>>>>>>>>>>>>>>2021-07-22 05:38:11.390322>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: crf
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-crf_lap14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: lap14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 17.7925 F1: 9.61% dev loss: 5.6946 F1: 9.65% 0.60 min
> Epoch: 2 Step: 200, train loss: 4.7880 F1: 23.53% dev loss: 3.3722 F1: 30.59% 0.59 min
> Epoch: 3 Step: 300, train loss: 2.8701 F1: 47.54% dev loss: 2.4454 F1: 50.35% 0.60 min
> Epoch: 5 Step: 400, train loss: 1.8325 F1: 69.73% dev loss: 2.4359 F1: 56.47% 0.60 min
> Epoch: 6 Step: 500, train loss: 1.1464 F1: 79.53% dev loss: 2.7164 F1: 61.50% 0.62 min
> Epoch: 7 Step: 600, train loss: 0.8539 F1: 85.46% dev loss: 3.1253 F1: 58.03% 0.62 min
> Epoch: 9 Step: 700, train loss: 0.5691 F1: 89.13% dev loss: 3.0214 F1: 65.25% 0.62 min
> Epoch: 10 Step: 800, train loss: 0.4211 F1: 94.15% dev loss: 3.4056 F1: 57.83% 0.62 min
> Epoch: 11 Step: 900, train loss: 0.3066 F1: 95.85% dev loss: 3.1730 F1: 66.06% 0.62 min
> Epoch: 12 Step: 1000, train loss: 0.2203 F1: 97.34% dev loss: 3.8743 F1: 63.39% 0.62 min
> Epoch: 14 Step: 1100, train loss: 0.1348 F1: 98.85% dev loss: 3.7619 F1: 63.66% 0.62 min
> Epoch: 15 Step: 1200, train loss: 0.1241 F1: 98.45% dev loss: 3.9669 F1: 62.58% 0.62 min
> Epoch: 16 Step: 1300, train loss: 0.0870 F1: 98.93% dev loss: 4.6666 F1: 63.78% 0.62 min
> Epoch: 18 Step: 1400, train loss: 0.0782 F1: 98.85% dev loss: 4.3992 F1: 63.19% 0.62 min
> Epoch: 19 Step: 1500, train loss: 0.0722 F1: 98.34% dev loss: 3.9603 F1: 66.68% 0.62 min
> Epoch: 20 Step: 1600, train loss: 0.0460 F1: 99.50% dev loss: 4.4206 F1: 64.18% 0.62 min
> Epoch: 22 Step: 1700, train loss: 0.0333 F1: 99.60% dev loss: 4.4870 F1: 65.81% 0.61 min
> Epoch: 23 Step: 1800, train loss: 0.0305 F1: 99.45% dev loss: 4.9383 F1: 65.32% 0.59 min
> Epoch: 24 Step: 1900, train loss: 0.0413 F1: 99.28% dev loss: 4.6083 F1: 65.00% 0.59 min
> Epoch: 25 Step: 2000, train loss: 0.0377 F1: 99.61% dev loss: 4.5619 F1: 65.91% 0.59 min
> Epoch: 27 Step: 2100, train loss: 0.0191 F1: 100.00% dev loss: 4.5658 F1: 66.65% 0.59 min
> Epoch: 28 Step: 2200, train loss: 0.0240 F1: 99.77% dev loss: 4.7619 F1: 63.50% 0.59 min
> Epoch: 29 Step: 2300, train loss: 0.0056 F1: 99.98% dev loss: 4.9302 F1: 64.09% 0.59 min
> Epoch: 31 Step: 2400, train loss: 0.0101 F1: 99.93% dev loss: 4.9382 F1: 65.24% 0.59 min
> Epoch: 32 Step: 2500, train loss: 0.0034 F1: 100.00% dev loss: 4.8486 F1: 63.76% 0.59 min
> Epoch: 33 Step: 2600, train loss: 0.0014 F1: 100.00% dev loss: 4.9919 F1: 64.65% 0.59 min
> Epoch: 35 Step: 2700, train loss: 0.0012 F1: 100.00% dev loss: 4.9824 F1: 65.00% 0.59 min
> Epoch: 36 Step: 2800, train loss: 0.0027 F1: 99.98% dev loss: 5.0320 F1: 66.92% 0.59 min
> Epoch: 37 Step: 2900, train loss: 0.0026 F1: 99.99% dev loss: 5.0039 F1: 66.33% 0.59 min
> Epoch: 38 Step: 3000, train loss: 0.0029 F1: 99.97% dev loss: 4.9873 F1: 65.82% 0.59 min

>>>>>>>>>>>>>>>>>>>>>2021-07-22 05:56:53.950903>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: crf
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-crf_res16_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: res16
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_Train_SB1_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_dev_SB1_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_test_SB1_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 2 Step: 100, train loss: 17.1445 F1: 9.65% dev loss: 5.9312 F1: 9.58% 0.59 min
> Epoch: 4 Step: 200, train loss: 5.1035 F1: 39.19% dev loss: 3.6969 F1: 25.57% 0.59 min
> Epoch: 6 Step: 300, train loss: 2.5247 F1: 54.39% dev loss: 3.8982 F1: 39.75% 0.59 min
> Epoch: 8 Step: 400, train loss: 1.4886 F1: 60.15% dev loss: 3.7788 F1: 40.54% 0.59 min
> Epoch: 10 Step: 500, train loss: 1.1411 F1: 69.84% dev loss: 5.0183 F1: 39.12% 0.59 min
> Epoch: 12 Step: 600, train loss: 0.6302 F1: 83.96% dev loss: 4.5775 F1: 41.31% 0.60 min
> Epoch: 14 Step: 700, train loss: 0.3655 F1: 93.35% dev loss: 5.3445 F1: 45.04% 0.59 min
> Epoch: 16 Step: 800, train loss: 0.3631 F1: 93.93% dev loss: 5.4289 F1: 43.20% 0.59 min
> Epoch: 18 Step: 900, train loss: 0.1997 F1: 99.03% dev loss: 4.7054 F1: 47.61% 0.59 min
> Epoch: 20 Step: 1000, train loss: 0.1500 F1: 99.48% dev loss: 5.5125 F1: 47.54% 0.59 min
> Epoch: 22 Step: 1100, train loss: 0.0750 F1: 99.39% dev loss: 5.9239 F1: 46.39% 0.59 min
> Epoch: 24 Step: 1200, train loss: 0.0660 F1: 99.47% dev loss: 5.5916 F1: 48.95% 0.59 min
> Epoch: 26 Step: 1300, train loss: 0.0408 F1: 98.60% dev loss: 6.2369 F1: 47.74% 0.58 min
> Epoch: 28 Step: 1400, train loss: 0.0287 F1: 99.92% dev loss: 5.8171 F1: 47.89% 0.59 min
> Epoch: 30 Step: 1500, train loss: 0.0227 F1: 99.75% dev loss: 6.0343 F1: 49.17% 0.58 min
> Epoch: 32 Step: 1600, train loss: 0.0240 F1: 99.48% dev loss: 6.2049 F1: 49.17% 0.59 min
> Epoch: 34 Step: 1700, train loss: 0.0172 F1: 99.86% dev loss: 6.1533 F1: 47.60% 0.58 min
> Epoch: 36 Step: 1800, train loss: 0.0274 F1: 99.91% dev loss: 6.0609 F1: 49.15% 0.59 min
> Epoch: 38 Step: 1900, train loss: 0.0138 F1: 99.96% dev loss: 6.5138 F1: 47.09% 0.58 min
> Epoch: 40 Step: 2000, train loss: 0.0040 F1: 99.99% dev loss: 6.3237 F1: 48.92% 0.58 min
> Epoch: 42 Step: 2100, train loss: 0.0083 F1: 100.00% dev loss: 6.3672 F1: 50.12% 0.59 min
> Epoch: 44 Step: 2200, train loss: 0.0042 F1: 99.96% dev loss: 6.4015 F1: 50.40% 0.60 min
> Epoch: 46 Step: 2300, train loss: 0.0009 F1: 100.00% dev loss: 6.4142 F1: 50.62% 0.59 min
> Epoch: 48 Step: 2400, train loss: 0.0009 F1: 100.00% dev loss: 6.6047 F1: 50.26% 0.59 min
> Epoch: 51 Step: 2500, train loss: 0.0053 F1: 70.00% dev loss: 6.3267 F1: 50.18% 0.59 min
> Epoch: 53 Step: 2600, train loss: 0.0012 F1: 80.00% dev loss: 6.2688 F1: 50.50% 0.59 min
> Epoch: 55 Step: 2700, train loss: 0.0008 F1: 80.00% dev loss: 6.3124 F1: 50.94% 0.59 min
> Epoch: 57 Step: 2800, train loss: 0.0010 F1: 80.00% dev loss: 6.4193 F1: 50.31% 0.59 min
> Epoch: 59 Step: 2900, train loss: 0.0007 F1: 100.00% dev loss: 6.4426 F1: 50.66% 0.59 min
> Epoch: 61 Step: 3000, train loss: 0.0008 F1: 100.00% dev loss: 6.4514 F1: 50.66% 0.59 min

>>>>>>>>>>>>>>>>>>>>>2021-07-22 04:15:58.427641>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/content/drive/MyDrive/E2E_ABSA/data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': '/content/drive/MyDrive/E2E_ABSA/data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': '/content/drive/MyDrive/E2E_ABSA/data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.5572 F1: 10.97% dev loss: 0.2059 F1: 12.17% 1.17 min
> Epoch: 2 Step: 200, train loss: 0.1714 F1: 28.97% dev loss: 0.1129 F1: 34.96% 1.24 min
> Epoch: 3 Step: 300, train loss: 0.0827 F1: 55.75% dev loss: 0.0947 F1: 52.35% 1.24 min
> Epoch: 5 Step: 400, train loss: 0.0472 F1: 74.03% dev loss: 0.1317 F1: 44.57% 1.25 min
> Epoch: 6 Step: 500, train loss: 0.0304 F1: 82.99% dev loss: 0.1080 F1: 53.90% 1.25 min
> Epoch: 7 Step: 600, train loss: 0.0172 F1: 90.01% dev loss: 0.1026 F1: 58.75% 1.25 min
> Epoch: 9 Step: 700, train loss: 0.0093 F1: 96.29% dev loss: 0.1276 F1: 55.66% 1.25 min
> Epoch: 10 Step: 800, train loss: 0.0060 F1: 96.27% dev loss: 0.1230 F1: 54.91% 1.24 min
> Epoch: 11 Step: 900, train loss: 0.0043 F1: 96.99% dev loss: 0.1213 F1: 58.07% 1.25 min
> Epoch: 13 Step: 1000, train loss: 0.0023 F1: 99.33% dev loss: 0.1299 F1: 59.94% 1.24 min
> Epoch: 14 Step: 1100, train loss: 0.0017 F1: 99.05% dev loss: 0.1408 F1: 58.61% 1.25 min
> Epoch: 15 Step: 1200, train loss: 0.0013 F1: 99.15% dev loss: 0.1381 F1: 59.25% 1.25 min
> Epoch: 17 Step: 1300, train loss: 0.0008 F1: 99.75% dev loss: 0.1371 F1: 59.65% 1.24 min
> Epoch: 18 Step: 1400, train loss: 0.0011 F1: 99.39% dev loss: 0.1426 F1: 59.63% 1.24 min
> Epoch: 19 Step: 1500, train loss: 0.0010 F1: 99.59% dev loss: 0.1293 F1: 60.34% 1.24 min
> Epoch: 21 Step: 1600, train loss: 0.0005 F1: 99.90% dev loss: 0.1366 F1: 59.97% 1.24 min
> Epoch: 22 Step: 1700, train loss: 0.0003 F1: 99.79% dev loss: 0.1419 F1: 59.38% 1.24 min
> Epoch: 23 Step: 1800, train loss: 0.0002 F1: 99.88% dev loss: 0.1413 F1: 60.71% 1.25 min
> Epoch: 25 Step: 1900, train loss: 0.0002 F1: 99.90% dev loss: 0.1439 F1: 61.79% 1.25 min
> Epoch: 26 Step: 2000, train loss: 0.0002 F1: 99.90% dev loss: 0.1468 F1: 60.35% 1.25 min
> Epoch: 27 Step: 2100, train loss: 0.0002 F1: 99.76% dev loss: 0.1364 F1: 62.49% 1.24 min
> Epoch: 29 Step: 2200, train loss: 0.0002 F1: 99.96% dev loss: 0.1485 F1: 60.10% 1.25 min
> Epoch: 30 Step: 2300, train loss: 0.0001 F1: 99.92% dev loss: 0.1552 F1: 60.68% 1.24 min
> Epoch: 31 Step: 2400, train loss: 0.0002 F1: 99.96% dev loss: 0.1466 F1: 60.51% 1.25 min
> Epoch: 33 Step: 2500, train loss: 0.0001 F1: 99.99% dev loss: 0.1458 F1: 61.01% 1.25 min
> Epoch: 34 Step: 2600, train loss: 0.0002 F1: 99.83% dev loss: 0.1529 F1: 59.59% 1.25 min
> Epoch: 35 Step: 2700, train loss: 0.0001 F1: 99.95% dev loss: 0.1461 F1: 61.84% 1.25 min
> Epoch: 37 Step: 2800, train loss: 0.0003 F1: 99.97% dev loss: 0.1459 F1: 61.83% 1.24 min
> Epoch: 38 Step: 2900, train loss: 0.0001 F1: 99.83% dev loss: 0.1466 F1: 60.77% 1.24 min
> Epoch: 39 Step: 3000, train loss: 0.0001 F1: 99.96% dev loss: 0.1460 F1: 60.73% 1.25 min


>>>>>>>>>>>>>>>>>>>>>2021-07-22 05:04:39.422503>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_lap14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: lap14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/content/drive/MyDrive/E2E_ABSA/data/Semeval2014/processed/Laptop_Train_v2.csv', 'dev': '/content/drive/MyDrive/E2E_ABSA/data/Semeval2014/processed/Laptop_dev_v2.csv', 'test': '/content/drive/MyDrive/E2E_ABSA/data/Semeval2014/processed/Laptop_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.4902 F1: 9.61% dev loss: 0.1495 F1: 9.65% 1.08 min
> Epoch: 2 Step: 200, train loss: 0.1324 F1: 26.77% dev loss: 0.0833 F1: 37.00% 1.14 min
> Epoch: 3 Step: 300, train loss: 0.0703 F1: 54.97% dev loss: 0.0512 F1: 58.08% 1.19 min
> Epoch: 5 Step: 400, train loss: 0.0386 F1: 74.69% dev loss: 0.0522 F1: 61.64% 1.21 min
> Epoch: 6 Step: 500, train loss: 0.0236 F1: 78.39% dev loss: 0.0736 F1: 55.26% 1.23 min
> Epoch: 7 Step: 600, train loss: 0.0151 F1: 88.73% dev loss: 0.0614 F1: 61.93% 1.24 min
> Epoch: 9 Step: 700, train loss: 0.0070 F1: 94.43% dev loss: 0.0599 F1: 65.09% 1.25 min
> Epoch: 10 Step: 800, train loss: 0.0041 F1: 96.00% dev loss: 0.0694 F1: 58.44% 1.25 min
> Epoch: 11 Step: 900, train loss: 0.0025 F1: 98.28% dev loss: 0.0647 F1: 64.61% 1.24 min
> Epoch: 12 Step: 1000, train loss: 0.0015 F1: 98.81% dev loss: 0.0671 F1: 64.16% 1.25 min
> Epoch: 14 Step: 1100, train loss: 0.0012 F1: 99.31% dev loss: 0.0703 F1: 65.66% 1.24 min
> Epoch: 15 Step: 1200, train loss: 0.0009 F1: 99.24% dev loss: 0.0701 F1: 67.28% 1.25 min
> Epoch: 16 Step: 1300, train loss: 0.0011 F1: 98.85% dev loss: 0.0691 F1: 67.96% 1.25 min
> Epoch: 18 Step: 1400, train loss: 0.0005 F1: 99.94% dev loss: 0.0685 F1: 66.09% 1.25 min
> Epoch: 19 Step: 1500, train loss: 0.0005 F1: 99.25% dev loss: 0.0752 F1: 67.13% 1.25 min
> Epoch: 20 Step: 1600, train loss: 0.0007 F1: 99.34% dev loss: 0.0700 F1: 65.82% 1.25 min
> Epoch: 22 Step: 1700, train loss: 0.0008 F1: 99.90% dev loss: 0.0740 F1: 65.03% 1.25 min
> Epoch: 23 Step: 1800, train loss: 0.0005 F1: 99.77% dev loss: 0.0809 F1: 65.87% 1.24 min
> Epoch: 24 Step: 1900, train loss: 0.0007 F1: 99.31% dev loss: 0.0750 F1: 64.68% 1.25 min
> Epoch: 25 Step: 2000, train loss: 0.0004 F1: 99.60% dev loss: 0.0700 F1: 68.86% 1.25 min
> Epoch: 27 Step: 2100, train loss: 0.0008 F1: 99.45% dev loss: 0.0770 F1: 69.83% 1.25 min
> Epoch: 28 Step: 2200, train loss: 0.0003 F1: 99.54% dev loss: 0.0738 F1: 68.85% 1.25 min
> Epoch: 29 Step: 2300, train loss: 0.0003 F1: 99.71% dev loss: 0.0742 F1: 68.95% 1.25 min
> Epoch: 31 Step: 2400, train loss: 0.0002 F1: 100.00% dev loss: 0.0752 F1: 68.25% 1.25 min
> Epoch: 32 Step: 2500, train loss: 0.0001 F1: 99.85% dev loss: 0.0746 F1: 70.01% 1.24 min
> Epoch: 33 Step: 2600, train loss: 0.0001 F1: 99.86% dev loss: 0.0740 F1: 69.67% 1.25 min
> Epoch: 35 Step: 2700, train loss: 0.0001 F1: 100.00% dev loss: 0.0760 F1: 69.24% 1.24 min
> Epoch: 36 Step: 2800, train loss: 0.0001 F1: 100.00% dev loss: 0.0764 F1: 69.63% 1.25 min
> Epoch: 37 Step: 2900, train loss: 0.0001 F1: 99.83% dev loss: 0.0765 F1: 69.96% 1.25 min
> Epoch: 38 Step: 3000, train loss: 0.0001 F1: 99.94% dev loss: 0.0761 F1: 70.30% 1.25 min

>>>>>>>>>>>>>>>>>>>>>2021-07-22 05:42:43.236729>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res16_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: res16
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/content/drive/MyDrive/E2E_ABSA/data/Semeval2016/processed/ABSA16_Restaurants_Train_SB1_v2.csv', 'dev': '/content/drive/MyDrive/E2E_ABSA/data/Semeval2016/processed/ABSA16_Restaurants_dev_SB1_v2.csv', 'test': '/content/drive/MyDrive/E2E_ABSA/data/Semeval2016/processed/ABSA16_Restaurants_test_SB1_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 2 Step: 100, train loss: 0.5326 F1: 9.65% dev loss: 0.1641 F1: 9.58% 1.19 min
> Epoch: 4 Step: 200, train loss: 0.1468 F1: 33.71% dev loss: 0.0864 F1: 29.59% 1.22 min
> Epoch: 6 Step: 300, train loss: 0.0615 F1: 56.72% dev loss: 0.0832 F1: 41.01% 1.23 min
> Epoch: 8 Step: 400, train loss: 0.0301 F1: 78.63% dev loss: 0.1112 F1: 39.73% 1.23 min
> Epoch: 10 Step: 500, train loss: 0.0203 F1: 85.19% dev loss: 0.0995 F1: 44.53% 1.23 min
> Epoch: 12 Step: 600, train loss: 0.0133 F1: 91.49% dev loss: 0.0769 F1: 47.44% 1.23 min
> Epoch: 14 Step: 700, train loss: 0.0059 F1: 98.34% dev loss: 0.0809 F1: 54.34% 1.23 min
> Epoch: 16 Step: 800, train loss: 0.0014 F1: 99.46% dev loss: 0.0856 F1: 52.22% 1.23 min
> Epoch: 18 Step: 900, train loss: 0.0009 F1: 99.32% dev loss: 0.0942 F1: 50.14% 1.23 min
> Epoch: 20 Step: 1000, train loss: 0.0005 F1: 99.86% dev loss: 0.0899 F1: 54.35% 1.23 min
> Epoch: 22 Step: 1100, train loss: 0.0004 F1: 99.52% dev loss: 0.0942 F1: 49.74% 1.23 min
> Epoch: 24 Step: 1200, train loss: 0.0004 F1: 99.82% dev loss: 0.0941 F1: 51.61% 1.23 min
> Epoch: 26 Step: 1300, train loss: 0.0005 F1: 99.96% dev loss: 0.0965 F1: 50.50% 1.23 min
> Epoch: 28 Step: 1400, train loss: 0.0003 F1: 99.81% dev loss: 0.0913 F1: 51.16% 1.23 min
> Epoch: 30 Step: 1500, train loss: 0.0008 F1: 99.55% dev loss: 0.0942 F1: 52.99% 1.23 min
> Epoch: 32 Step: 1600, train loss: 0.0002 F1: 100.00% dev loss: 0.1003 F1: 50.72% 1.23 min
> Epoch: 34 Step: 1700, train loss: 0.0003 F1: 99.97% dev loss: 0.0996 F1: 51.94% 1.23 min
> Epoch: 36 Step: 1800, train loss: 0.0001 F1: 99.80% dev loss: 0.1005 F1: 51.89% 1.23 min
> Epoch: 38 Step: 1900, train loss: 0.0001 F1: 99.99% dev loss: 0.1040 F1: 52.60% 1.23 min
> Epoch: 40 Step: 2000, train loss: 0.0002 F1: 99.96% dev loss: 0.1014 F1: 52.85% 1.23 min
> Epoch: 42 Step: 2100, train loss: 0.0002 F1: 100.00% dev loss: 0.1023 F1: 53.07% 1.23 min
> Epoch: 44 Step: 2200, train loss: 0.0001 F1: 99.98% dev loss: 0.1043 F1: 52.40% 1.23 min
> Epoch: 46 Step: 2300, train loss: 0.0001 F1: 100.00% dev loss: 0.1021 F1: 52.86% 1.23 min
> Epoch: 48 Step: 2400, train loss: 0.0000 F1: 100.00% dev loss: 0.1037 F1: 54.00% 1.23 min
> Epoch: 51 Step: 2500, train loss: 0.0001 F1: 70.00% dev loss: 0.1052 F1: 52.88% 1.23 min
> Epoch: 53 Step: 2600, train loss: 0.0000 F1: 80.00% dev loss: 0.1051 F1: 52.95% 1.23 min
> Epoch: 55 Step: 2700, train loss: 0.0000 F1: 80.00% dev loss: 0.1059 F1: 52.95% 1.23 min
> Epoch: 57 Step: 2800, train loss: 0.0001 F1: 80.00% dev loss: 0.1062 F1: 52.55% 1.23 min
> Epoch: 59 Step: 2900, train loss: 0.0000 F1: 100.00% dev loss: 0.1059 F1: 52.32% 1.23 min
> Epoch: 61 Step: 3000, train loss: 0.0000 F1: 99.98% dev loss: 0.1061 F1: 52.32% 1.23 min
>>>>>>>>>>>>>>>>>>>>>2021-07-22 06:45:54.753368>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res14_seed7.pth
>>> seed: 7
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.5075 F1: 9.32% dev loss: 0.2421 F1: 12.28% 0.45 min
> Epoch: 2 Step: 200, train loss: 0.1771 F1: 28.79% dev loss: 0.1361 F1: 32.83% 0.44 min
> Epoch: 3 Step: 300, train loss: 0.0887 F1: 51.77% dev loss: 0.1000 F1: 55.96% 0.45 min
> Epoch: 5 Step: 400, train loss: 0.0492 F1: 73.85% dev loss: 0.1070 F1: 55.99% 0.45 min
> Epoch: 6 Step: 500, train loss: 0.0314 F1: 80.56% dev loss: 0.1244 F1: 58.33% 0.45 min
> Epoch: 7 Step: 600, train loss: 0.0228 F1: 86.77% dev loss: 0.1178 F1: 56.59% 0.45 min
> Epoch: 9 Step: 700, train loss: 0.0127 F1: 93.62% dev loss: 0.1158 F1: 62.65% 0.45 min
> Epoch: 10 Step: 800, train loss: 0.0139 F1: 85.17% dev loss: 0.1258 F1: 61.36% 0.45 min
> Epoch: 11 Step: 900, train loss: 0.0068 F1: 94.83% dev loss: 0.1260 F1: 60.28% 0.45 min
> Epoch: 13 Step: 1000, train loss: 0.0040 F1: 97.05% dev loss: 0.1336 F1: 63.21% 0.45 min
> Epoch: 14 Step: 1100, train loss: 0.0029 F1: 98.25% dev loss: 0.1217 F1: 62.41% 0.45 min
> Epoch: 15 Step: 1200, train loss: 0.0013 F1: 99.37% dev loss: 0.1407 F1: 62.41% 0.45 min
> Epoch: 17 Step: 1300, train loss: 0.0009 F1: 99.84% dev loss: 0.1458 F1: 63.34% 0.45 min
> Epoch: 18 Step: 1400, train loss: 0.0009 F1: 99.61% dev loss: 0.1420 F1: 64.00% 0.45 min
> Epoch: 19 Step: 1500, train loss: 0.0008 F1: 99.64% dev loss: 0.1453 F1: 62.69% 0.45 min
> Epoch: 21 Step: 1600, train loss: 0.0005 F1: 99.65% dev loss: 0.1567 F1: 65.20% 0.45 min
> Epoch: 22 Step: 1700, train loss: 0.0003 F1: 99.94% dev loss: 0.1482 F1: 62.64% 0.45 min
> Epoch: 23 Step: 1800, train loss: 0.0002 F1: 99.91% dev loss: 0.1499 F1: 63.41% 0.45 min
> Epoch: 24 Step: 1900, train loss: 0.0003 F1: 99.94% dev loss: 0.1538 F1: 63.45% 0.45 min
> Epoch: 26 Step: 2000, train loss: 0.0002 F1: 100.00% dev loss: 0.1548 F1: 63.09% 0.45 min
> Epoch: 27 Step: 2100, train loss: 0.0003 F1: 99.95% dev loss: 0.1518 F1: 63.03% 0.45 min
> Epoch: 28 Step: 2200, train loss: 0.0004 F1: 99.87% dev loss: 0.1534 F1: 61.88% 0.45 min
> Epoch: 30 Step: 2300, train loss: 0.0003 F1: 100.00% dev loss: 0.1518 F1: 62.07% 0.45 min
> Epoch: 31 Step: 2400, train loss: 0.0002 F1: 99.96% dev loss: 0.1516 F1: 61.95% 0.45 min
> Epoch: 32 Step: 2500, train loss: 0.0002 F1: 99.89% dev loss: 0.1508 F1: 62.68% 0.45 min
> Epoch: 34 Step: 2600, train loss: 0.0001 F1: 100.00% dev loss: 0.1564 F1: 62.16% 0.45 min
> Epoch: 35 Step: 2700, train loss: 0.0001 F1: 100.00% dev loss: 0.1570 F1: 62.51% 0.45 min
> Epoch: 36 Step: 2800, train loss: 0.0001 F1: 99.92% dev loss: 0.1550 F1: 62.25% 0.45 min
> Epoch: 38 Step: 2900, train loss: 0.0001 F1: 99.98% dev loss: 0.1546 F1: 62.17% 0.45 min
> Epoch: 39 Step: 3000, train loss: 0.0002 F1: 99.97% dev loss: 0.1553 F1: 62.34% 0.45 min

>>>>>>>>>>>>>>>>>>>>>2021-07-22 07:00:08.401731>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_lap14_seed7.pth
>>> seed: 7
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: lap14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.4295 F1: 9.61% dev loss: 0.1819 F1: 9.59% 0.46 min
> Epoch: 2 Step: 200, train loss: 0.1351 F1: 21.35% dev loss: 0.1069 F1: 31.96% 0.45 min
> Epoch: 3 Step: 300, train loss: 0.0715 F1: 51.86% dev loss: 0.0710 F1: 55.72% 0.45 min
> Epoch: 5 Step: 400, train loss: 0.0390 F1: 78.65% dev loss: 0.0745 F1: 56.23% 0.45 min
> Epoch: 6 Step: 500, train loss: 0.0224 F1: 82.34% dev loss: 0.0765 F1: 58.21% 0.45 min
> Epoch: 7 Step: 600, train loss: 0.0155 F1: 86.43% dev loss: 0.0864 F1: 57.87% 0.45 min
> Epoch: 9 Step: 700, train loss: 0.0093 F1: 93.59% dev loss: 0.0773 F1: 59.77% 0.45 min
> Epoch: 10 Step: 800, train loss: 0.0046 F1: 96.47% dev loss: 0.0873 F1: 59.81% 0.45 min
> Epoch: 11 Step: 900, train loss: 0.0026 F1: 97.81% dev loss: 0.0832 F1: 58.16% 0.45 min
> Epoch: 13 Step: 1000, train loss: 0.0017 F1: 99.10% dev loss: 0.0897 F1: 59.53% 0.45 min
> Epoch: 14 Step: 1100, train loss: 0.0014 F1: 98.61% dev loss: 0.0869 F1: 58.77% 0.45 min
> Epoch: 15 Step: 1200, train loss: 0.0007 F1: 99.07% dev loss: 0.0932 F1: 56.35% 0.45 min
> Epoch: 17 Step: 1300, train loss: 0.0007 F1: 99.16% dev loss: 0.0980 F1: 55.07% 0.45 min
> Epoch: 18 Step: 1400, train loss: 0.0006 F1: 99.16% dev loss: 0.1013 F1: 60.57% 0.45 min
> Epoch: 19 Step: 1500, train loss: 0.0007 F1: 99.26% dev loss: 0.0895 F1: 59.60% 0.45 min
> Epoch: 21 Step: 1600, train loss: 0.0003 F1: 98.49% dev loss: 0.0997 F1: 57.61% 0.45 min
> Epoch: 22 Step: 1700, train loss: 0.0004 F1: 99.51% dev loss: 0.0986 F1: 58.82% 0.45 min
> Epoch: 23 Step: 1800, train loss: 0.0003 F1: 99.73% dev loss: 0.0989 F1: 58.45% 0.45 min
> Epoch: 24 Step: 1900, train loss: 0.0003 F1: 99.80% dev loss: 0.1045 F1: 57.21% 0.45 min
> Epoch: 26 Step: 2000, train loss: 0.0003 F1: 99.73% dev loss: 0.1019 F1: 59.08% 0.45 min
> Epoch: 27 Step: 2100, train loss: 0.0003 F1: 99.85% dev loss: 0.1044 F1: 59.51% 0.45 min
> Epoch: 28 Step: 2200, train loss: 0.0003 F1: 99.86% dev loss: 0.1060 F1: 59.26% 0.45 min
> Epoch: 30 Step: 2300, train loss: 0.0002 F1: 99.76% dev loss: 0.1058 F1: 58.75% 0.45 min
> Epoch: 31 Step: 2400, train loss: 0.0002 F1: 99.84% dev loss: 0.1022 F1: 59.29% 0.45 min
> Epoch: 32 Step: 2500, train loss: 0.0002 F1: 99.72% dev loss: 0.1026 F1: 59.23% 0.45 min
> Epoch: 34 Step: 2600, train loss: 0.0001 F1: 100.00% dev loss: 0.1025 F1: 58.73% 0.45 min
> Epoch: 35 Step: 2700, train loss: 0.0002 F1: 100.00% dev loss: 0.1014 F1: 58.24% 0.45 min
> Epoch: 36 Step: 2800, train loss: 0.0001 F1: 99.91% dev loss: 0.1038 F1: 58.23% 0.45 min
> Epoch: 38 Step: 2900, train loss: 0.0001 F1: 100.00% dev loss: 0.1026 F1: 58.94% 0.45 min
> Epoch: 39 Step: 3000, train loss: 0.0001 F1: 99.86% dev loss: 0.1030 F1: 59.05% 0.45 min

>>>>>>>>>>>>>>>>>>>>>2021-07-22 07:14:24.574161>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res16_seed7.pth
>>> seed: 7
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: res16
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_Train_SB1_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_dev_SB1_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_test_SB1_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 2 Step: 100, train loss: 0.4664 F1: 9.53% dev loss: 0.2360 F1: 9.30% 0.45 min
> Epoch: 4 Step: 200, train loss: 0.1480 F1: 34.03% dev loss: 0.1173 F1: 29.85% 0.44 min
> Epoch: 6 Step: 300, train loss: 0.0642 F1: 51.58% dev loss: 0.0993 F1: 40.96% 0.45 min
> Epoch: 8 Step: 400, train loss: 0.0342 F1: 61.85% dev loss: 0.1086 F1: 40.89% 0.45 min
> Epoch: 10 Step: 500, train loss: 0.0208 F1: 93.59% dev loss: 0.1035 F1: 52.04% 0.45 min
> Epoch: 12 Step: 600, train loss: 0.0074 F1: 95.35% dev loss: 0.1018 F1: 59.65% 0.45 min
> Epoch: 14 Step: 700, train loss: 0.0054 F1: 84.65% dev loss: 0.1160 F1: 56.58% 0.45 min
> Epoch: 16 Step: 800, train loss: 0.0023 F1: 98.41% dev loss: 0.1188 F1: 57.37% 0.44 min
> Epoch: 18 Step: 900, train loss: 0.0012 F1: 98.55% dev loss: 0.1293 F1: 59.19% 0.44 min
> Epoch: 20 Step: 1000, train loss: 0.0009 F1: 99.32% dev loss: 0.1300 F1: 59.10% 0.45 min
> Epoch: 22 Step: 1100, train loss: 0.0007 F1: 99.58% dev loss: 0.1298 F1: 59.22% 0.44 min
> Epoch: 24 Step: 1200, train loss: 0.0004 F1: 99.70% dev loss: 0.1364 F1: 59.05% 0.45 min
> Epoch: 26 Step: 1300, train loss: 0.0003 F1: 99.51% dev loss: 0.1483 F1: 57.73% 0.45 min
> Epoch: 28 Step: 1400, train loss: 0.0004 F1: 99.96% dev loss: 0.1284 F1: 57.67% 0.45 min
> Epoch: 30 Step: 1500, train loss: 0.0002 F1: 99.92% dev loss: 0.1345 F1: 57.83% 0.45 min
> Epoch: 32 Step: 1600, train loss: 0.0002 F1: 99.81% dev loss: 0.1425 F1: 58.19% 0.45 min
> Epoch: 34 Step: 1700, train loss: 0.0001 F1: 99.88% dev loss: 0.1335 F1: 59.46% 0.45 min
> Epoch: 36 Step: 1800, train loss: 0.0002 F1: 99.82% dev loss: 0.1379 F1: 58.12% 0.45 min
> Epoch: 38 Step: 1900, train loss: 0.0003 F1: 99.97% dev loss: 0.1399 F1: 58.93% 0.45 min
> Epoch: 40 Step: 2000, train loss: 0.0002 F1: 99.98% dev loss: 0.1335 F1: 57.98% 0.45 min
> Epoch: 42 Step: 2100, train loss: 0.0002 F1: 99.98% dev loss: 0.1304 F1: 60.31% 0.45 min
> Epoch: 44 Step: 2200, train loss: 0.0004 F1: 99.91% dev loss: 0.1290 F1: 59.72% 0.45 min
> Epoch: 46 Step: 2300, train loss: 0.0002 F1: 99.93% dev loss: 0.1438 F1: 59.04% 0.45 min
> Epoch: 48 Step: 2400, train loss: 0.0001 F1: 99.85% dev loss: 0.1471 F1: 58.16% 0.45 min
> Epoch: 51 Step: 2500, train loss: 0.0001 F1: 80.00% dev loss: 0.1497 F1: 57.80% 0.45 min
> Epoch: 53 Step: 2600, train loss: 0.0001 F1: 80.00% dev loss: 0.1451 F1: 58.20% 0.45 min
> Epoch: 55 Step: 2700, train loss: 0.0001 F1: 80.00% dev loss: 0.1425 F1: 57.99% 0.44 min
> Epoch: 57 Step: 2800, train loss: 0.0001 F1: 80.00% dev loss: 0.1426 F1: 57.87% 0.45 min
> Epoch: 59 Step: 2900, train loss: 0.0000 F1: 100.00% dev loss: 0.1447 F1: 57.69% 0.45 min
> Epoch: 61 Step: 3000, train loss: 0.0000 F1: 100.00% dev loss: 0.1449 F1: 57.72% 0.45 min

>>>>>>>>>>>>>>>>>>>>>2021-07-22 07:28:30.988705>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-lstm_res14_seed7.pth
>>> seed: 7
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.7353 F1: 9.32% dev loss: 0.2958 F1: 9.32% 0.49 min
> Epoch: 2 Step: 200, train loss: 0.2733 F1: 11.06% dev loss: 0.2386 F1: 13.96% 0.49 min
> Epoch: 3 Step: 300, train loss: 0.1713 F1: 25.48% dev loss: 0.1510 F1: 30.21% 0.49 min
> Epoch: 5 Step: 400, train loss: 0.1046 F1: 50.75% dev loss: 0.1202 F1: 54.05% 0.50 min
> Epoch: 6 Step: 500, train loss: 0.0606 F1: 71.06% dev loss: 0.0998 F1: 59.83% 0.49 min
> Epoch: 7 Step: 600, train loss: 0.0379 F1: 79.11% dev loss: 0.1293 F1: 56.32% 0.50 min
> Epoch: 9 Step: 700, train loss: 0.0246 F1: 89.91% dev loss: 0.1058 F1: 59.26% 0.50 min
> Epoch: 10 Step: 800, train loss: 0.0148 F1: 92.27% dev loss: 0.1180 F1: 62.16% 0.49 min
> Epoch: 11 Step: 900, train loss: 0.0101 F1: 92.84% dev loss: 0.1166 F1: 59.98% 0.49 min
> Epoch: 13 Step: 1000, train loss: 0.0074 F1: 96.91% dev loss: 0.1282 F1: 63.31% 0.50 min
> Epoch: 14 Step: 1100, train loss: 0.0054 F1: 97.05% dev loss: 0.1383 F1: 65.45% 0.50 min
> Epoch: 15 Step: 1200, train loss: 0.0037 F1: 97.82% dev loss: 0.1276 F1: 64.06% 0.49 min
> Epoch: 17 Step: 1300, train loss: 0.0027 F1: 99.47% dev loss: 0.1386 F1: 65.42% 0.50 min
> Epoch: 18 Step: 1400, train loss: 0.0023 F1: 98.85% dev loss: 0.1293 F1: 67.11% 0.50 min
> Epoch: 19 Step: 1500, train loss: 0.0020 F1: 99.07% dev loss: 0.1368 F1: 66.21% 0.50 min
> Epoch: 21 Step: 1600, train loss: 0.0018 F1: 98.74% dev loss: 0.1347 F1: 67.99% 0.50 min
> Epoch: 22 Step: 1700, train loss: 0.0018 F1: 98.90% dev loss: 0.1226 F1: 64.74% 0.50 min
> Epoch: 23 Step: 1800, train loss: 0.0013 F1: 99.26% dev loss: 0.1349 F1: 66.92% 0.50 min
> Epoch: 24 Step: 1900, train loss: 0.0008 F1: 99.55% dev loss: 0.1402 F1: 67.30% 0.49 min
> Epoch: 26 Step: 2000, train loss: 0.0007 F1: 99.86% dev loss: 0.1354 F1: 66.76% 0.49 min
> Epoch: 27 Step: 2100, train loss: 0.0005 F1: 99.73% dev loss: 0.1341 F1: 64.73% 0.49 min
> Epoch: 28 Step: 2200, train loss: 0.0004 F1: 99.87% dev loss: 0.1402 F1: 66.81% 0.50 min
> Epoch: 30 Step: 2300, train loss: 0.0003 F1: 99.95% dev loss: 0.1422 F1: 67.21% 0.50 min
> Epoch: 31 Step: 2400, train loss: 0.0004 F1: 99.79% dev loss: 0.1373 F1: 64.26% 0.50 min
> Epoch: 32 Step: 2500, train loss: 0.0004 F1: 99.82% dev loss: 0.1397 F1: 65.04% 0.50 min
> Epoch: 34 Step: 2600, train loss: 0.0002 F1: 99.90% dev loss: 0.1411 F1: 67.07% 0.50 min
> Epoch: 35 Step: 2700, train loss: 0.0003 F1: 99.97% dev loss: 0.1433 F1: 66.70% 0.50 min
> Epoch: 36 Step: 2800, train loss: 0.0002 F1: 99.83% dev loss: 0.1408 F1: 65.96% 0.50 min
> Epoch: 38 Step: 2900, train loss: 0.0002 F1: 100.00% dev loss: 0.1409 F1: 65.61% 0.50 min
> Epoch: 39 Step: 3000, train loss: 0.0002 F1: 100.00% dev loss: 0.1409 F1: 66.60% 0.49 min

>>>>>>>>>>>>>>>>>>>>>2021-07-22 07:44:08.476250>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-lstm_lap14_seed7.pth
>>> seed: 7
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: lap14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.6659 F1: 9.61% dev loss: 0.2055 F1: 9.59% 0.50 min
> Epoch: 2 Step: 200, train loss: 0.1798 F1: 9.61% dev loss: 0.1774 F1: 9.59% 0.49 min
> Epoch: 3 Step: 300, train loss: 0.1275 F1: 16.25% dev loss: 0.1148 F1: 23.46% 0.50 min
> Epoch: 5 Step: 400, train loss: 0.0848 F1: 46.97% dev loss: 0.0937 F1: 42.43% 0.49 min
> Epoch: 6 Step: 500, train loss: 0.0503 F1: 66.36% dev loss: 0.0871 F1: 49.64% 0.50 min
> Epoch: 7 Step: 600, train loss: 0.0304 F1: 78.25% dev loss: 0.0806 F1: 59.38% 0.49 min
> Epoch: 9 Step: 700, train loss: 0.0180 F1: 86.28% dev loss: 0.0844 F1: 55.58% 0.50 min
> Epoch: 10 Step: 800, train loss: 0.0115 F1: 93.64% dev loss: 0.0900 F1: 59.42% 0.50 min
> Epoch: 11 Step: 900, train loss: 0.0069 F1: 96.02% dev loss: 0.1025 F1: 61.83% 0.50 min
> Epoch: 13 Step: 1000, train loss: 0.0051 F1: 96.53% dev loss: 0.0849 F1: 60.89% 0.50 min
> Epoch: 14 Step: 1100, train loss: 0.0038 F1: 97.98% dev loss: 0.0864 F1: 61.56% 0.49 min
> Epoch: 15 Step: 1200, train loss: 0.0023 F1: 98.04% dev loss: 0.0936 F1: 62.40% 0.50 min
> Epoch: 17 Step: 1300, train loss: 0.0024 F1: 98.23% dev loss: 0.0974 F1: 61.79% 0.50 min
> Epoch: 18 Step: 1400, train loss: 0.0021 F1: 98.54% dev loss: 0.1027 F1: 61.17% 0.49 min
> Epoch: 19 Step: 1500, train loss: 0.0016 F1: 98.94% dev loss: 0.0944 F1: 65.51% 0.49 min
> Epoch: 21 Step: 1600, train loss: 0.0011 F1: 98.97% dev loss: 0.0999 F1: 62.26% 0.49 min
> Epoch: 22 Step: 1700, train loss: 0.0008 F1: 99.39% dev loss: 0.0966 F1: 64.00% 0.49 min
> Epoch: 23 Step: 1800, train loss: 0.0006 F1: 99.56% dev loss: 0.0994 F1: 63.51% 0.50 min
> Epoch: 24 Step: 1900, train loss: 0.0005 F1: 99.42% dev loss: 0.0967 F1: 64.14% 0.50 min
> Epoch: 26 Step: 2000, train loss: 0.0004 F1: 99.83% dev loss: 0.1013 F1: 63.43% 0.50 min
> Epoch: 27 Step: 2100, train loss: 0.0003 F1: 99.84% dev loss: 0.1005 F1: 63.67% 0.50 min
> Epoch: 28 Step: 2200, train loss: 0.0003 F1: 99.77% dev loss: 0.0998 F1: 63.83% 0.50 min
> Epoch: 30 Step: 2300, train loss: 0.0003 F1: 99.89% dev loss: 0.0988 F1: 64.17% 0.50 min
> Epoch: 31 Step: 2400, train loss: 0.0002 F1: 99.92% dev loss: 0.1043 F1: 63.02% 0.50 min
> Epoch: 32 Step: 2500, train loss: 0.0002 F1: 99.86% dev loss: 0.1059 F1: 62.50% 0.50 min
> Epoch: 34 Step: 2600, train loss: 0.0002 F1: 100.00% dev loss: 0.1058 F1: 63.88% 0.50 min
> Epoch: 35 Step: 2700, train loss: 0.0002 F1: 99.82% dev loss: 0.1040 F1: 63.82% 0.50 min
> Epoch: 36 Step: 2800, train loss: 0.0002 F1: 99.94% dev loss: 0.1030 F1: 63.97% 0.50 min
> Epoch: 38 Step: 2900, train loss: 0.0002 F1: 100.00% dev loss: 0.1044 F1: 64.21% 0.50 min
> Epoch: 39 Step: 3000, train loss: 0.0002 F1: 99.90% dev loss: 0.1043 F1: 64.21% 0.50 min

>>>>>>>>>>>>>>>>>>>>>2021-07-22 07:59:46.696364>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-lstm_res16_seed7.pth
>>> seed: 7
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: res16
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_Train_SB1_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_dev_SB1_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_test_SB1_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 2 Step: 100, train loss: 0.7122 F1: 9.53% dev loss: 0.3003 F1: 9.30% 0.49 min
> Epoch: 4 Step: 200, train loss: 0.2204 F1: 9.49% dev loss: 0.2264 F1: 9.30% 0.49 min
> Epoch: 6 Step: 300, train loss: 0.1528 F1: 26.93% dev loss: 0.1288 F1: 30.21% 0.49 min
> Epoch: 8 Step: 400, train loss: 0.0737 F1: 55.65% dev loss: 0.1014 F1: 44.05% 0.49 min
> Epoch: 10 Step: 500, train loss: 0.0421 F1: 48.12% dev loss: 0.1195 F1: 31.71% 0.49 min
> Epoch: 12 Step: 600, train loss: 0.0248 F1: 65.24% dev loss: 0.1233 F1: 44.05% 0.49 min
> Epoch: 14 Step: 700, train loss: 0.0146 F1: 65.64% dev loss: 0.1363 F1: 44.82% 0.49 min
> Epoch: 16 Step: 800, train loss: 0.0070 F1: 75.57% dev loss: 0.1323 F1: 51.34% 0.49 min
> Epoch: 18 Step: 900, train loss: 0.0040 F1: 97.27% dev loss: 0.1359 F1: 52.60% 0.49 min
> Epoch: 20 Step: 1000, train loss: 0.0028 F1: 94.05% dev loss: 0.1293 F1: 47.42% 0.49 min
> Epoch: 22 Step: 1100, train loss: 0.0022 F1: 99.08% dev loss: 0.1377 F1: 45.89% 0.49 min
> Epoch: 24 Step: 1200, train loss: 0.0016 F1: 98.65% dev loss: 0.1405 F1: 49.34% 0.49 min
> Epoch: 26 Step: 1300, train loss: 0.0011 F1: 99.69% dev loss: 0.1454 F1: 57.06% 0.49 min
> Epoch: 28 Step: 1400, train loss: 0.0010 F1: 98.76% dev loss: 0.1422 F1: 56.14% 0.49 min
> Epoch: 30 Step: 1500, train loss: 0.0008 F1: 99.66% dev loss: 0.1534 F1: 58.22% 0.49 min
> Epoch: 32 Step: 1600, train loss: 0.0007 F1: 99.66% dev loss: 0.1429 F1: 52.52% 0.49 min
> Epoch: 34 Step: 1700, train loss: 0.0005 F1: 99.76% dev loss: 0.1550 F1: 50.56% 0.49 min
> Epoch: 36 Step: 1800, train loss: 0.0006 F1: 99.73% dev loss: 0.1421 F1: 51.63% 0.49 min
> Epoch: 38 Step: 1900, train loss: 0.0004 F1: 99.46% dev loss: 0.1469 F1: 45.66% 0.49 min
> Epoch: 40 Step: 2000, train loss: 0.0003 F1: 99.79% dev loss: 0.1465 F1: 58.88% 0.49 min
> Epoch: 42 Step: 2100, train loss: 0.0003 F1: 99.84% dev loss: 0.1431 F1: 52.30% 0.49 min
> Epoch: 44 Step: 2200, train loss: 0.0003 F1: 99.98% dev loss: 0.1482 F1: 51.63% 0.49 min
> Epoch: 46 Step: 2300, train loss: 0.0002 F1: 99.81% dev loss: 0.1509 F1: 58.44% 0.49 min
> Epoch: 48 Step: 2400, train loss: 0.0002 F1: 100.00% dev loss: 0.1543 F1: 61.14% 0.49 min
> Epoch: 51 Step: 2500, train loss: 0.0002 F1: 80.00% dev loss: 0.1552 F1: 52.55% 0.49 min
> Epoch: 53 Step: 2600, train loss: 0.0001 F1: 80.00% dev loss: 0.1548 F1: 61.05% 0.49 min
> Epoch: 55 Step: 2700, train loss: 0.0001 F1: 80.00% dev loss: 0.1550 F1: 60.46% 0.49 min
> Epoch: 57 Step: 2800, train loss: 0.0002 F1: 80.00% dev loss: 0.1539 F1: 53.24% 0.49 min
> Epoch: 59 Step: 2900, train loss: 0.0001 F1: 100.00% dev loss: 0.1534 F1: 53.06% 0.49 min
> Epoch: 61 Step: 3000, train loss: 0.0001 F1: 100.00% dev loss: 0.1536 F1: 53.01% 0.49 min

>>>>>>>>>>>>>>>>>>>>>2021-07-22 08:15:16.530030>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: san
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-san_res14_seed7.pth
>>> seed: 7
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.3998 F1: 16.04% dev loss: 0.2055 F1: 17.77% 0.48 min
> Epoch: 2 Step: 200, train loss: 0.1540 F1: 36.08% dev loss: 0.1270 F1: 37.67% 0.47 min
> Epoch: 3 Step: 300, train loss: 0.0806 F1: 56.29% dev loss: 0.1006 F1: 54.40% 0.47 min
> Epoch: 5 Step: 400, train loss: 0.0436 F1: 79.76% dev loss: 0.1135 F1: 57.18% 0.47 min
> Epoch: 6 Step: 500, train loss: 0.0295 F1: 83.87% dev loss: 0.1674 F1: 52.18% 0.47 min
> Epoch: 7 Step: 600, train loss: 0.0239 F1: 85.72% dev loss: 0.1338 F1: 52.94% 0.47 min
> Epoch: 9 Step: 700, train loss: 0.0131 F1: 92.42% dev loss: 0.1301 F1: 62.51% 0.47 min
> Epoch: 10 Step: 800, train loss: 0.0078 F1: 95.77% dev loss: 0.1272 F1: 57.14% 0.47 min
> Epoch: 11 Step: 900, train loss: 0.0034 F1: 97.57% dev loss: 0.1424 F1: 61.74% 0.47 min
> Epoch: 13 Step: 1000, train loss: 0.0026 F1: 99.27% dev loss: 0.1390 F1: 63.24% 0.47 min
> Epoch: 14 Step: 1100, train loss: 0.0017 F1: 99.05% dev loss: 0.1429 F1: 62.38% 0.48 min
> Epoch: 15 Step: 1200, train loss: 0.0010 F1: 99.37% dev loss: 0.1567 F1: 60.51% 0.48 min
> Epoch: 17 Step: 1300, train loss: 0.0006 F1: 99.90% dev loss: 0.1571 F1: 64.25% 0.48 min
> Epoch: 18 Step: 1400, train loss: 0.0004 F1: 99.84% dev loss: 0.1562 F1: 61.82% 0.48 min
> Epoch: 19 Step: 1500, train loss: 0.0004 F1: 99.95% dev loss: 0.1554 F1: 62.56% 0.48 min
> Epoch: 21 Step: 1600, train loss: 0.0004 F1: 99.94% dev loss: 0.1648 F1: 64.08% 0.48 min
> Epoch: 22 Step: 1700, train loss: 0.0004 F1: 99.87% dev loss: 0.1612 F1: 63.05% 0.48 min
> Epoch: 23 Step: 1800, train loss: 0.0003 F1: 99.78% dev loss: 0.1609 F1: 61.02% 0.48 min
> Epoch: 24 Step: 1900, train loss: 0.0003 F1: 99.76% dev loss: 0.1654 F1: 62.08% 0.48 min
> Epoch: 26 Step: 2000, train loss: 0.0004 F1: 99.91% dev loss: 0.1702 F1: 64.43% 0.48 min
> Epoch: 27 Step: 2100, train loss: 0.0004 F1: 99.84% dev loss: 0.1588 F1: 61.27% 0.47 min
> Epoch: 28 Step: 2200, train loss: 0.0002 F1: 99.95% dev loss: 0.1602 F1: 61.76% 0.48 min
> Epoch: 30 Step: 2300, train loss: 0.0005 F1: 99.97% dev loss: 0.1604 F1: 61.99% 0.47 min
> Epoch: 31 Step: 2400, train loss: 0.0002 F1: 99.99% dev loss: 0.1643 F1: 63.18% 0.47 min
> Epoch: 32 Step: 2500, train loss: 0.0001 F1: 99.99% dev loss: 0.1642 F1: 62.18% 0.47 min
> Epoch: 34 Step: 2600, train loss: 0.0002 F1: 100.00% dev loss: 0.1661 F1: 62.33% 0.47 min
> Epoch: 35 Step: 2700, train loss: 0.0001 F1: 99.98% dev loss: 0.1658 F1: 62.10% 0.47 min
> Epoch: 36 Step: 2800, train loss: 0.0001 F1: 99.99% dev loss: 0.1674 F1: 62.91% 0.47 min
> Epoch: 38 Step: 2900, train loss: 0.0001 F1: 100.00% dev loss: 0.1667 F1: 63.07% 0.48 min
> Epoch: 39 Step: 3000, train loss: 0.0001 F1: 99.84% dev loss: 0.1665 F1: 62.55% 0.48 min

>>>>>>>>>>>>>>>>>>>>>2021-07-22 08:30:16.028785>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: san
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-san_lap14_seed7.pth
>>> seed: 7
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: lap14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.3134 F1: 13.03% dev loss: 0.1586 F1: 13.34% 0.48 min
> Epoch: 2 Step: 200, train loss: 0.1172 F1: 35.35% dev loss: 0.0933 F1: 43.35% 0.47 min
> Epoch: 3 Step: 300, train loss: 0.0615 F1: 59.24% dev loss: 0.0752 F1: 55.76% 0.48 min
> Epoch: 5 Step: 400, train loss: 0.0345 F1: 81.17% dev loss: 0.0692 F1: 55.48% 0.48 min
> Epoch: 6 Step: 500, train loss: 0.0196 F1: 86.49% dev loss: 0.0788 F1: 56.59% 0.48 min
> Epoch: 7 Step: 600, train loss: 0.0142 F1: 88.97% dev loss: 0.0906 F1: 59.10% 0.47 min
> Epoch: 9 Step: 700, train loss: 0.0075 F1: 93.90% dev loss: 0.0821 F1: 54.94% 0.47 min
> Epoch: 10 Step: 800, train loss: 0.0037 F1: 97.60% dev loss: 0.0977 F1: 58.13% 0.48 min
> Epoch: 11 Step: 900, train loss: 0.0027 F1: 97.73% dev loss: 0.0919 F1: 56.58% 0.47 min
> Epoch: 13 Step: 1000, train loss: 0.0018 F1: 97.98% dev loss: 0.0931 F1: 58.45% 0.47 min
> Epoch: 14 Step: 1100, train loss: 0.0011 F1: 98.87% dev loss: 0.1042 F1: 59.42% 0.47 min
> Epoch: 15 Step: 1200, train loss: 0.0009 F1: 99.27% dev loss: 0.0941 F1: 63.91% 0.47 min
> Epoch: 17 Step: 1300, train loss: 0.0005 F1: 99.73% dev loss: 0.1029 F1: 61.02% 0.47 min
> Epoch: 18 Step: 1400, train loss: 0.0005 F1: 99.39% dev loss: 0.1005 F1: 61.82% 0.47 min
> Epoch: 19 Step: 1500, train loss: 0.0003 F1: 99.69% dev loss: 0.1079 F1: 62.00% 0.48 min
> Epoch: 21 Step: 1600, train loss: 0.0003 F1: 100.00% dev loss: 0.0984 F1: 63.96% 0.48 min
> Epoch: 22 Step: 1700, train loss: 0.0002 F1: 99.78% dev loss: 0.0992 F1: 61.91% 0.48 min
> Epoch: 23 Step: 1800, train loss: 0.0003 F1: 99.59% dev loss: 0.1034 F1: 60.15% 0.48 min
> Epoch: 24 Step: 1900, train loss: 0.0004 F1: 99.59% dev loss: 0.0995 F1: 61.81% 0.47 min
> Epoch: 26 Step: 2000, train loss: 0.0004 F1: 99.59% dev loss: 0.1007 F1: 59.45% 0.47 min
> Epoch: 27 Step: 2100, train loss: 0.0004 F1: 99.75% dev loss: 0.1055 F1: 60.54% 0.47 min
> Epoch: 28 Step: 2200, train loss: 0.0004 F1: 99.90% dev loss: 0.1079 F1: 60.55% 0.48 min
> Epoch: 30 Step: 2300, train loss: 0.0002 F1: 99.76% dev loss: 0.1091 F1: 58.55% 0.48 min
> Epoch: 31 Step: 2400, train loss: 0.0001 F1: 99.94% dev loss: 0.1102 F1: 60.70% 0.48 min
> Epoch: 32 Step: 2500, train loss: 0.0001 F1: 99.93% dev loss: 0.1087 F1: 61.89% 0.48 min
> Epoch: 34 Step: 2600, train loss: 0.0002 F1: 99.97% dev loss: 0.1078 F1: 60.60% 0.48 min
> Epoch: 35 Step: 2700, train loss: 0.0001 F1: 99.99% dev loss: 0.1076 F1: 61.71% 0.48 min
> Epoch: 36 Step: 2800, train loss: 0.0001 F1: 99.89% dev loss: 0.1084 F1: 61.24% 0.48 min
> Epoch: 38 Step: 2900, train loss: 0.0001 F1: 100.00% dev loss: 0.1076 F1: 61.28% 0.48 min
> Epoch: 39 Step: 3000, train loss: 0.0000 F1: 99.97% dev loss: 0.1077 F1: 60.90% 0.48 min

>>>>>>>>>>>>>>>>>>>>>2021-07-22 08:45:17.413072>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: san
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-san_res16_seed7.pth
>>> seed: 7
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: res16
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_Train_SB1_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_dev_SB1_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_test_SB1_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 2 Step: 100, train loss: 0.3541 F1: 14.05% dev loss: 0.1869 F1: 17.48% 0.48 min
> Epoch: 4 Step: 200, train loss: 0.1220 F1: 39.35% dev loss: 0.0979 F1: 31.43% 0.47 min
> Epoch: 6 Step: 300, train loss: 0.0539 F1: 55.71% dev loss: 0.0875 F1: 40.51% 0.47 min
> Epoch: 8 Step: 400, train loss: 0.0255 F1: 78.93% dev loss: 0.1169 F1: 41.17% 0.47 min
> Epoch: 10 Step: 500, train loss: 0.0280 F1: 79.08% dev loss: 0.1590 F1: 52.38% 0.47 min
> Epoch: 12 Step: 600, train loss: 0.0133 F1: 95.30% dev loss: 0.1148 F1: 56.29% 0.47 min
> Epoch: 14 Step: 700, train loss: 0.0050 F1: 96.99% dev loss: 0.1327 F1: 59.74% 0.47 min
> Epoch: 16 Step: 800, train loss: 0.0014 F1: 98.92% dev loss: 0.1257 F1: 60.34% 0.47 min
> Epoch: 18 Step: 900, train loss: 0.0008 F1: 99.65% dev loss: 0.1380 F1: 52.06% 0.47 min
> Epoch: 20 Step: 1000, train loss: 0.0006 F1: 99.11% dev loss: 0.1349 F1: 59.04% 0.47 min
> Epoch: 22 Step: 1100, train loss: 0.0005 F1: 99.60% dev loss: 0.1432 F1: 45.40% 0.47 min
> Epoch: 24 Step: 1200, train loss: 0.0003 F1: 99.96% dev loss: 0.1442 F1: 50.53% 0.47 min
> Epoch: 26 Step: 1300, train loss: 0.0003 F1: 99.78% dev loss: 0.1400 F1: 57.19% 0.47 min
> Epoch: 28 Step: 1400, train loss: 0.0003 F1: 99.95% dev loss: 0.1431 F1: 56.96% 0.47 min
> Epoch: 30 Step: 1500, train loss: 0.0003 F1: 99.74% dev loss: 0.1438 F1: 57.50% 0.47 min
> Epoch: 32 Step: 1600, train loss: 0.0001 F1: 99.98% dev loss: 0.1454 F1: 57.81% 0.47 min
> Epoch: 34 Step: 1700, train loss: 0.0001 F1: 99.98% dev loss: 0.1493 F1: 59.95% 0.47 min
> Epoch: 36 Step: 1800, train loss: 0.0001 F1: 99.99% dev loss: 0.1490 F1: 59.41% 0.47 min
> Epoch: 38 Step: 1900, train loss: 0.0001 F1: 99.90% dev loss: 0.1490 F1: 59.72% 0.47 min
> Epoch: 40 Step: 2000, train loss: 0.0001 F1: 100.00% dev loss: 0.1533 F1: 60.33% 0.47 min
> Epoch: 42 Step: 2100, train loss: 0.0001 F1: 99.98% dev loss: 0.1517 F1: 59.06% 0.47 min
> Epoch: 44 Step: 2200, train loss: 0.0002 F1: 99.85% dev loss: 0.1528 F1: 57.53% 0.47 min
> Epoch: 46 Step: 2300, train loss: 0.0004 F1: 99.98% dev loss: 0.1433 F1: 59.93% 0.47 min
> Epoch: 48 Step: 2400, train loss: 0.0001 F1: 99.98% dev loss: 0.1433 F1: 59.65% 0.47 min
> Epoch: 51 Step: 2500, train loss: 0.0001 F1: 80.00% dev loss: 0.1461 F1: 59.67% 0.47 min
> Epoch: 53 Step: 2600, train loss: 0.0001 F1: 80.00% dev loss: 0.1472 F1: 59.47% 0.47 min
> Epoch: 55 Step: 2700, train loss: 0.0001 F1: 80.00% dev loss: 0.1476 F1: 59.49% 0.47 min
> Epoch: 57 Step: 2800, train loss: 0.0001 F1: 80.00% dev loss: 0.1497 F1: 59.60% 0.47 min
> Epoch: 59 Step: 2900, train loss: 0.0000 F1: 100.00% dev loss: 0.1503 F1: 59.60% 0.47 min
> Epoch: 61 Step: 3000, train loss: 0.0001 F1: 100.00% dev loss: 0.1503 F1: 59.60% 0.47 min

>>>>>>>>>>>>>>>>>>>>>2021-07-22 09:00:09.852800>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: crf
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-crf_res14_seed7.pth
>>> seed: 7
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 17.2419 F1: 9.32% dev loss: 7.4009 F1: 11.78% 0.60 min
> Epoch: 2 Step: 200, train loss: 5.6304 F1: 28.91% dev loss: 4.5626 F1: 32.68% 0.60 min
> Epoch: 3 Step: 300, train loss: 3.3290 F1: 48.63% dev loss: 3.7882 F1: 50.86% 0.60 min
> Epoch: 5 Step: 400, train loss: 2.2258 F1: 68.62% dev loss: 3.5952 F1: 58.41% 0.60 min
> Epoch: 6 Step: 500, train loss: 1.5838 F1: 75.67% dev loss: 4.3936 F1: 56.27% 0.60 min
> Epoch: 7 Step: 600, train loss: 1.1521 F1: 83.66% dev loss: 5.0553 F1: 54.15% 0.60 min
> Epoch: 9 Step: 700, train loss: 0.7579 F1: 90.21% dev loss: 5.2270 F1: 54.57% 0.60 min
> Epoch: 10 Step: 800, train loss: 0.6992 F1: 94.25% dev loss: 4.9893 F1: 58.00% 0.59 min
> Epoch: 11 Step: 900, train loss: 0.4776 F1: 93.89% dev loss: 4.9655 F1: 59.22% 0.60 min
> Epoch: 13 Step: 1000, train loss: 0.3276 F1: 95.01% dev loss: 6.0673 F1: 58.88% 0.60 min
> Epoch: 14 Step: 1100, train loss: 0.2311 F1: 98.22% dev loss: 5.9579 F1: 60.63% 0.60 min
> Epoch: 15 Step: 1200, train loss: 0.1400 F1: 98.87% dev loss: 7.0520 F1: 59.02% 0.60 min
> Epoch: 17 Step: 1300, train loss: 0.1259 F1: 98.52% dev loss: 6.7279 F1: 59.68% 0.59 min
> Epoch: 18 Step: 1400, train loss: 0.0883 F1: 99.34% dev loss: 6.6196 F1: 62.88% 0.59 min
> Epoch: 19 Step: 1500, train loss: 0.0711 F1: 99.27% dev loss: 7.4883 F1: 58.12% 0.59 min
> Epoch: 21 Step: 1600, train loss: 0.0490 F1: 99.77% dev loss: 7.0591 F1: 60.87% 0.59 min
> Epoch: 22 Step: 1700, train loss: 0.0440 F1: 99.86% dev loss: 7.2646 F1: 60.12% 0.59 min
> Epoch: 23 Step: 1800, train loss: 0.0120 F1: 99.95% dev loss: 7.2846 F1: 60.35% 0.59 min
> Epoch: 24 Step: 1900, train loss: 0.0226 F1: 99.89% dev loss: 7.6607 F1: 61.16% 0.60 min
> Epoch: 26 Step: 2000, train loss: 0.0161 F1: 99.81% dev loss: 7.5476 F1: 61.60% 0.59 min
> Epoch: 27 Step: 2100, train loss: 0.0167 F1: 99.94% dev loss: 7.8164 F1: 59.11% 0.61 min
> Epoch: 28 Step: 2200, train loss: 0.0169 F1: 99.91% dev loss: 8.2908 F1: 60.93% 0.60 min
> Epoch: 30 Step: 2300, train loss: 0.0068 F1: 99.88% dev loss: 8.0003 F1: 61.44% 0.59 min
> Epoch: 31 Step: 2400, train loss: 0.0134 F1: 99.94% dev loss: 7.8800 F1: 61.19% 0.60 min
> Epoch: 32 Step: 2500, train loss: 0.0064 F1: 99.99% dev loss: 7.7883 F1: 60.65% 0.61 min
> Epoch: 34 Step: 2600, train loss: 0.0012 F1: 100.00% dev loss: 7.9065 F1: 60.84% 0.59 min
> Epoch: 35 Step: 2700, train loss: 0.0080 F1: 99.99% dev loss: 7.8007 F1: 61.98% 0.59 min
> Epoch: 36 Step: 2800, train loss: 0.0045 F1: 100.00% dev loss: 7.9416 F1: 61.72% 0.59 min
> Epoch: 38 Step: 2900, train loss: 0.0014 F1: 100.00% dev loss: 7.9331 F1: 61.86% 0.59 min
> Epoch: 39 Step: 3000, train loss: 0.0031 F1: 99.99% dev loss: 7.9561 F1: 61.06% 0.59 min

>>>>>>>>>>>>>>>>>>>>>2021-07-22 09:18:51.001692>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: crf
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-crf_lap14_seed7.pth
>>> seed: 7
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: lap14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 15.5011 F1: 9.61% dev loss: 6.5172 F1: 9.59% 0.60 min
> Epoch: 2 Step: 200, train loss: 4.3564 F1: 22.63% dev loss: 4.2338 F1: 26.39% 0.60 min
> Epoch: 3 Step: 300, train loss: 2.7281 F1: 45.44% dev loss: 3.1442 F1: 50.25% 0.60 min
> Epoch: 5 Step: 400, train loss: 1.8309 F1: 66.60% dev loss: 3.0886 F1: 61.74% 0.60 min
> Epoch: 6 Step: 500, train loss: 1.2454 F1: 74.75% dev loss: 3.2981 F1: 59.30% 0.60 min
> Epoch: 7 Step: 600, train loss: 0.8065 F1: 85.56% dev loss: 4.2763 F1: 54.93% 0.60 min
> Epoch: 9 Step: 700, train loss: 0.5890 F1: 93.92% dev loss: 4.1596 F1: 54.50% 0.60 min
> Epoch: 10 Step: 800, train loss: 0.4125 F1: 95.23% dev loss: 4.2634 F1: 62.55% 0.60 min
> Epoch: 11 Step: 900, train loss: 0.2697 F1: 95.70% dev loss: 4.7692 F1: 60.64% 0.60 min
> Epoch: 13 Step: 1000, train loss: 0.2384 F1: 96.66% dev loss: 5.0754 F1: 60.09% 0.60 min
> Epoch: 14 Step: 1100, train loss: 0.1830 F1: 96.73% dev loss: 5.3371 F1: 59.63% 0.60 min
> Epoch: 15 Step: 1200, train loss: 0.1527 F1: 97.19% dev loss: 5.2930 F1: 60.62% 0.60 min
> Epoch: 17 Step: 1300, train loss: 0.1201 F1: 99.31% dev loss: 5.3706 F1: 59.34% 0.60 min
> Epoch: 18 Step: 1400, train loss: 0.0651 F1: 99.14% dev loss: 5.6152 F1: 61.20% 0.60 min
> Epoch: 19 Step: 1500, train loss: 0.0502 F1: 99.07% dev loss: 5.9181 F1: 61.15% 0.60 min
> Epoch: 21 Step: 1600, train loss: 0.0336 F1: 100.00% dev loss: 6.2406 F1: 58.37% 0.60 min
> Epoch: 22 Step: 1700, train loss: 0.0394 F1: 99.63% dev loss: 6.5931 F1: 61.30% 0.60 min
> Epoch: 23 Step: 1800, train loss: 0.0317 F1: 99.54% dev loss: 6.4288 F1: 58.17% 0.59 min
> Epoch: 24 Step: 1900, train loss: 0.0138 F1: 99.77% dev loss: 6.5205 F1: 59.63% 0.59 min
> Epoch: 26 Step: 2000, train loss: 0.0094 F1: 99.95% dev loss: 6.6468 F1: 60.85% 0.59 min
> Epoch: 27 Step: 2100, train loss: 0.0219 F1: 99.71% dev loss: 6.4120 F1: 60.16% 0.59 min
> Epoch: 28 Step: 2200, train loss: 0.0135 F1: 99.85% dev loss: 6.4837 F1: 61.83% 0.59 min
> Epoch: 30 Step: 2300, train loss: 0.0078 F1: 100.00% dev loss: 6.5950 F1: 62.60% 0.59 min
> Epoch: 31 Step: 2400, train loss: 0.0037 F1: 100.00% dev loss: 6.8066 F1: 61.69% 0.59 min
> Epoch: 32 Step: 2500, train loss: 0.0056 F1: 99.88% dev loss: 7.0631 F1: 60.74% 0.59 min
> Epoch: 34 Step: 2600, train loss: 0.0043 F1: 99.87% dev loss: 6.9057 F1: 62.04% 0.59 min
> Epoch: 35 Step: 2700, train loss: 0.0019 F1: 99.99% dev loss: 7.0578 F1: 61.73% 0.59 min
> Epoch: 36 Step: 2800, train loss: 0.0033 F1: 99.97% dev loss: 7.1166 F1: 61.92% 0.59 min
> Epoch: 38 Step: 2900, train loss: 0.0014 F1: 100.00% dev loss: 6.9570 F1: 62.33% 0.59 min
> Epoch: 39 Step: 3000, train loss: 0.0013 F1: 100.00% dev loss: 6.9454 F1: 62.26% 0.59 min

>>>>>>>>>>>>>>>>>>>>>2021-07-22 09:37:29.588384>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: crf
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-crf_res16_seed7.pth
>>> seed: 7
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: res16
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_Train_SB1_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_dev_SB1_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_test_SB1_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 2 Step: 100, train loss: 15.0383 F1: 9.53% dev loss: 7.6757 F1: 9.30% 0.58 min
> Epoch: 4 Step: 200, train loss: 4.5479 F1: 38.81% dev loss: 4.3800 F1: 30.23% 0.58 min
> Epoch: 6 Step: 300, train loss: 2.2715 F1: 48.80% dev loss: 4.1211 F1: 34.70% 0.58 min
> Epoch: 8 Step: 400, train loss: 1.5190 F1: 49.92% dev loss: 4.8459 F1: 39.61% 0.58 min
> Epoch: 10 Step: 500, train loss: 0.9015 F1: 67.70% dev loss: 5.2233 F1: 42.55% 0.59 min
> Epoch: 12 Step: 600, train loss: 0.4541 F1: 91.15% dev loss: 5.0832 F1: 52.91% 0.59 min
> Epoch: 14 Step: 700, train loss: 0.4287 F1: 94.57% dev loss: 5.0976 F1: 61.45% 0.59 min
> Epoch: 16 Step: 800, train loss: 0.2234 F1: 96.96% dev loss: 5.9181 F1: 59.31% 0.58 min
> Epoch: 18 Step: 900, train loss: 0.1317 F1: 98.47% dev loss: 6.8417 F1: 62.29% 0.59 min
> Epoch: 20 Step: 1000, train loss: 0.0945 F1: 98.15% dev loss: 6.9180 F1: 59.55% 0.59 min
> Epoch: 22 Step: 1100, train loss: 0.0581 F1: 99.01% dev loss: 6.8976 F1: 60.32% 0.59 min
> Epoch: 24 Step: 1200, train loss: 0.0430 F1: 99.09% dev loss: 7.1548 F1: 60.83% 0.59 min
> Epoch: 26 Step: 1300, train loss: 0.0309 F1: 99.27% dev loss: 7.5638 F1: 56.53% 0.59 min
> Epoch: 28 Step: 1400, train loss: 0.0293 F1: 99.33% dev loss: 7.6369 F1: 57.85% 0.59 min
> Epoch: 30 Step: 1500, train loss: 0.0165 F1: 99.49% dev loss: 8.0196 F1: 59.71% 0.59 min
> Epoch: 32 Step: 1600, train loss: 0.0141 F1: 99.93% dev loss: 8.2585 F1: 58.83% 0.59 min
> Epoch: 34 Step: 1700, train loss: 0.0072 F1: 99.99% dev loss: 7.7724 F1: 59.35% 0.59 min
> Epoch: 36 Step: 1800, train loss: 0.0165 F1: 99.98% dev loss: 7.3674 F1: 58.87% 0.59 min
> Epoch: 38 Step: 1900, train loss: 0.0130 F1: 99.89% dev loss: 7.4545 F1: 61.50% 0.59 min
> Epoch: 40 Step: 2000, train loss: 0.0129 F1: 99.94% dev loss: 7.9540 F1: 61.57% 0.59 min
> Epoch: 42 Step: 2100, train loss: 0.0037 F1: 99.97% dev loss: 7.8601 F1: 63.29% 0.59 min
> Epoch: 44 Step: 2200, train loss: 0.0012 F1: 99.99% dev loss: 8.1298 F1: 61.98% 0.59 min
> Epoch: 46 Step: 2300, train loss: 0.0016 F1: 99.98% dev loss: 8.3656 F1: 59.60% 0.59 min
> Epoch: 48 Step: 2400, train loss: 0.0049 F1: 99.98% dev loss: 8.3223 F1: 61.09% 0.59 min
> Epoch: 51 Step: 2500, train loss: 0.0008 F1: 80.00% dev loss: 8.2776 F1: 61.07% 0.59 min
> Epoch: 53 Step: 2600, train loss: 0.0023 F1: 80.00% dev loss: 8.4663 F1: 60.95% 0.59 min
> Epoch: 55 Step: 2700, train loss: 0.0007 F1: 80.00% dev loss: 8.5158 F1: 60.95% 0.60 min
> Epoch: 57 Step: 2800, train loss: 0.0009 F1: 80.00% dev loss: 8.4533 F1: 61.09% 0.59 min
> Epoch: 59 Step: 2900, train loss: 0.0043 F1: 99.95% dev loss: 8.3028 F1: 61.04% 0.59 min
> Epoch: 61 Step: 3000, train loss: 0.0008 F1: 100.00% dev loss: 8.3264 F1: 61.24% 0.59 min

>>>>>>>>>>>>>>>>>>>>>2021-07-22 11:28:44.626347>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res14_seed8.pth
>>> seed: 8
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.5489 F1: 11.88% dev loss: 0.2155 F1: 10.79% 0.45 min
> Epoch: 2 Step: 200, train loss: 0.1721 F1: 27.40% dev loss: 0.1254 F1: 34.23% 0.44 min
> Epoch: 3 Step: 300, train loss: 0.0927 F1: 47.93% dev loss: 0.1017 F1: 54.63% 0.45 min
> Epoch: 5 Step: 400, train loss: 0.0529 F1: 68.35% dev loss: 0.1170 F1: 55.09% 0.45 min
> Epoch: 6 Step: 500, train loss: 0.0345 F1: 78.51% dev loss: 0.1062 F1: 61.82% 0.45 min
> Epoch: 7 Step: 600, train loss: 0.0214 F1: 85.80% dev loss: 0.1052 F1: 57.04% 0.45 min
> Epoch: 9 Step: 700, train loss: 0.0143 F1: 92.24% dev loss: 0.1127 F1: 58.93% 0.45 min
> Epoch: 10 Step: 800, train loss: 0.0084 F1: 94.83% dev loss: 0.1275 F1: 56.65% 0.45 min
> Epoch: 11 Step: 900, train loss: 0.0051 F1: 96.82% dev loss: 0.1336 F1: 56.31% 0.45 min
> Epoch: 13 Step: 1000, train loss: 0.0032 F1: 98.80% dev loss: 0.1342 F1: 58.34% 0.45 min
> Epoch: 14 Step: 1100, train loss: 0.0022 F1: 97.87% dev loss: 0.1322 F1: 55.92% 0.45 min
> Epoch: 15 Step: 1200, train loss: 0.0018 F1: 98.53% dev loss: 0.1438 F1: 57.66% 0.45 min
> Epoch: 17 Step: 1300, train loss: 0.0013 F1: 98.93% dev loss: 0.1424 F1: 54.27% 0.45 min
> Epoch: 18 Step: 1400, train loss: 0.0008 F1: 99.49% dev loss: 0.1578 F1: 57.55% 0.45 min
> Epoch: 19 Step: 1500, train loss: 0.0005 F1: 99.66% dev loss: 0.1526 F1: 58.02% 0.45 min
> Epoch: 21 Step: 1600, train loss: 0.0004 F1: 99.90% dev loss: 0.1562 F1: 57.56% 0.45 min
> Epoch: 22 Step: 1700, train loss: 0.0003 F1: 99.96% dev loss: 0.1594 F1: 57.51% 0.45 min
> Epoch: 23 Step: 1800, train loss: 0.0003 F1: 99.87% dev loss: 0.1593 F1: 57.98% 0.45 min
> Epoch: 25 Step: 1900, train loss: 0.0004 F1: 99.97% dev loss: 0.1572 F1: 56.60% 0.45 min
> Epoch: 26 Step: 2000, train loss: 0.0003 F1: 99.74% dev loss: 0.1614 F1: 57.85% 0.45 min
> Epoch: 27 Step: 2100, train loss: 0.0002 F1: 99.93% dev loss: 0.1595 F1: 58.35% 0.45 min
> Epoch: 29 Step: 2200, train loss: 0.0002 F1: 99.97% dev loss: 0.1639 F1: 58.93% 0.45 min
> Epoch: 30 Step: 2300, train loss: 0.0002 F1: 99.93% dev loss: 0.1631 F1: 58.53% 0.45 min
> Epoch: 31 Step: 2400, train loss: 0.0001 F1: 99.96% dev loss: 0.1644 F1: 57.93% 0.45 min
> Epoch: 33 Step: 2500, train loss: 0.0002 F1: 99.81% dev loss: 0.1690 F1: 58.14% 0.45 min
> Epoch: 34 Step: 2600, train loss: 0.0001 F1: 99.99% dev loss: 0.1690 F1: 56.80% 0.45 min
> Epoch: 35 Step: 2700, train loss: 0.0001 F1: 99.91% dev loss: 0.1689 F1: 57.04% 0.45 min
> Epoch: 37 Step: 2800, train loss: 0.0001 F1: 99.97% dev loss: 0.1682 F1: 56.59% 0.45 min
> Epoch: 38 Step: 2900, train loss: 0.0001 F1: 99.87% dev loss: 0.1677 F1: 56.85% 0.45 min
> Epoch: 39 Step: 3000, train loss: 0.0001 F1: 99.98% dev loss: 0.1678 F1: 56.90% 0.45 min

>>>>>>>>>>>>>>>>>>>>>2021-07-22 11:43:04.375901>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_lap14_seed8.pth
>>> seed: 8
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: lap14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.4778 F1: 9.57% dev loss: 0.1780 F1: 9.57% 0.46 min
> Epoch: 2 Step: 200, train loss: 0.1303 F1: 26.97% dev loss: 0.1107 F1: 29.12% 0.45 min
> Epoch: 3 Step: 300, train loss: 0.0708 F1: 53.39% dev loss: 0.0827 F1: 48.56% 0.45 min
> Epoch: 5 Step: 400, train loss: 0.0405 F1: 75.15% dev loss: 0.1012 F1: 48.13% 0.45 min
> Epoch: 6 Step: 500, train loss: 0.0260 F1: 85.50% dev loss: 0.0830 F1: 60.35% 0.45 min
> Epoch: 7 Step: 600, train loss: 0.0141 F1: 86.42% dev loss: 0.0939 F1: 58.39% 0.45 min
> Epoch: 9 Step: 700, train loss: 0.0101 F1: 89.34% dev loss: 0.1060 F1: 55.73% 0.45 min
> Epoch: 10 Step: 800, train loss: 0.0061 F1: 95.18% dev loss: 0.1037 F1: 56.69% 0.45 min
> Epoch: 11 Step: 900, train loss: 0.0027 F1: 97.80% dev loss: 0.0999 F1: 58.42% 0.45 min
> Epoch: 13 Step: 1000, train loss: 0.0015 F1: 98.98% dev loss: 0.0985 F1: 60.13% 0.45 min
> Epoch: 14 Step: 1100, train loss: 0.0017 F1: 98.79% dev loss: 0.0976 F1: 59.53% 0.45 min
> Epoch: 15 Step: 1200, train loss: 0.0011 F1: 99.04% dev loss: 0.1026 F1: 59.99% 0.45 min
> Epoch: 17 Step: 1300, train loss: 0.0007 F1: 99.41% dev loss: 0.1044 F1: 61.07% 0.45 min
> Epoch: 18 Step: 1400, train loss: 0.0007 F1: 99.43% dev loss: 0.1063 F1: 62.32% 0.45 min
> Epoch: 19 Step: 1500, train loss: 0.0005 F1: 99.58% dev loss: 0.1033 F1: 61.31% 0.45 min
> Epoch: 21 Step: 1600, train loss: 0.0011 F1: 99.53% dev loss: 0.1049 F1: 60.16% 0.45 min
> Epoch: 22 Step: 1700, train loss: 0.0004 F1: 99.75% dev loss: 0.1091 F1: 57.28% 0.45 min
> Epoch: 23 Step: 1800, train loss: 0.0004 F1: 99.56% dev loss: 0.1089 F1: 61.60% 0.45 min
> Epoch: 25 Step: 1900, train loss: 0.0004 F1: 99.65% dev loss: 0.1088 F1: 59.83% 0.45 min
> Epoch: 26 Step: 2000, train loss: 0.0002 F1: 99.77% dev loss: 0.1081 F1: 60.59% 0.45 min
> Epoch: 27 Step: 2100, train loss: 0.0003 F1: 99.89% dev loss: 0.1066 F1: 60.53% 0.45 min
> Epoch: 29 Step: 2200, train loss: 0.0001 F1: 100.00% dev loss: 0.1093 F1: 62.15% 0.45 min
> Epoch: 30 Step: 2300, train loss: 0.0001 F1: 99.98% dev loss: 0.1095 F1: 61.95% 0.45 min
> Epoch: 31 Step: 2400, train loss: 0.0001 F1: 99.91% dev loss: 0.1109 F1: 61.21% 0.45 min
> Epoch: 33 Step: 2500, train loss: 0.0002 F1: 99.94% dev loss: 0.1119 F1: 60.34% 0.45 min
> Epoch: 34 Step: 2600, train loss: 0.0001 F1: 99.91% dev loss: 0.1131 F1: 61.69% 0.45 min
> Epoch: 35 Step: 2700, train loss: 0.0001 F1: 99.89% dev loss: 0.1140 F1: 61.32% 0.45 min
> Epoch: 37 Step: 2800, train loss: 0.0001 F1: 100.00% dev loss: 0.1142 F1: 61.41% 0.45 min
> Epoch: 38 Step: 2900, train loss: 0.0001 F1: 100.00% dev loss: 0.1144 F1: 61.24% 0.45 min
> Epoch: 39 Step: 3000, train loss: 0.0000 F1: 99.99% dev loss: 0.1144 F1: 61.23% 0.45 min

>>>>>>>>>>>>>>>>>>>>>2021-07-22 11:57:30.273544>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: linear
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-linear_res16_seed8.pth
>>> seed: 8
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: res16
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_Train_SB1_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_dev_SB1_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_test_SB1_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.5263 F1: 9.45% dev loss: 0.1649 F1: 9.51% 0.45 min
> Epoch: 3 Step: 200, train loss: 0.1474 F1: 25.47% dev loss: 0.0899 F1: 35.87% 0.45 min
> Epoch: 5 Step: 300, train loss: 0.0678 F1: 48.42% dev loss: 0.0754 F1: 42.49% 0.45 min
> Epoch: 7 Step: 400, train loss: 0.0368 F1: 59.71% dev loss: 0.1088 F1: 35.97% 0.44 min
> Epoch: 9 Step: 500, train loss: 0.0216 F1: 82.84% dev loss: 0.0806 F1: 37.37% 0.44 min
> Epoch: 11 Step: 600, train loss: 0.0085 F1: 89.73% dev loss: 0.1088 F1: 38.43% 0.45 min
> Epoch: 13 Step: 700, train loss: 0.0055 F1: 93.02% dev loss: 0.1045 F1: 36.26% 0.45 min
> Epoch: 15 Step: 800, train loss: 0.0033 F1: 96.54% dev loss: 0.1073 F1: 49.45% 0.44 min
> Epoch: 17 Step: 900, train loss: 0.0015 F1: 98.53% dev loss: 0.1024 F1: 43.98% 0.44 min
> Epoch: 19 Step: 1000, train loss: 0.0007 F1: 99.69% dev loss: 0.1090 F1: 47.37% 0.45 min
> Epoch: 21 Step: 1100, train loss: 0.0004 F1: 99.58% dev loss: 0.1154 F1: 43.94% 0.45 min
> Epoch: 23 Step: 1200, train loss: 0.0004 F1: 99.86% dev loss: 0.1145 F1: 44.91% 0.44 min
> Epoch: 25 Step: 1300, train loss: 0.0002 F1: 99.96% dev loss: 0.1164 F1: 44.27% 0.45 min
> Epoch: 27 Step: 1400, train loss: 0.0003 F1: 99.84% dev loss: 0.1233 F1: 49.59% 0.45 min
> Epoch: 29 Step: 1500, train loss: 0.0002 F1: 99.84% dev loss: 0.1256 F1: 47.46% 0.45 min
> Epoch: 31 Step: 1600, train loss: 0.0002 F1: 99.96% dev loss: 0.1229 F1: 44.65% 0.45 min
> Epoch: 33 Step: 1700, train loss: 0.0002 F1: 99.82% dev loss: 0.1315 F1: 45.25% 0.45 min
> Epoch: 35 Step: 1800, train loss: 0.0001 F1: 99.95% dev loss: 0.1248 F1: 43.47% 0.44 min
> Epoch: 37 Step: 1900, train loss: 0.0001 F1: 99.98% dev loss: 0.1299 F1: 43.83% 0.44 min
> Epoch: 39 Step: 2000, train loss: 0.0001 F1: 99.97% dev loss: 0.1274 F1: 43.71% 0.44 min
> Epoch: 41 Step: 2100, train loss: 0.0001 F1: 100.00% dev loss: 0.1225 F1: 43.72% 0.44 min
> Epoch: 43 Step: 2200, train loss: 0.0001 F1: 99.95% dev loss: 0.1313 F1: 45.07% 0.45 min
> Epoch: 45 Step: 2300, train loss: 0.0001 F1: 99.98% dev loss: 0.1266 F1: 44.01% 0.44 min
> Epoch: 47 Step: 2400, train loss: 0.0001 F1: 99.89% dev loss: 0.1267 F1: 44.10% 0.44 min
> Epoch: 49 Step: 2500, train loss: 0.0000 F1: 99.99% dev loss: 0.1292 F1: 43.80% 0.45 min
> Epoch: 51 Step: 2600, train loss: 0.0001 F1: 99.95% dev loss: 0.1285 F1: 43.51% 0.44 min
> Epoch: 53 Step: 2700, train loss: 0.0002 F1: 99.76% dev loss: 0.1286 F1: 43.81% 0.44 min
> Epoch: 55 Step: 2800, train loss: 0.0001 F1: 99.99% dev loss: 0.1282 F1: 44.08% 0.44 min
> Epoch: 57 Step: 2900, train loss: 0.0000 F1: 100.00% dev loss: 0.1282 F1: 44.13% 0.45 min
> Epoch: 59 Step: 3000, train loss: 0.0001 F1: 100.00% dev loss: 0.1281 F1: 44.19% 0.45 min

>>>>>>>>>>>>>>>>>>>>>2021-07-22 12:11:36.369327>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-lstm_res14_seed8.pth
>>> seed: 8
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.7377 F1: 9.34% dev loss: 0.2830 F1: 9.35% 0.49 min
> Epoch: 2 Step: 200, train loss: 0.2689 F1: 9.33% dev loss: 0.2259 F1: 12.16% 0.49 min
> Epoch: 3 Step: 300, train loss: 0.1755 F1: 24.06% dev loss: 0.1571 F1: 24.63% 0.49 min
> Epoch: 5 Step: 400, train loss: 0.1118 F1: 41.37% dev loss: 0.1260 F1: 44.05% 0.49 min
> Epoch: 6 Step: 500, train loss: 0.0702 F1: 64.63% dev loss: 0.1363 F1: 54.03% 0.49 min
> Epoch: 7 Step: 600, train loss: 0.0453 F1: 73.87% dev loss: 0.1095 F1: 51.02% 0.50 min
> Epoch: 9 Step: 700, train loss: 0.0283 F1: 87.52% dev loss: 0.1126 F1: 58.54% 0.49 min
> Epoch: 10 Step: 800, train loss: 0.0163 F1: 91.18% dev loss: 0.1292 F1: 56.48% 0.50 min
> Epoch: 11 Step: 900, train loss: 0.0115 F1: 92.96% dev loss: 0.1443 F1: 55.16% 0.50 min
> Epoch: 13 Step: 1000, train loss: 0.0084 F1: 94.27% dev loss: 0.1294 F1: 54.79% 0.50 min
> Epoch: 14 Step: 1100, train loss: 0.0063 F1: 96.29% dev loss: 0.1395 F1: 56.25% 0.50 min
> Epoch: 15 Step: 1200, train loss: 0.0052 F1: 96.05% dev loss: 0.1320 F1: 58.32% 0.50 min
> Epoch: 17 Step: 1300, train loss: 0.0038 F1: 97.65% dev loss: 0.1420 F1: 57.67% 0.50 min
> Epoch: 18 Step: 1400, train loss: 0.0027 F1: 98.31% dev loss: 0.1403 F1: 57.82% 0.50 min
> Epoch: 19 Step: 1500, train loss: 0.0023 F1: 98.45% dev loss: 0.1502 F1: 56.60% 0.50 min
> Epoch: 21 Step: 1600, train loss: 0.0019 F1: 98.18% dev loss: 0.1421 F1: 61.54% 0.50 min
> Epoch: 22 Step: 1700, train loss: 0.0015 F1: 98.91% dev loss: 0.1408 F1: 57.27% 0.50 min
> Epoch: 23 Step: 1800, train loss: 0.0011 F1: 99.14% dev loss: 0.1476 F1: 56.85% 0.50 min
> Epoch: 25 Step: 1900, train loss: 0.0010 F1: 99.27% dev loss: 0.1509 F1: 56.66% 0.50 min
> Epoch: 26 Step: 2000, train loss: 0.0007 F1: 99.41% dev loss: 0.1521 F1: 56.85% 0.50 min
> Epoch: 27 Step: 2100, train loss: 0.0006 F1: 99.75% dev loss: 0.1528 F1: 57.56% 0.50 min
> Epoch: 29 Step: 2200, train loss: 0.0006 F1: 99.70% dev loss: 0.1579 F1: 56.69% 0.50 min
> Epoch: 30 Step: 2300, train loss: 0.0005 F1: 99.76% dev loss: 0.1544 F1: 56.62% 0.50 min
> Epoch: 31 Step: 2400, train loss: 0.0003 F1: 99.89% dev loss: 0.1564 F1: 56.07% 0.50 min
> Epoch: 33 Step: 2500, train loss: 0.0003 F1: 99.95% dev loss: 0.1597 F1: 57.04% 0.50 min
> Epoch: 34 Step: 2600, train loss: 0.0003 F1: 99.76% dev loss: 0.1600 F1: 56.66% 0.49 min
> Epoch: 35 Step: 2700, train loss: 0.0003 F1: 99.87% dev loss: 0.1605 F1: 56.46% 0.50 min
> Epoch: 37 Step: 2800, train loss: 0.0003 F1: 99.92% dev loss: 0.1603 F1: 56.33% 0.50 min
> Epoch: 38 Step: 2900, train loss: 0.0003 F1: 99.84% dev loss: 0.1610 F1: 56.34% 0.50 min
> Epoch: 39 Step: 3000, train loss: 0.0002 F1: 99.95% dev loss: 0.1609 F1: 56.19% 0.50 min

>>>>>>>>>>>>>>>>>>>>>2021-07-22 12:27:24.185750>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-lstm_lap14_seed8.pth
>>> seed: 8
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: lap14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.6794 F1: 9.57% dev loss: 0.2112 F1: 9.57% 0.49 min
> Epoch: 2 Step: 200, train loss: 0.1805 F1: 9.61% dev loss: 0.1918 F1: 9.57% 0.50 min
> Epoch: 3 Step: 300, train loss: 0.1436 F1: 15.54% dev loss: 0.1243 F1: 23.63% 0.50 min
> Epoch: 5 Step: 400, train loss: 0.0928 F1: 45.09% dev loss: 0.1247 F1: 28.67% 0.50 min
> Epoch: 6 Step: 500, train loss: 0.0609 F1: 59.52% dev loss: 0.0865 F1: 52.52% 0.50 min
> Epoch: 7 Step: 600, train loss: 0.0349 F1: 76.82% dev loss: 0.0794 F1: 56.20% 0.50 min
> Epoch: 9 Step: 700, train loss: 0.0207 F1: 90.01% dev loss: 0.0843 F1: 55.05% 0.50 min
> Epoch: 10 Step: 800, train loss: 0.0133 F1: 91.09% dev loss: 0.0853 F1: 62.91% 0.50 min
> Epoch: 11 Step: 900, train loss: 0.0100 F1: 91.74% dev loss: 0.0918 F1: 60.57% 0.50 min
> Epoch: 13 Step: 1000, train loss: 0.0059 F1: 97.28% dev loss: 0.0851 F1: 64.78% 0.50 min
> Epoch: 14 Step: 1100, train loss: 0.0041 F1: 96.71% dev loss: 0.0957 F1: 65.57% 0.50 min
> Epoch: 15 Step: 1200, train loss: 0.0041 F1: 97.13% dev loss: 0.0937 F1: 63.74% 0.50 min
> Epoch: 17 Step: 1300, train loss: 0.0022 F1: 98.57% dev loss: 0.0963 F1: 64.56% 0.50 min
> Epoch: 18 Step: 1400, train loss: 0.0016 F1: 98.58% dev loss: 0.0968 F1: 66.64% 0.50 min
> Epoch: 19 Step: 1500, train loss: 0.0011 F1: 99.00% dev loss: 0.0983 F1: 66.42% 0.50 min
> Epoch: 21 Step: 1600, train loss: 0.0008 F1: 99.43% dev loss: 0.0993 F1: 65.46% 0.50 min
> Epoch: 22 Step: 1700, train loss: 0.0010 F1: 99.32% dev loss: 0.1008 F1: 66.69% 0.50 min
> Epoch: 23 Step: 1800, train loss: 0.0009 F1: 99.41% dev loss: 0.1036 F1: 67.04% 0.50 min
> Epoch: 25 Step: 1900, train loss: 0.0005 F1: 99.60% dev loss: 0.1016 F1: 66.80% 0.50 min
> Epoch: 26 Step: 2000, train loss: 0.0004 F1: 99.94% dev loss: 0.1032 F1: 65.32% 0.50 min
> Epoch: 27 Step: 2100, train loss: 0.0004 F1: 99.83% dev loss: 0.1031 F1: 65.44% 0.50 min
> Epoch: 29 Step: 2200, train loss: 0.0004 F1: 99.90% dev loss: 0.1039 F1: 65.61% 0.50 min
> Epoch: 30 Step: 2300, train loss: 0.0003 F1: 99.76% dev loss: 0.1036 F1: 65.98% 0.50 min
> Epoch: 31 Step: 2400, train loss: 0.0003 F1: 99.70% dev loss: 0.1047 F1: 65.57% 0.50 min
> Epoch: 33 Step: 2500, train loss: 0.0004 F1: 99.65% dev loss: 0.1064 F1: 65.89% 0.50 min
> Epoch: 34 Step: 2600, train loss: 0.0003 F1: 99.91% dev loss: 0.1066 F1: 67.02% 0.50 min
> Epoch: 35 Step: 2700, train loss: 0.0002 F1: 99.93% dev loss: 0.1055 F1: 66.15% 0.50 min
> Epoch: 37 Step: 2800, train loss: 0.0002 F1: 99.98% dev loss: 0.1057 F1: 66.46% 0.50 min
> Epoch: 38 Step: 2900, train loss: 0.0002 F1: 99.94% dev loss: 0.1058 F1: 66.28% 0.50 min
> Epoch: 39 Step: 3000, train loss: 0.0002 F1: 99.99% dev loss: 0.1057 F1: 66.27% 0.50 min

>>>>>>>>>>>>>>>>>>>>>2021-07-22 12:43:20.285303>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: lstm
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-lstm_res16_seed8.pth
>>> seed: 8
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: res16
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_Train_SB1_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_dev_SB1_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_test_SB1_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.7167 F1: 9.45% dev loss: 0.2116 F1: 9.51% 0.49 min
> Epoch: 3 Step: 200, train loss: 0.2256 F1: 9.45% dev loss: 0.1711 F1: 9.51% 0.49 min
> Epoch: 5 Step: 300, train loss: 0.1601 F1: 26.48% dev loss: 0.1141 F1: 29.62% 0.49 min
> Epoch: 7 Step: 400, train loss: 0.0874 F1: 43.07% dev loss: 0.0771 F1: 34.65% 0.49 min
> Epoch: 9 Step: 500, train loss: 0.0462 F1: 57.46% dev loss: 0.0948 F1: 43.54% 0.49 min
> Epoch: 11 Step: 600, train loss: 0.0239 F1: 63.62% dev loss: 0.1018 F1: 44.63% 0.49 min
> Epoch: 13 Step: 700, train loss: 0.0154 F1: 70.48% dev loss: 0.1037 F1: 45.84% 0.49 min
> Epoch: 15 Step: 800, train loss: 0.0097 F1: 86.49% dev loss: 0.0983 F1: 38.43% 0.49 min
> Epoch: 17 Step: 900, train loss: 0.0039 F1: 97.19% dev loss: 0.0983 F1: 43.09% 0.49 min
> Epoch: 19 Step: 1000, train loss: 0.0023 F1: 97.80% dev loss: 0.0981 F1: 45.76% 0.49 min
> Epoch: 21 Step: 1100, train loss: 0.0014 F1: 98.98% dev loss: 0.1008 F1: 43.99% 0.49 min
> Epoch: 23 Step: 1200, train loss: 0.0010 F1: 99.35% dev loss: 0.1040 F1: 45.13% 0.49 min
> Epoch: 25 Step: 1300, train loss: 0.0008 F1: 99.36% dev loss: 0.1094 F1: 43.87% 0.49 min
> Epoch: 27 Step: 1400, train loss: 0.0008 F1: 99.12% dev loss: 0.1102 F1: 43.72% 0.49 min
> Epoch: 29 Step: 1500, train loss: 0.0006 F1: 99.63% dev loss: 0.1100 F1: 43.53% 0.49 min
> Epoch: 31 Step: 1600, train loss: 0.0004 F1: 99.81% dev loss: 0.1093 F1: 42.56% 0.49 min
> Epoch: 33 Step: 1700, train loss: 0.0004 F1: 99.88% dev loss: 0.1087 F1: 44.80% 0.49 min
> Epoch: 35 Step: 1800, train loss: 0.0005 F1: 99.71% dev loss: 0.1131 F1: 44.11% 0.49 min
> Epoch: 37 Step: 1900, train loss: 0.0003 F1: 99.76% dev loss: 0.1119 F1: 45.38% 0.49 min
> Epoch: 39 Step: 2000, train loss: 0.0003 F1: 99.86% dev loss: 0.1095 F1: 44.83% 0.49 min
> Epoch: 41 Step: 2100, train loss: 0.0003 F1: 99.58% dev loss: 0.1104 F1: 47.78% 0.49 min
> Epoch: 43 Step: 2200, train loss: 0.0003 F1: 99.88% dev loss: 0.1104 F1: 43.84% 0.49 min
> Epoch: 45 Step: 2300, train loss: 0.0002 F1: 100.00% dev loss: 0.1117 F1: 44.94% 0.49 min
> Epoch: 47 Step: 2400, train loss: 0.0001 F1: 100.00% dev loss: 0.1124 F1: 43.17% 0.49 min
> Epoch: 49 Step: 2500, train loss: 0.0002 F1: 99.99% dev loss: 0.1125 F1: 43.73% 0.49 min
> Epoch: 51 Step: 2600, train loss: 0.0001 F1: 99.99% dev loss: 0.1126 F1: 43.42% 0.49 min
> Epoch: 53 Step: 2700, train loss: 0.0001 F1: 99.98% dev loss: 0.1126 F1: 43.54% 0.49 min
> Epoch: 55 Step: 2800, train loss: 0.0001 F1: 100.00% dev loss: 0.1132 F1: 43.89% 0.49 min
> Epoch: 57 Step: 2900, train loss: 0.0003 F1: 99.96% dev loss: 0.1149 F1: 43.56% 0.49 min
> Epoch: 59 Step: 3000, train loss: 0.0001 F1: 99.98% dev loss: 0.1148 F1: 43.15% 0.49 min

>>>>>>>>>>>>>>>>>>>>>2021-07-22 12:58:54.643914>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: san
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-san_res14_seed8.pth
>>> seed: 8
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.4518 F1: 16.96% dev loss: 0.1885 F1: 17.70% 0.48 min
> Epoch: 2 Step: 200, train loss: 0.1528 F1: 36.33% dev loss: 0.1094 F1: 47.47% 0.47 min
> Epoch: 3 Step: 300, train loss: 0.0793 F1: 59.07% dev loss: 0.0942 F1: 54.17% 0.47 min
> Epoch: 5 Step: 400, train loss: 0.0436 F1: 78.06% dev loss: 0.1030 F1: 56.13% 0.47 min
> Epoch: 6 Step: 500, train loss: 0.0270 F1: 84.02% dev loss: 0.1231 F1: 56.94% 0.47 min
> Epoch: 7 Step: 600, train loss: 0.0219 F1: 83.49% dev loss: 0.1290 F1: 55.84% 0.47 min
> Epoch: 9 Step: 700, train loss: 0.0100 F1: 95.75% dev loss: 0.1299 F1: 59.51% 0.47 min
> Epoch: 10 Step: 800, train loss: 0.0064 F1: 96.93% dev loss: 0.1374 F1: 60.09% 0.48 min
> Epoch: 11 Step: 900, train loss: 0.0056 F1: 95.68% dev loss: 0.1369 F1: 55.19% 0.47 min
> Epoch: 13 Step: 1000, train loss: 0.0029 F1: 98.59% dev loss: 0.1427 F1: 60.14% 0.47 min
> Epoch: 14 Step: 1100, train loss: 0.0016 F1: 99.11% dev loss: 0.1470 F1: 59.14% 0.47 min
> Epoch: 15 Step: 1200, train loss: 0.0013 F1: 99.18% dev loss: 0.1539 F1: 58.22% 0.47 min
> Epoch: 17 Step: 1300, train loss: 0.0008 F1: 99.68% dev loss: 0.1499 F1: 57.14% 0.48 min
> Epoch: 18 Step: 1400, train loss: 0.0010 F1: 99.56% dev loss: 0.1539 F1: 61.05% 0.48 min
> Epoch: 19 Step: 1500, train loss: 0.0006 F1: 99.72% dev loss: 0.1621 F1: 59.46% 0.48 min
> Epoch: 21 Step: 1600, train loss: 0.0005 F1: 99.82% dev loss: 0.1669 F1: 60.30% 0.48 min
> Epoch: 22 Step: 1700, train loss: 0.0008 F1: 99.65% dev loss: 0.1640 F1: 57.17% 0.48 min
> Epoch: 23 Step: 1800, train loss: 0.0004 F1: 99.80% dev loss: 0.1595 F1: 59.32% 0.47 min
> Epoch: 25 Step: 1900, train loss: 0.0003 F1: 99.94% dev loss: 0.1611 F1: 59.35% 0.47 min
> Epoch: 26 Step: 2000, train loss: 0.0005 F1: 99.88% dev loss: 0.1674 F1: 57.57% 0.47 min
> Epoch: 27 Step: 2100, train loss: 0.0004 F1: 99.74% dev loss: 0.1612 F1: 57.97% 0.47 min
> Epoch: 29 Step: 2200, train loss: 0.0003 F1: 99.93% dev loss: 0.1660 F1: 58.82% 0.47 min
> Epoch: 30 Step: 2300, train loss: 0.0002 F1: 99.87% dev loss: 0.1663 F1: 56.50% 0.47 min
> Epoch: 31 Step: 2400, train loss: 0.0003 F1: 99.89% dev loss: 0.1587 F1: 58.16% 0.47 min
> Epoch: 33 Step: 2500, train loss: 0.0002 F1: 99.89% dev loss: 0.1589 F1: 58.64% 0.48 min
> Epoch: 34 Step: 2600, train loss: 0.0001 F1: 99.98% dev loss: 0.1589 F1: 59.50% 0.47 min
> Epoch: 35 Step: 2700, train loss: 0.0002 F1: 99.87% dev loss: 0.1616 F1: 58.67% 0.48 min
> Epoch: 37 Step: 2800, train loss: 0.0001 F1: 99.99% dev loss: 0.1631 F1: 58.29% 0.47 min
> Epoch: 38 Step: 2900, train loss: 0.0001 F1: 99.97% dev loss: 0.1618 F1: 59.20% 0.48 min
> Epoch: 39 Step: 3000, train loss: 0.0002 F1: 99.94% dev loss: 0.1616 F1: 59.33% 0.48 min

>>>>>>>>>>>>>>>>>>>>>2021-07-22 13:14:07.897204>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: san
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-san_lap14_seed8.pth
>>> seed: 8
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: lap14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.3771 F1: 10.59% dev loss: 0.1639 F1: 14.74% 0.48 min
> Epoch: 2 Step: 200, train loss: 0.1159 F1: 32.68% dev loss: 0.0997 F1: 37.19% 0.48 min
> Epoch: 3 Step: 300, train loss: 0.0638 F1: 55.64% dev loss: 0.0797 F1: 53.77% 0.48 min
> Epoch: 5 Step: 400, train loss: 0.0313 F1: 83.51% dev loss: 0.0905 F1: 55.88% 0.47 min
> Epoch: 6 Step: 500, train loss: 0.0186 F1: 86.66% dev loss: 0.0936 F1: 56.35% 0.47 min
> Epoch: 7 Step: 600, train loss: 0.0161 F1: 87.93% dev loss: 0.0940 F1: 53.06% 0.48 min
> Epoch: 9 Step: 700, train loss: 0.0097 F1: 89.99% dev loss: 0.0939 F1: 59.37% 0.48 min
> Epoch: 10 Step: 800, train loss: 0.0076 F1: 92.57% dev loss: 0.1134 F1: 58.08% 0.48 min
> Epoch: 11 Step: 900, train loss: 0.0035 F1: 96.46% dev loss: 0.1020 F1: 57.87% 0.48 min
> Epoch: 13 Step: 1000, train loss: 0.0017 F1: 98.98% dev loss: 0.1003 F1: 60.89% 0.48 min
> Epoch: 14 Step: 1100, train loss: 0.0009 F1: 99.09% dev loss: 0.1068 F1: 58.79% 0.48 min
> Epoch: 15 Step: 1200, train loss: 0.0007 F1: 99.34% dev loss: 0.1097 F1: 58.69% 0.48 min
> Epoch: 17 Step: 1300, train loss: 0.0004 F1: 99.75% dev loss: 0.1093 F1: 62.08% 0.48 min
> Epoch: 18 Step: 1400, train loss: 0.0003 F1: 99.50% dev loss: 0.1107 F1: 60.85% 0.48 min
> Epoch: 19 Step: 1500, train loss: 0.0003 F1: 99.75% dev loss: 0.1129 F1: 61.08% 0.47 min
> Epoch: 21 Step: 1600, train loss: 0.0005 F1: 99.54% dev loss: 0.1129 F1: 61.38% 0.48 min
> Epoch: 22 Step: 1700, train loss: 0.0003 F1: 99.78% dev loss: 0.1134 F1: 60.83% 0.48 min
> Epoch: 23 Step: 1800, train loss: 0.0002 F1: 99.79% dev loss: 0.1158 F1: 60.63% 0.47 min
> Epoch: 25 Step: 1900, train loss: 0.0002 F1: 99.94% dev loss: 0.1151 F1: 62.36% 0.47 min
> Epoch: 26 Step: 2000, train loss: 0.0001 F1: 99.98% dev loss: 0.1161 F1: 62.67% 0.48 min
> Epoch: 27 Step: 2100, train loss: 0.0002 F1: 99.88% dev loss: 0.1144 F1: 62.95% 0.47 min
> Epoch: 29 Step: 2200, train loss: 0.0002 F1: 99.86% dev loss: 0.1166 F1: 61.04% 0.48 min
> Epoch: 30 Step: 2300, train loss: 0.0001 F1: 100.00% dev loss: 0.1164 F1: 61.58% 0.47 min
> Epoch: 31 Step: 2400, train loss: 0.0002 F1: 99.82% dev loss: 0.1196 F1: 60.50% 0.47 min
> Epoch: 33 Step: 2500, train loss: 0.0002 F1: 99.83% dev loss: 0.1204 F1: 62.73% 0.48 min
> Epoch: 34 Step: 2600, train loss: 0.0001 F1: 99.98% dev loss: 0.1206 F1: 62.36% 0.47 min
> Epoch: 35 Step: 2700, train loss: 0.0001 F1: 99.99% dev loss: 0.1213 F1: 61.81% 0.47 min
> Epoch: 37 Step: 2800, train loss: 0.0001 F1: 100.00% dev loss: 0.1218 F1: 61.50% 0.47 min
> Epoch: 38 Step: 2900, train loss: 0.0001 F1: 99.88% dev loss: 0.1225 F1: 61.38% 0.47 min
> Epoch: 39 Step: 3000, train loss: 0.0001 F1: 99.98% dev loss: 0.1230 F1: 61.07% 0.47 min

>>>>>>>>>>>>>>>>>>>>>2021-07-22 13:29:21.809982>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: san
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-san_res16_seed8.pth
>>> seed: 8
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: res16
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_Train_SB1_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_dev_SB1_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_test_SB1_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 0.4235 F1: 14.08% dev loss: 0.1335 F1: 17.22% 0.47 min
> Epoch: 3 Step: 200, train loss: 0.1248 F1: 30.96% dev loss: 0.0831 F1: 36.02% 0.47 min
> Epoch: 5 Step: 300, train loss: 0.0547 F1: 53.06% dev loss: 0.0751 F1: 48.63% 0.47 min
> Epoch: 7 Step: 400, train loss: 0.0252 F1: 71.95% dev loss: 0.0911 F1: 43.70% 0.47 min
> Epoch: 9 Step: 500, train loss: 0.0226 F1: 82.72% dev loss: 0.1295 F1: 34.76% 0.47 min
> Epoch: 11 Step: 600, train loss: 0.0125 F1: 87.64% dev loss: 0.0980 F1: 43.61% 0.47 min
> Epoch: 13 Step: 700, train loss: 0.0044 F1: 94.35% dev loss: 0.1034 F1: 41.46% 0.47 min
> Epoch: 15 Step: 800, train loss: 0.0034 F1: 97.05% dev loss: 0.1055 F1: 40.67% 0.47 min
> Epoch: 17 Step: 900, train loss: 0.0012 F1: 98.73% dev loss: 0.1054 F1: 40.13% 0.47 min
> Epoch: 19 Step: 1000, train loss: 0.0007 F1: 99.58% dev loss: 0.1061 F1: 37.01% 0.47 min
> Epoch: 21 Step: 1100, train loss: 0.0003 F1: 99.80% dev loss: 0.1094 F1: 42.65% 0.47 min
> Epoch: 23 Step: 1200, train loss: 0.0002 F1: 99.60% dev loss: 0.1126 F1: 39.17% 0.47 min
> Epoch: 25 Step: 1300, train loss: 0.0003 F1: 99.93% dev loss: 0.1127 F1: 39.01% 0.47 min
> Epoch: 27 Step: 1400, train loss: 0.0003 F1: 99.75% dev loss: 0.1134 F1: 37.11% 0.47 min
> Epoch: 29 Step: 1500, train loss: 0.0002 F1: 99.77% dev loss: 0.1145 F1: 36.82% 0.47 min
> Epoch: 31 Step: 1600, train loss: 0.0002 F1: 99.81% dev loss: 0.1157 F1: 37.20% 0.47 min
> Epoch: 33 Step: 1700, train loss: 0.0003 F1: 99.62% dev loss: 0.1190 F1: 41.64% 0.47 min
> Epoch: 35 Step: 1800, train loss: 0.0004 F1: 99.90% dev loss: 0.1147 F1: 37.15% 0.47 min
> Epoch: 37 Step: 1900, train loss: 0.0003 F1: 99.91% dev loss: 0.1211 F1: 36.96% 0.47 min
> Epoch: 39 Step: 2000, train loss: 0.0002 F1: 99.86% dev loss: 0.1141 F1: 42.47% 0.47 min
> Epoch: 41 Step: 2100, train loss: 0.0001 F1: 100.00% dev loss: 0.1210 F1: 37.00% 0.47 min
> Epoch: 43 Step: 2200, train loss: 0.0001 F1: 99.94% dev loss: 0.1260 F1: 37.34% 0.47 min
> Epoch: 45 Step: 2300, train loss: 0.0001 F1: 99.95% dev loss: 0.1248 F1: 40.66% 0.47 min
> Epoch: 47 Step: 2400, train loss: 0.0001 F1: 99.90% dev loss: 0.1270 F1: 37.25% 0.47 min
> Epoch: 49 Step: 2500, train loss: 0.0001 F1: 99.88% dev loss: 0.1240 F1: 36.91% 0.47 min
> Epoch: 51 Step: 2600, train loss: 0.0001 F1: 100.00% dev loss: 0.1260 F1: 37.31% 0.47 min
> Epoch: 53 Step: 2700, train loss: 0.0001 F1: 99.87% dev loss: 0.1265 F1: 36.99% 0.47 min
> Epoch: 55 Step: 2800, train loss: 0.0001 F1: 100.00% dev loss: 0.1281 F1: 37.06% 0.47 min
> Epoch: 57 Step: 2900, train loss: 0.0001 F1: 99.99% dev loss: 0.1282 F1: 37.01% 0.47 min
> Epoch: 59 Step: 3000, train loss: 0.0000 F1: 100.00% dev loss: 0.1283 F1: 37.01% 0.47 min

>>>>>>>>>>>>>>>>>>>>>2021-07-22 13:44:09.704943>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: crf
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-crf_res14_seed8.pth
>>> seed: 8
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 19.1218 F1: 10.49% dev loss: 8.0556 F1: 10.73% 0.59 min
> Epoch: 2 Step: 200, train loss: 5.9748 F1: 27.86% dev loss: 4.9586 F1: 33.82% 0.59 min
> Epoch: 3 Step: 300, train loss: 3.6214 F1: 45.81% dev loss: 4.2852 F1: 46.44% 0.59 min
> Epoch: 5 Step: 400, train loss: 2.3303 F1: 65.03% dev loss: 4.3811 F1: 56.18% 0.59 min
> Epoch: 6 Step: 500, train loss: 1.5568 F1: 77.93% dev loss: 4.6106 F1: 56.48% 0.59 min
> Epoch: 7 Step: 600, train loss: 1.2214 F1: 80.74% dev loss: 4.9710 F1: 56.12% 0.59 min
> Epoch: 9 Step: 700, train loss: 0.8011 F1: 93.20% dev loss: 6.2649 F1: 53.28% 0.59 min
> Epoch: 10 Step: 800, train loss: 0.5795 F1: 92.72% dev loss: 6.4565 F1: 57.42% 0.59 min
> Epoch: 11 Step: 900, train loss: 0.4049 F1: 94.57% dev loss: 6.4557 F1: 54.76% 0.59 min
> Epoch: 13 Step: 1000, train loss: 0.3371 F1: 97.67% dev loss: 6.4726 F1: 61.08% 0.60 min
> Epoch: 14 Step: 1100, train loss: 0.2398 F1: 97.93% dev loss: 6.4723 F1: 58.67% 0.60 min
> Epoch: 15 Step: 1200, train loss: 0.1751 F1: 98.20% dev loss: 7.2726 F1: 54.23% 0.60 min
> Epoch: 17 Step: 1300, train loss: 0.1179 F1: 98.59% dev loss: 7.4341 F1: 52.10% 0.60 min
> Epoch: 18 Step: 1400, train loss: 0.0806 F1: 99.23% dev loss: 7.7702 F1: 56.29% 0.59 min
> Epoch: 19 Step: 1500, train loss: 0.0644 F1: 99.56% dev loss: 8.1533 F1: 59.92% 0.60 min
> Epoch: 21 Step: 1600, train loss: 0.0481 F1: 99.79% dev loss: 8.2680 F1: 57.68% 0.59 min
> Epoch: 22 Step: 1700, train loss: 0.0452 F1: 99.71% dev loss: 8.6727 F1: 56.66% 0.59 min
> Epoch: 23 Step: 1800, train loss: 0.0317 F1: 99.73% dev loss: 8.9651 F1: 54.07% 0.59 min
> Epoch: 25 Step: 1900, train loss: 0.0361 F1: 99.66% dev loss: 8.6072 F1: 56.26% 0.60 min
> Epoch: 26 Step: 2000, train loss: 0.0246 F1: 99.90% dev loss: 8.6227 F1: 57.51% 0.59 min
> Epoch: 27 Step: 2100, train loss: 0.0162 F1: 99.96% dev loss: 8.7658 F1: 58.27% 0.59 min
> Epoch: 29 Step: 2200, train loss: 0.0143 F1: 100.00% dev loss: 8.9078 F1: 58.80% 0.59 min
> Epoch: 30 Step: 2300, train loss: 0.0071 F1: 99.95% dev loss: 9.0745 F1: 57.66% 0.59 min
> Epoch: 31 Step: 2400, train loss: 0.0151 F1: 99.88% dev loss: 8.9996 F1: 57.41% 0.59 min
> Epoch: 33 Step: 2500, train loss: 0.0070 F1: 100.00% dev loss: 8.9032 F1: 58.61% 0.58 min
> Epoch: 34 Step: 2600, train loss: 0.0103 F1: 99.97% dev loss: 9.0514 F1: 57.54% 0.59 min
> Epoch: 35 Step: 2700, train loss: 0.0087 F1: 99.97% dev loss: 9.1552 F1: 58.08% 0.59 min
> Epoch: 37 Step: 2800, train loss: 0.0064 F1: 100.00% dev loss: 9.1736 F1: 58.17% 0.59 min
> Epoch: 38 Step: 2900, train loss: 0.0052 F1: 100.00% dev loss: 9.1363 F1: 57.36% 0.59 min
> Epoch: 39 Step: 3000, train loss: 0.0058 F1: 100.00% dev loss: 9.1521 F1: 57.65% 0.59 min

>>>>>>>>>>>>>>>>>>>>>2021-07-22 14:02:53.435702>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: crf
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-crf_lap14_seed8.pth
>>> seed: 8
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: lap14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Laptop_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 18.1105 F1: 9.57% dev loss: 6.5012 F1: 9.57% 0.60 min
> Epoch: 2 Step: 200, train loss: 4.8731 F1: 23.34% dev loss: 4.1633 F1: 29.15% 0.60 min
> Epoch: 3 Step: 300, train loss: 2.9327 F1: 47.13% dev loss: 3.5964 F1: 46.77% 0.60 min
> Epoch: 5 Step: 400, train loss: 1.9669 F1: 69.61% dev loss: 3.7714 F1: 50.35% 0.60 min
> Epoch: 6 Step: 500, train loss: 1.3191 F1: 83.84% dev loss: 3.5133 F1: 60.86% 0.60 min
> Epoch: 7 Step: 600, train loss: 0.8758 F1: 86.21% dev loss: 4.1788 F1: 55.04% 0.60 min
> Epoch: 9 Step: 700, train loss: 0.6369 F1: 93.19% dev loss: 4.2939 F1: 59.75% 0.60 min
> Epoch: 10 Step: 800, train loss: 0.4095 F1: 95.18% dev loss: 5.1808 F1: 62.25% 0.59 min
> Epoch: 11 Step: 900, train loss: 0.4015 F1: 94.36% dev loss: 5.2153 F1: 59.29% 0.61 min
> Epoch: 13 Step: 1000, train loss: 0.2418 F1: 96.83% dev loss: 5.4302 F1: 59.62% 0.60 min
> Epoch: 14 Step: 1100, train loss: 0.1902 F1: 97.97% dev loss: 5.5850 F1: 61.04% 0.60 min
> Epoch: 15 Step: 1200, train loss: 0.1080 F1: 98.49% dev loss: 5.9259 F1: 56.34% 0.60 min
> Epoch: 17 Step: 1300, train loss: 0.0957 F1: 98.98% dev loss: 5.8415 F1: 61.78% 0.60 min
> Epoch: 18 Step: 1400, train loss: 0.0996 F1: 98.66% dev loss: 5.8414 F1: 61.62% 0.60 min
> Epoch: 19 Step: 1500, train loss: 0.0574 F1: 99.38% dev loss: 6.0038 F1: 61.20% 0.60 min
> Epoch: 21 Step: 1600, train loss: 0.0585 F1: 99.53% dev loss: 6.4627 F1: 60.01% 0.60 min
> Epoch: 22 Step: 1700, train loss: 0.0239 F1: 99.58% dev loss: 6.5823 F1: 62.79% 0.60 min
> Epoch: 23 Step: 1800, train loss: 0.0458 F1: 99.41% dev loss: 6.2095 F1: 62.07% 0.59 min
> Epoch: 25 Step: 1900, train loss: 0.0186 F1: 99.85% dev loss: 6.6019 F1: 61.49% 0.59 min
> Epoch: 26 Step: 2000, train loss: 0.0195 F1: 99.61% dev loss: 6.4946 F1: 63.39% 0.59 min
> Epoch: 27 Step: 2100, train loss: 0.0111 F1: 99.84% dev loss: 6.3977 F1: 62.88% 0.59 min
> Epoch: 29 Step: 2200, train loss: 0.0199 F1: 99.98% dev loss: 6.4930 F1: 63.06% 0.59 min
> Epoch: 30 Step: 2300, train loss: 0.0099 F1: 99.90% dev loss: 6.8275 F1: 64.22% 0.59 min
> Epoch: 31 Step: 2400, train loss: 0.0118 F1: 99.93% dev loss: 7.1576 F1: 62.42% 0.60 min
> Epoch: 33 Step: 2500, train loss: 0.0096 F1: 100.00% dev loss: 6.8277 F1: 61.80% 0.59 min
> Epoch: 34 Step: 2600, train loss: 0.0053 F1: 100.00% dev loss: 6.8083 F1: 61.55% 0.59 min
> Epoch: 35 Step: 2700, train loss: 0.0022 F1: 99.99% dev loss: 6.8274 F1: 61.89% 0.60 min
> Epoch: 37 Step: 2800, train loss: 0.0009 F1: 100.00% dev loss: 6.8611 F1: 61.86% 0.59 min
> Epoch: 38 Step: 2900, train loss: 0.0010 F1: 100.00% dev loss: 6.9272 F1: 62.77% 0.58 min
> Epoch: 39 Step: 3000, train loss: 0.0019 F1: 99.94% dev loss: 6.9226 F1: 62.42% 0.59 min

>>>>>>>>>>>>>>>>>>>>>2021-07-22 14:21:45.397285>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: crf
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-crf_res16_seed8.pth
>>> seed: 8
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: res16
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_Train_SB1_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_dev_SB1_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_test_SB1_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 17.1701 F1: 9.45% dev loss: 6.0225 F1: 9.51% 0.59 min
> Epoch: 3 Step: 200, train loss: 5.0009 F1: 25.71% dev loss: 3.2062 F1: 31.51% 0.58 min
> Epoch: 5 Step: 300, train loss: 2.4878 F1: 46.44% dev loss: 3.2727 F1: 41.36% 0.58 min
> Epoch: 7 Step: 400, train loss: 1.6059 F1: 57.35% dev loss: 3.3963 F1: 47.14% 0.58 min
> Epoch: 9 Step: 500, train loss: 1.0506 F1: 64.24% dev loss: 3.9086 F1: 49.01% 0.58 min
> Epoch: 11 Step: 600, train loss: 0.5715 F1: 77.33% dev loss: 3.4714 F1: 48.02% 0.59 min
> Epoch: 13 Step: 700, train loss: 0.4448 F1: 84.68% dev loss: 4.5742 F1: 49.14% 0.58 min
> Epoch: 15 Step: 800, train loss: 0.3034 F1: 90.19% dev loss: 4.7403 F1: 40.93% 0.58 min
> Epoch: 17 Step: 900, train loss: 0.1896 F1: 91.35% dev loss: 5.3589 F1: 42.29% 0.58 min
> Epoch: 19 Step: 1000, train loss: 0.1257 F1: 98.46% dev loss: 4.8914 F1: 43.49% 0.57 min
> Epoch: 21 Step: 1100, train loss: 0.0567 F1: 98.14% dev loss: 4.9799 F1: 50.38% 0.58 min
> Epoch: 23 Step: 1200, train loss: 0.0452 F1: 99.48% dev loss: 5.6542 F1: 48.38% 0.58 min
> Epoch: 25 Step: 1300, train loss: 0.0773 F1: 99.21% dev loss: 5.4118 F1: 46.36% 0.58 min
> Epoch: 27 Step: 1400, train loss: 0.0431 F1: 99.24% dev loss: 5.9303 F1: 44.57% 0.59 min
> Epoch: 29 Step: 1500, train loss: 0.0233 F1: 99.03% dev loss: 6.4685 F1: 47.11% 0.60 min
> Epoch: 31 Step: 1600, train loss: 0.0132 F1: 99.94% dev loss: 6.6258 F1: 47.93% 0.59 min
> Epoch: 33 Step: 1700, train loss: 0.0163 F1: 99.94% dev loss: 6.2321 F1: 43.80% 0.59 min
> Epoch: 35 Step: 1800, train loss: 0.0059 F1: 99.99% dev loss: 6.5874 F1: 44.05% 0.59 min
> Epoch: 37 Step: 1900, train loss: 0.0262 F1: 99.92% dev loss: 6.3969 F1: 44.15% 0.59 min
> Epoch: 39 Step: 2000, train loss: 0.0137 F1: 99.94% dev loss: 6.6235 F1: 46.19% 0.59 min
> Epoch: 41 Step: 2100, train loss: 0.0114 F1: 99.96% dev loss: 6.4706 F1: 42.60% 0.59 min
> Epoch: 43 Step: 2200, train loss: 0.0011 F1: 100.00% dev loss: 6.4423 F1: 42.92% 0.60 min
> Epoch: 45 Step: 2300, train loss: 0.0039 F1: 100.00% dev loss: 6.5140 F1: 42.66% 0.60 min
> Epoch: 47 Step: 2400, train loss: 0.0009 F1: 100.00% dev loss: 6.5677 F1: 43.02% 0.59 min
> Epoch: 49 Step: 2500, train loss: 0.0035 F1: 99.99% dev loss: 6.5901 F1: 42.73% 0.60 min
> Epoch: 51 Step: 2600, train loss: 0.0023 F1: 100.00% dev loss: 6.5872 F1: 43.19% 0.61 min
> Epoch: 53 Step: 2700, train loss: 0.0008 F1: 100.00% dev loss: 6.5826 F1: 43.21% 0.59 min
> Epoch: 55 Step: 2800, train loss: 0.0007 F1: 100.00% dev loss: 6.5874 F1: 43.21% 0.60 min
> Epoch: 57 Step: 2900, train loss: 0.0011 F1: 100.00% dev loss: 6.6413 F1: 45.71% 0.60 min
> Epoch: 59 Step: 3000, train loss: 0.0007 F1: 100.00% dev loss: 6.6463 F1: 46.40% 0.59 min


>>>>>>>>>>>>>>>>>>>>>2021-07-22 15:24:14.678695>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 5e-05
>>> epochs: 100
>>> step: 100
>>> model_name: bert
>>> downstream: lstm-crf
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/bert-lstm-crf_res16_seed8.pth
>>> seed: 8
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: False
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: res16
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_Train_SB1_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_dev_SB1_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2016/processed/ABSA16_Restaurants_test_SB1_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 19.6801 F1: 9.34% dev loss: 7.9090 F1: 9.51% 0.62 min
Initializing ELMo

>>>>>>>>>>>>>>>>>>>>>2021-07-22 15:27:51.907791>>>>>>>>>>>>>>>>>>>>>>>>
>>> max_seq_len: 128
>>> pretrained_bert_name: bert-base-uncased
>>> split_ratio: [0.8, 0.1, 0.1]
>>> dropout: 0.1
>>> weight_decay: 0.0001
>>> num_classes: 10
>>> batch_size: 32
>>> lr: 0.001
>>> epochs: 100
>>> step: 100
>>> model_name: elmo
>>> downstream: lstm-crf
>>> num_heads: 12
>>> num_layers: 1
>>> optimizer: <class 'transformers.optimization.AdamW'>
>>> loss: focal
>>> gamma: 2.0
>>> alpha: 0.75
>>> shuffle: False
>>> load_model: False
>>> state_dict_path: checkout/state_dict/elmo-lstm-crf_res14_seed6.pth
>>> seed: 6
>>> metrics: f1
>>> verbose: False
>>> warmup_steps: 500
>>> max_steps: 3000
>>> max_grad_norm: 2.0
>>> clip_large_grad: True
>>> adam_beta1: 0.9
>>> adam_beta2: 0.999
>>> adam_epsilon: 1e-08
>>> adam_amsgrad: False
>>> mode: res14
>>> finetune_elmo: False
>>> demo: False
>>> file_path: {'train': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_Train_v2.csv', 'dev': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_dev_v2.csv', 'test': '/home/kevin/nut/1_project/absa/data/Semeval2014/processed/Restaurants_test_v2.csv'}
>>> optimizer_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 0.001, 'weight_decay': 0.0001}
>>> device: cuda:0
>>> class weight(or alpha) [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]
> Epoch: 1 Step: 100, train loss: 15.1988 F1: 17.33% dev loss: 4.0836 F1: 23.02% 0.47 min
> Epoch: 2 Step: 200, train loss: 3.2442 F1: 42.30% dev loss: 2.9703 F1: 38.74% 0.48 min
> Epoch: 3 Step: 300, train loss: 2.2324 F1: 56.53% dev loss: 2.9309 F1: 49.63% 0.47 min
> Epoch: 5 Step: 400, train loss: 1.6302 F1: 71.32% dev loss: 2.9595 F1: 49.11% 0.48 min
> Epoch: 6 Step: 500, train loss: 1.2440 F1: 80.63% dev loss: 3.6342 F1: 50.64% 0.48 min
> Epoch: 7 Step: 600, train loss: 1.0375 F1: 86.24% dev loss: 3.3373 F1: 52.16% 0.49 min
> Epoch: 9 Step: 700, train loss: 0.4920 F1: 97.28% dev loss: 3.7265 F1: 51.73% 0.49 min
> Epoch: 10 Step: 800, train loss: 0.3938 F1: 96.19% dev loss: 4.6643 F1: 50.87% 0.48 min
> Epoch: 11 Step: 900, train loss: 0.2873 F1: 97.46% dev loss: 4.3499 F1: 54.01% 0.48 min
> Epoch: 13 Step: 1000, train loss: 0.1400 F1: 99.67% dev loss: 4.2189 F1: 54.98% 0.48 min
> Epoch: 14 Step: 1100, train loss: 0.0609 F1: 99.82% dev loss: 4.8269 F1: 50.17% 0.49 min
> Epoch: 15 Step: 1200, train loss: 0.0317 F1: 99.92% dev loss: 4.7452 F1: 55.84% 0.48 min
> Epoch: 17 Step: 1300, train loss: 0.0068 F1: 100.00% dev loss: 4.8675 F1: 54.07% 0.48 min
> Epoch: 18 Step: 1400, train loss: 0.0034 F1: 100.00% dev loss: 4.9552 F1: 53.83% 0.49 min
> Epoch: 19 Step: 1500, train loss: 0.0026 F1: 100.00% dev loss: 5.0376 F1: 54.50% 0.50 min
> Epoch: 21 Step: 1600, train loss: 0.0022 F1: 100.00% dev loss: 5.0965 F1: 54.27% 0.50 min
> Epoch: 22 Step: 1700, train loss: 0.0018 F1: 100.00% dev loss: 5.1372 F1: 54.55% 0.49 min
> Epoch: 23 Step: 1800, train loss: 0.0016 F1: 100.00% dev loss: 5.1920 F1: 54.53% 0.48 min
> Epoch: 25 Step: 1900, train loss: 0.0014 F1: 100.00% dev loss: 5.2393 F1: 54.61% 0.48 min
> Epoch: 26 Step: 2000, train loss: 0.0013 F1: 100.00% dev loss: 5.2694 F1: 54.59% 0.48 min
> Epoch: 27 Step: 2100, train loss: 0.0012 F1: 100.00% dev loss: 5.3006 F1: 54.64% 0.48 min
> Epoch: 29 Step: 2200, train loss: 0.0011 F1: 100.00% dev loss: 5.3433 F1: 54.75% 0.49 min
> Epoch: 30 Step: 2300, train loss: 0.0010 F1: 100.00% dev loss: 5.3633 F1: 54.93% 0.48 min
> Epoch: 31 Step: 2400, train loss: 0.0010 F1: 100.00% dev loss: 5.3859 F1: 54.99% 0.48 min
> Epoch: 33 Step: 2500, train loss: 0.0009 F1: 100.00% dev loss: 5.4105 F1: 54.79% 0.49 min
> Epoch: 34 Step: 2600, train loss: 0.0009 F1: 100.00% dev loss: 5.4209 F1: 54.95% 0.48 min
> Epoch: 35 Step: 2700, train loss: 0.0009 F1: 100.00% dev loss: 5.4405 F1: 54.79% 0.48 min
> Epoch: 37 Step: 2800, train loss: 0.0009 F1: 100.00% dev loss: 5.4548 F1: 54.75% 0.48 min
> Epoch: 38 Step: 2900, train loss: 0.0009 F1: 100.00% dev loss: 5.4577 F1: 54.92% 0.48 min
> Epoch: 39 Step: 3000, train loss: 0.0008 F1: 100.00% dev loss: 5.4589 F1: 54.92% 0.48 min
